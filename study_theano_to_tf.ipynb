{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py, random, csv, gzip, time \n",
    "import pickle\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span_data, span_size, wmap, cmap, bmap = load_data('data/relationships.csv.gz', 'data/metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length=list()\n",
    "for i in span_data:\n",
    "    length.append(len(i[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/glove.We', 'rb') as f:\n",
    "    We = pickle.load(f, encoding='latin1').astype('float32')\n",
    "\n",
    "#norm_We = We / np.linalg.norm(We, axis=1)[:, None]\n",
    "We = np.nan_to_num(We / np.linalg.norm(We, axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20046"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(span_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]]\n",
      "[[ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]\n",
      " [ 1.  1.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "for book, chars, curr, cm, in span_data[:3]:\n",
    "    bname = bmap[book[0]]\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SonsofTheOak',\n",
       " 'Postman',\n",
       " 'DeathTrance',\n",
       " 'SummerofNight',\n",
       " 'WitchesofEastwick',\n",
       " 'OpCenter',\n",
       " 'Sourcery',\n",
       " 'Phantoms',\n",
       " 'AnubisGates',\n",
       " 'IrishRose',\n",
       " 'HeartsInAtlantis',\n",
       " 'ThreeBedrooms,OneCorpse',\n",
       " 'GodInRuins',\n",
       " 'DivideAndConquer',\n",
       " 'CurrantEvents',\n",
       " 'SkeletonCrew',\n",
       " 'DayItRainedForever',\n",
       " 'Persuasion',\n",
       " 'MemoirsofAGeishaANovel',\n",
       " 'DarknessatSethanon',\n",
       " 'BloodMeridian',\n",
       " 'alcott-little-261',\n",
       " 'LotusEatersaNovel',\n",
       " 'Confusion',\n",
       " 'HoundofTheBaskervilles',\n",
       " 'NineTomorrows',\n",
       " 'IslandofTheSequinedLoveNun',\n",
       " 'TaleoftheBodyThief',\n",
       " 'MurderIntheMews',\n",
       " 'UnderTheBannerofHeavenAStoryofV',\n",
       " 'MazeofDeath',\n",
       " 'BlackWind',\n",
       " 'WasteLands',\n",
       " 'ShortHistoryofNearlyEverything',\n",
       " 'OddHours',\n",
       " 'GrandFinale',\n",
       " 'AmericanGods',\n",
       " 'ParrotAndOlivierInAmerica',\n",
       " 'HolyThief',\n",
       " 'LeapofFaith',\n",
       " 'RobotsAndAliens5Maverick',\n",
       " 'GodEmperorofDune',\n",
       " 'SaharaTradePaper',\n",
       " 'Provenance',\n",
       " 'ElegantUniverseSuperstrings,Hidd',\n",
       " 'SleepingMurder',\n",
       " 'DarklyDreamingDexter',\n",
       " 'WorldWarZAnOralHistoryofTheZomb',\n",
       " 'FantasticVoyageIIDestinationBrain',\n",
       " 'london-iron-204',\n",
       " 'EndoftheWholeMess',\n",
       " 'GuardiansofTheWest',\n",
       " 'MartianWayAndOtherStories',\n",
       " '4thofJuly',\n",
       " 'ProblematPollensaBayAndOtherStorie',\n",
       " 'RoseMadder',\n",
       " 'DeusIrae',\n",
       " 'GhostwrittenaNovel',\n",
       " 'TwototheFifth',\n",
       " 'ExtremeMeasuresaThriller',\n",
       " 'UnchartedTerritory',\n",
       " 'Scar',\n",
       " 'PetPeeve',\n",
       " 'Maskerade',\n",
       " 'ChildrenofHurin',\n",
       " 'AtTheGatesofDarkness',\n",
       " 'IcarusAgenda',\n",
       " 'StormIsland',\n",
       " 'RedProphet',\n",
       " 'PerdidoStreetStation',\n",
       " 'MysteryoftheBlueTrain',\n",
       " 'HoundofDeath',\n",
       " 'CityofBones',\n",
       " 'DeadWater',\n",
       " 'CompanyaNoveloftheCIA',\n",
       " 'Twilight',\n",
       " 'GolemIntheGears',\n",
       " 'BraveNewWorld',\n",
       " 'ConvincingAlex',\n",
       " 'BendIntheRoad',\n",
       " 'RoomWithaView',\n",
       " 'LolaCarlyleRevealsAll',\n",
       " 'Summons',\n",
       " 'Dr.Bloodmoney',\n",
       " 'CrimeAndPunishment',\n",
       " 'Scarecrow',\n",
       " 'Silverthorn',\n",
       " 'SpyWhoLovedMeaJamesBondNovel',\n",
       " 'Child44',\n",
       " 'SizzlingSixteen',\n",
       " 'defoe-robinson-103',\n",
       " 'TearsofTheMoon',\n",
       " 'LastLecture',\n",
       " 'CollectedPoemsofEmilyDickinson',\n",
       " 'PlayingforPizza',\n",
       " 'MysteriousAffairatStyles',\n",
       " 'SwallowingDarkness',\n",
       " 'CardsonTheTable',\n",
       " 'NorwaytoHideaPassporttoPerilMyst',\n",
       " 'FlightofTheNighthawks',\n",
       " '7thHeaven',\n",
       " 'GlassCastleaMemoir',\n",
       " 'GoodbyeDolly',\n",
       " 'UnseenAcademicals',\n",
       " 'WhenIStopTalking,YouLlKnowIMDea',\n",
       " '2010OdysseyTwo',\n",
       " 'SignofFour',\n",
       " 'CompleteStories',\n",
       " 'Mort',\n",
       " 'LastCamelDiedatNoon',\n",
       " 'RemarkableCreatures',\n",
       " 'DivineMisdemeanors',\n",
       " 'SecondFoundation',\n",
       " 'Pygmy',\n",
       " 'PercyJacksonAndtheLastOlympian',\n",
       " 'ManLayDead',\n",
       " 'VagabondinganUncommonGuidetotheAr',\n",
       " 'AtlantisFound',\n",
       " 'FourtoScore',\n",
       " 'QuickerThantheEye',\n",
       " 'AdventuresofTomSawyer',\n",
       " 'DeadOverHeels',\n",
       " 'Beowulf',\n",
       " 'Utopia',\n",
       " 'HornetFlight',\n",
       " 'BestDetectiveShortStories',\n",
       " 'GoingPostal',\n",
       " 'ByThePrickingofMyThumbs',\n",
       " 'WyrmlingHordetheSeventhBookof',\n",
       " 'InterviewWiththeVampireaNovel',\n",
       " 'MemorialDay',\n",
       " 'GraveyardforLunaticsAnotherTaleO',\n",
       " 'NotebookaNovel',\n",
       " 'Thud!',\n",
       " 'AliceInWonderland',\n",
       " 'GirlWhoLovedTomGordon',\n",
       " 'FullBlast',\n",
       " 'MurderInMesopotamia',\n",
       " 'ChamberMusic',\n",
       " 'BourneDeception',\n",
       " 'ElevenMinutes',\n",
       " 'DevilAndSherlockHolmesTalesof',\n",
       " 'ListerdaleMystery',\n",
       " 'ShadowofADarkQueen',\n",
       " 'CrimsonRooms',\n",
       " 'GodsThemselves',\n",
       " 'carol10',\n",
       " 'Humanity',\n",
       " 'DuneMessiah',\n",
       " 'toKillaMockingbird',\n",
       " 'AgeofInnocence',\n",
       " 'ThreeBlindMice',\n",
       " 'Necromancer',\n",
       " 'BloodBrothers',\n",
       " 'TalonofTheSilverHawk',\n",
       " 'ColdDish',\n",
       " 'SanctuarySparrowtheSeventhChron',\n",
       " 'ParkerPyneInvestigates',\n",
       " 'IAmLegend',\n",
       " 'LastoftheMohicans',\n",
       " 'AbrahamLincolnVampireHunterSethGraham',\n",
       " 'AsimovsChimeratheNewIsaacAsimovs',\n",
       " 'HeartofDevinMacKade',\n",
       " 'BloodofTheFold',\n",
       " 'VirginIntheIcetheSixthChronic',\n",
       " 'MistressofTheEmpire',\n",
       " 'DolledUpforMurder',\n",
       " 'Sail',\n",
       " 'Dictator',\n",
       " 'AltmanCode',\n",
       " 'AlongCameaSpider',\n",
       " 'PilgramofHate',\n",
       " 'RisingTides',\n",
       " 'WhiteDeath',\n",
       " 'WitchofPortobello',\n",
       " 'DearJohn',\n",
       " 'PuzzlesofTheBlackWidowers',\n",
       " 'CasinoRoyaleaJamesBondNovel',\n",
       " 'BladeItself',\n",
       " 'EngineSummer',\n",
       " 'ofMiceAndMen',\n",
       " 'BlueMoon',\n",
       " 'RageofADemonKing',\n",
       " 'ArtemisFowl',\n",
       " 'WithoutaTrace',\n",
       " 'V',\n",
       " 'FireIce',\n",
       " 'DangerousLiaisons',\n",
       " 'DaisyMillerAnd,WashingtonSquare',\n",
       " 'ApocalypseWatch',\n",
       " 'NOrM',\n",
       " 'RosesAreRed',\n",
       " 'PillarsofCreation',\n",
       " 'TrueBeliever',\n",
       " 'PerfectNeighbor',\n",
       " 'DeathonTheNileAHerculePoirotMyst',\n",
       " 'UseofWeapons',\n",
       " 'LordsAndLadies',\n",
       " 'Moonraker',\n",
       " 'GeisofTheGargoyle',\n",
       " 'ManIntheMiddle',\n",
       " 'OnceThereWasaWar',\n",
       " 'ValleyofFear',\n",
       " 'ApeWhoGuardstheBalance',\n",
       " 'NakedSun',\n",
       " 'MenatArms',\n",
       " 'LaughingCorpse',\n",
       " 'CurseofthePharaohs',\n",
       " 'LastArgumentofKings',\n",
       " 'swift-gullivers-728',\n",
       " 'TiedUpInTinsel',\n",
       " 'StrokeofMidnight',\n",
       " 'Rebellion',\n",
       " 'fromDeadtoWorse',\n",
       " 'AsimovsMiragetheNewIsaacAsimovs',\n",
       " 'WyrdSistersDiscworldSeriesTerryPra',\n",
       " 'PerfectStormaTrueStoryofMena',\n",
       " 'SongofSusannah',\n",
       " 'LightFantastic',\n",
       " 'Guns,GermsndSteelTheFatesofHum',\n",
       " 'Sunstorm',\n",
       " 'MurderInLaMut',\n",
       " 'ChokeaNovel',\n",
       " 'Timeline',\n",
       " 'MorbidTasteforBones',\n",
       " 'CodetoZero',\n",
       " 'SummerKnight',\n",
       " 'CaptiveMarket',\n",
       " 'Depraved',\n",
       " 'Magician',\n",
       " 'ShipofMagic',\n",
       " 'ConsiderPhlebas',\n",
       " 'MyDeadBody',\n",
       " 'Captivated',\n",
       " 'kipling-kim-149',\n",
       " 'LuckyStarrAndTheRingsofSaturn',\n",
       " 'Level7',\n",
       " 'SacredGround',\n",
       " '9thJudgment',\n",
       " 'KeepingFaithaNovel',\n",
       " 'AmazingAdventuresofKavalierAndC',\n",
       " '48LawsofPower',\n",
       " 'eliot-silas-242',\n",
       " 'ArcticEvent',\n",
       " 'AmazingMauriceAndHisEducatedRod',\n",
       " 'SpeakstheNightbird',\n",
       " 'HadesFactor',\n",
       " 'TermLimits',\n",
       " 'FearsUnnamed',\n",
       " 'totheNines',\n",
       " 'PocketfulofRye',\n",
       " 'GreenMile',\n",
       " 'RantAnOralBiographyofBusterCasey',\n",
       " 'atFirstSight',\n",
       " 'NightsInRodanthe',\n",
       " 'HellboundHeart',\n",
       " 'DevilIntheWhiteCityMurder,Mag',\n",
       " 'DeathMasks',\n",
       " 'Eric',\n",
       " 'Pearl',\n",
       " 'MatareseCircle',\n",
       " 'Mist',\n",
       " 'BlindSideEvolutionofaGame',\n",
       " 'DrownedWorld',\n",
       " 'Chainfire',\n",
       " 'Roadwork',\n",
       " 'HardEight',\n",
       " 'NineDragons',\n",
       " 'PoirotLosesaClient',\n",
       " 'BrassVerdict',\n",
       " 'BourneIdentity',\n",
       " 'SumofAllMen',\n",
       " 'FangaMaximumRideNovel',\n",
       " '1984',\n",
       " 'mdmar10',\n",
       " 'DeadAsaDoornail',\n",
       " 'CrocodileonTheSandbank',\n",
       " '2061OdysseyThree',\n",
       " 'PatientZeroaJoeLedgerNovel',\n",
       " 'CryoftheHalidon',\n",
       " 'Fool',\n",
       " 'Stranger',\n",
       " 'SmallGods',\n",
       " 'RiseofAMerchantPrince',\n",
       " 'DeathAndtheDancingFootman',\n",
       " 'TopOtheMournin',\n",
       " 'WhattheDogSawAndOtherAdventures',\n",
       " 'DevilsNovice',\n",
       " 'KingoftheMurgos',\n",
       " 'CompleteShortStoriesofMissMarple',\n",
       " 'ClutchofConstables',\n",
       " 'WolvesofTheCalla',\n",
       " 'Jingo',\n",
       " 'FourBlindMice',\n",
       " 'BandofBrothersECompany,506thRegim',\n",
       " 'VoyageoftheDawnTreader',\n",
       " 'PlumLucky',\n",
       " '3rdDegree',\n",
       " 'InColdBlood',\n",
       " 'melville-moby-106',\n",
       " 'OpenAndShut',\n",
       " 'CurseofLono',\n",
       " 'ScarlettiInheritance.SpokenWord',\n",
       " 'FoolMoon',\n",
       " 'GreatestTradeEvertheBehindthe',\n",
       " 'Descent',\n",
       " 'TownLikeAlice',\n",
       " 'WalktoRemember',\n",
       " 'PerilatEndHouse',\n",
       " 'Medusa',\n",
       " 'AllthePrettyHorses',\n",
       " 'PastaImperfect',\n",
       " 'FifthMountain',\n",
       " 'SingingIntheShrouds',\n",
       " 'ForbiddenCity',\n",
       " 'MatareseCountdown',\n",
       " 'DanceofTheGods',\n",
       " 'FinalWarning',\n",
       " 'BeforeTheyAreHanged',\n",
       " 'RestaurantattheEndoftheUnivers',\n",
       " 'PoppyDonetoDeath',\n",
       " 'DeepSix',\n",
       " 'Gunslinger',\n",
       " 'ChaosboundtheEighthBookoftheRunel',\n",
       " 'Faefever',\n",
       " 'DeadBeat',\n",
       " 'Brethren',\n",
       " 'Thinner',\n",
       " 'DevilMayCare',\n",
       " 'P.S.ILoveYou',\n",
       " 'SevenHabitsofHighlyEffectivePeople',\n",
       " 'CeruleanSins',\n",
       " 'Ogre,Ogre',\n",
       " '450fromPaddington',\n",
       " 'MadShip',\n",
       " 'TristanBetrayal',\n",
       " 'Desperation',\n",
       " 'FightClubaNovel',\n",
       " 'DrivingBlind',\n",
       " 'IntotheWild',\n",
       " 'PhotoFinish',\n",
       " 'MacGregorGrooms',\n",
       " 'NorthangerAbbey',\n",
       " 'DeathofAPeer',\n",
       " 'IronCouncil',\n",
       " 'Partner',\n",
       " 'HickoryDickoryDeath',\n",
       " 'HoldingtheDream',\n",
       " 'TurnCoat',\n",
       " 'ThirdGirl',\n",
       " 'GuiltyPleasures',\n",
       " 'Alchemist',\n",
       " 'TalesofTheBlackWidowers',\n",
       " 'FuriesofCalderon',\n",
       " 'DanceUpontheAir',\n",
       " 'Broker',\n",
       " 'FreakonomicsaRogueEconomistExplores',\n",
       " 'ProdigalSon',\n",
       " 'JuliusHouse',\n",
       " 'Poet',\n",
       " 'Clocks',\n",
       " 'LuckyStarrAndtheBigSunofMercury',\n",
       " '7gabl10',\n",
       " 'Carrie',\n",
       " 'PacificVortex!',\n",
       " 'NursingHomeMurder',\n",
       " 'RichDadPoorDadforTeenstheSecrets',\n",
       " 'CatAmongthePigeons',\n",
       " 'PilgrimageaContemporaryQuestfor',\n",
       " 'BanquetsoftheBlackWidowers',\n",
       " 'ThirteenProblems',\n",
       " 'ShadowofTheHegemon',\n",
       " 'Chapterhouse,Dune',\n",
       " 'LoveOverboard',\n",
       " 'PhysicsofStarTrek',\n",
       " 'LullabyaNovel',\n",
       " 'Overlook',\n",
       " 'WutheringHeights',\n",
       " 'DangerousFortune',\n",
       " 'Stardust',\n",
       " 'FallofShaneMacKade',\n",
       " 'CarpeJugulum',\n",
       " 'Warrior',\n",
       " 'ExcellentMysteryn',\n",
       " 'Pyramids',\n",
       " 'DeadGiveaway',\n",
       " 'OdysseyMichaelP.Kube',\n",
       " 'CardinaloftheKremlin',\n",
       " 'ZodiactheEcoThriller',\n",
       " 'ShockWave',\n",
       " 'IKnowYouGotSoulMachinesWithThat',\n",
       " 'ColorofHerPanties',\n",
       " 'PaleHorse',\n",
       " 'Emperor',\n",
       " 'PastImperfect',\n",
       " 'HorseAndHisBoy',\n",
       " '5thHorseman',\n",
       " 'GravePeril',\n",
       " 'ShardsofABrokenCrown',\n",
       " 'conrad-lord-373',\n",
       " 'FindingtheDream',\n",
       " 'Coraline',\n",
       " 'SpiderStone',\n",
       " 'FearAndLoathingInLasVegasndOthe',\n",
       " 'BourneUltimatum',\n",
       " 'SummeroftheDanes',\n",
       " 'DarkAssassin',\n",
       " 'HungerGames',\n",
       " 'ZenMind,BeginnersMind',\n",
       " 'VintageMurder',\n",
       " 'Micah',\n",
       " 'BodyIntheLibrary',\n",
       " 'LickofFrost',\n",
       " 'ReturnofSherlockHolmes',\n",
       " 'Enchanted',\n",
       " 'BourneSupremacy',\n",
       " '2001aSpaceOdyssey',\n",
       " 'Silmarillion',\n",
       " 'HarvestHome',\n",
       " 'onStrangerTides',\n",
       " 'Shining',\n",
       " 'WhenWillJesusBringthePorkChops',\n",
       " 'onTheBeach',\n",
       " 'SkinTrade',\n",
       " 'MinorityReport',\n",
       " 'SittafordMystery',\n",
       " 'NameoftheWind',\n",
       " 'PatriotGames',\n",
       " 'crane-red-376',\n",
       " 'Blaze',\n",
       " 'ChangeofHeartANovel',\n",
       " 'SilverChair',\n",
       " 'Thanksgiving',\n",
       " 'AngelExperiment',\n",
       " 'NightfallAndOtherStories',\n",
       " 'IsleofView',\n",
       " 'PlumSpooky',\n",
       " 'UndertheDomeaNovel',\n",
       " 'BloodsuckingFiendsaLoveStory',\n",
       " 'PositronicMan',\n",
       " 'FoolAndHisHoney',\n",
       " 'LastJuror',\n",
       " 'Stars,LikeDust',\n",
       " 'AndAnotherThingtheWorldAccordingT',\n",
       " 'KisstheGirls',\n",
       " 'MemoirsofSherlockHolmes',\n",
       " 'warpeace',\n",
       " 'IHopeIShallArriveSoon',\n",
       " 'FevreDream',\n",
       " 'DearlyDevotedDexter',\n",
       " 'StreetGangTheCompleteHistoryofSes',\n",
       " 'PaganStone',\n",
       " 'BreakfastInBed',\n",
       " 'Closers',\n",
       " 'FirstDegree',\n",
       " 'QueenofBedlam',\n",
       " 'DumaKey',\n",
       " 'CrookedHouse',\n",
       " 'Alliance',\n",
       " 'DingDongDead',\n",
       " 'ScalesofJustice',\n",
       " 'TwelveSharp',\n",
       " 'BigBadWolf',\n",
       " 'BlackRose',\n",
       " 'HotelonTheCornerofBitterAndSweet',\n",
       " 'ArtemisFowltheEternityCode',\n",
       " 'Nemesis',\n",
       " 'NapalmAndSillyPutty',\n",
       " 'StreetLawyer',\n",
       " 'baum-wonderful-282',\n",
       " 'AudacityofHopeThoughtsOnReclai',\n",
       " 'kidnap',\n",
       " 'LoneEagle',\n",
       " 'FalseScent',\n",
       " 'BookofUselessInformation',\n",
       " 'CubeRoute',\n",
       " 'CouncilofDads',\n",
       " 'WhiletheLightLasts',\n",
       " 'Flirt',\n",
       " 'ConfessionofBrotherHaluin',\n",
       " 'ManIntheBrownSuit',\n",
       " 'DaVinciCode',\n",
       " 'Goldfinger007JamesBondNovel',\n",
       " 'PoirotInvestigates',\n",
       " 'Lost',\n",
       " 'dreiser-sister-393',\n",
       " 'MyAntonia',\n",
       " 'DeathattheDolphin',\n",
       " 'DeathInEcstasy',\n",
       " 'ArtofWar',\n",
       " 'OneDoorAwayfromHeaven',\n",
       " 'KeyofLight',\n",
       " 'Strangers',\n",
       " 'UpInaHeaval',\n",
       " 'hardy-jude-132',\n",
       " 'DarknessMoreThanNight',\n",
       " 'DoubleCross',\n",
       " 'WitchesAbroad',\n",
       " 'IntoaDarkRealm',\n",
       " 'AllWorkedUp',\n",
       " 'DexterIntheDark',\n",
       " 'SigmaProtocol',\n",
       " 'DeadZone',\n",
       " 'GettingOldIstheBestRevenge',\n",
       " 'JurassicPark',\n",
       " 'OrdealByInnocence',\n",
       " 'wells-war-189',\n",
       " 'HighFive',\n",
       " 'NewTricks',\n",
       " 'Alchemyst',\n",
       " 'Autobiographyn',\n",
       " 'CellaNovel',\n",
       " 'DrawingoftheThree',\n",
       " 'ClockworkOrange',\n",
       " 'Cyborg',\n",
       " 'BrotherhoodofTheWolf',\n",
       " 'SonofAWitchVolumeTwoInTheWicked',\n",
       " 'DeadAndAlive',\n",
       " 'FablehavenKeystotheDemonPrison',\n",
       " 'PowerofNow',\n",
       " 'melville-typee-107',\n",
       " 'DressYourFamilyInCorduroyAndDenim',\n",
       " 'Ask',\n",
       " 'SevenDialsMystery',\n",
       " 'SadCypress',\n",
       " 'CommonSenseRightsofManAndOthe',\n",
       " 'Dune',\n",
       " 'scott-ivanhoe-159',\n",
       " 'MoonIsDown',\n",
       " 'One,Two,BuckleMyShoe',\n",
       " 'SecretSanction',\n",
       " 'ChristianitytheFirstThreeThousandY',\n",
       " 'DaughterofTheEmpire',\n",
       " 'DeceptionPoint',\n",
       " 'ForestMage',\n",
       " 'Hannibal',\n",
       " 'DeadCenter',\n",
       " 'RareBenedictine',\n",
       " '3001theFinalOdyssey',\n",
       " 'ShortSecondLifeofBreeTanneran',\n",
       " 'DyingforDinner',\n",
       " 'Snuff',\n",
       " 'NightMare',\n",
       " 'LieDownWithLions',\n",
       " 'LastCoyote',\n",
       " 'Inferno',\n",
       " 'DestinationUnknown',\n",
       " 'BlindAssassin',\n",
       " 'EchoPark',\n",
       " 'HermitofEytonForest',\n",
       " 'MemnochtheDevil',\n",
       " 'BuyJupiterAndOtherStories',\n",
       " 'BlackCoffee',\n",
       " 'DirtyJob',\n",
       " 'Breeder',\n",
       " 'WhentheWindBlows',\n",
       " 'SorceressofDarshiva',\n",
       " 'RockyRoadtoRomance',\n",
       " 'AirApparent',\n",
       " 'Catch22',\n",
       " 'GameofThrones',\n",
       " 'PracticeMakesPerfect',\n",
       " 'TamingNatasha',\n",
       " 'DoomsdayBook',\n",
       " 'AndThenThereWereNone',\n",
       " 'MurderIsAnnounced',\n",
       " 'SafeHarbor',\n",
       " 'Reflex',\n",
       " 'HouseRules',\n",
       " 'LovingJack',\n",
       " 'cather-song-367',\n",
       " 'AnnaKarenina',\n",
       " 'FaithofTheFallen',\n",
       " 'RobotVisions',\n",
       " 'NeverwhereaNovel',\n",
       " 'InnocentManMurderAndInjusticeI',\n",
       " 'cather-alexanders-365',\n",
       " 'Chaos',\n",
       " 'Confessor',\n",
       " 'SuperfreakonomicsGlobalCooling,Patri',\n",
       " 'NightSmoke',\n",
       " 'austen-sense-758',\n",
       " 'BagofBones',\n",
       " 'RealMurders',\n",
       " 'james-portrait-134',\n",
       " 'MonaLisaOverdrive',\n",
       " 'Naked',\n",
       " 'JohnAdams',\n",
       " 'DancetothePiper',\n",
       " 'orczy-scarlet-225',\n",
       " 'Innocent',\n",
       " 'IncaGold',\n",
       " '1sttoDie',\n",
       " 'LostScrolls',\n",
       " 'RidesaDreadLegion',\n",
       " 'forNow,Forever',\n",
       " 'NarrativeofTheLifeofFrederickDougl',\n",
       " 'NoCountryforOldMen',\n",
       " 'DaringtoDream',\n",
       " 'WhiteNight',\n",
       " 'ShadowoftheWind',\n",
       " 'WhatDreamsMayCome',\n",
       " 'BloodRites',\n",
       " 'Motorworld',\n",
       " 'BestLaidPlans',\n",
       " 'StormFront',\n",
       " 'FiveLittlePigs',\n",
       " 'SkippingChristmas',\n",
       " 'TwofortheDough',\n",
       " 'GamesofState',\n",
       " 'MereChristianityaRevisedAndAmplifi',\n",
       " 'BlackSwanGreen',\n",
       " 'SecretsofAPerfectNight',\n",
       " 'Airframe',\n",
       " 'LicksofLoveShortStoriesAndASeque',\n",
       " 'SeaSwept',\n",
       " 'PictureofDorianGray',\n",
       " 'ProtectAndDefendaThriller',\n",
       " 'RobotDreams',\n",
       " '2ndChance',\n",
       " 'SpellforChameleon',\n",
       " 'LastDitch',\n",
       " 'Dreamcatcher',\n",
       " 'JoyLuckClub',\n",
       " 'FantasticVoyage',\n",
       " 'HighDruidofShannaraJarkaRuus',\n",
       " 'SecretsofTheDragonSanctuary',\n",
       " 'GoldtheFinalScienceFictionCollecti',\n",
       " 'DoorIntoSummer',\n",
       " 'SparklingCyanide',\n",
       " 'MediumRawaBloodyValentinetotheWo',\n",
       " 'ThunderballaJamesBondNovel',\n",
       " 'SecondVariety',\n",
       " 'StupidestAngelaHeartwarmingTale',\n",
       " 'ParisOption',\n",
       " 'MonksHood',\n",
       " 'LightThickens',\n",
       " 'EarthIsRoomEnough',\n",
       " 'Godfather',\n",
       " 'Iceberg',\n",
       " 'LastSceneAlive',\n",
       " 'Hamlet',\n",
       " 'ArtemisFowltheOpalDeception',\n",
       " 'VioletsAreBlue',\n",
       " 'BloodyBones',\n",
       " '61Hours',\n",
       " 'PrayerforOwenMeanyaNovel',\n",
       " 'BloodAndGold',\n",
       " '8thConfession',\n",
       " 'Dubliners',\n",
       " 'NotesfromaSmallIsland',\n",
       " '1776',\n",
       " 'PerfectMatch',\n",
       " 'SuperSadTrueLoveStoryaNovel',\n",
       " 'Shadowland',\n",
       " 'NightWatch',\n",
       " 'MacGregorBrides',\n",
       " 'GreatGatsby',\n",
       " 'NorbyChronicles',\n",
       " 'WinterHaunting',\n",
       " 'BorntoBeRiled',\n",
       " 'CompleteRobot',\n",
       " 'DeadUntilDark',\n",
       " 'GodDelusion',\n",
       " 'HarryBoschNovelstheLastCoyote,',\n",
       " 'DeathComesAstheEnd',\n",
       " 'CyranoDeBergerac',\n",
       " 'KrondortheAssassins',\n",
       " 'KillingDance',\n",
       " 'MAXaMaximumRideNovel',\n",
       " 'LuringaLady',\n",
       " 'PaperMoney',\n",
       " 'Cyclops',\n",
       " 'Entranced',\n",
       " 'GettingOldIsMurder',\n",
       " 'Narrows',\n",
       " 'Mine',\n",
       " 'DexterByDesign',\n",
       " 'CoyoteBlue',\n",
       " 'HotSix',\n",
       " 'GirlWiththeDragonTattoo',\n",
       " 'ValleyofSilence',\n",
       " 'Bleachers',\n",
       " 'Refuge',\n",
       " 'DiamondAge',\n",
       " 'LuckyStarrAndTheOceansofVenus',\n",
       " 'ItMustBeLove',\n",
       " 'Associate',\n",
       " 'GodofThunder',\n",
       " 'Perelandra',\n",
       " 'HeavenCent',\n",
       " 'Rabbit,Run',\n",
       " 'FoundationAndEarth',\n",
       " 'Marauder',\n",
       " 'WarofTheTwins',\n",
       " 'IntheDark',\n",
       " 'OnefortheMoney',\n",
       " 'ScrewjackaShortStory',\n",
       " 'Watchers',\n",
       " 'LairofBones',\n",
       " 'DeathattheBar',\n",
       " 'kipling-jungle-148',\n",
       " 'Dhammapada',\n",
       " 'ManforAmanda',\n",
       " 'Sphere',\n",
       " 'CatchingFire',\n",
       " 'EatersofTheDead',\n",
       " 'GirlWhoPlayedWithFire',\n",
       " 'Siddhartha',\n",
       " 'Prestige',\n",
       " 'Matter',\n",
       " 'QuestionQuest',\n",
       " 'Cujo',\n",
       " 'SwanThievesaNovel',\n",
       " 'ForTheLoveofLilah',\n",
       " 'DarkUniverse',\n",
       " 'OffWithHisHeadSingingIntheShroud',\n",
       " 'IllustratedMan',\n",
       " 'WitchAndWizard',\n",
       " 'PreludetoFoundation',\n",
       " 'AlpineforYou',\n",
       " 'VeronikaDecidestoDie',\n",
       " '6thTarget',\n",
       " 'FountainsofParadise',\n",
       " 'FinnegansWake',\n",
       " 'Guards!Guards!',\n",
       " 'CrystalCity',\n",
       " 'CatcherIntheRye',\n",
       " 'ScannerDarkly',\n",
       " 'NarcissusInChains',\n",
       " 'DollyDeparted',\n",
       " 'CharlotteGray',\n",
       " 'LiveAndLetDieaJamesBondNovel',\n",
       " 'LineofControl',\n",
       " 'NothingButTrouble',\n",
       " 'OneHundredYearsofSolitude',\n",
       " 'HuntforRedOctober',\n",
       " 'WorldWithoutUs',\n",
       " 'SpiritBanner',\n",
       " 'ThreetoGetDeadlyAndFourtoScore',\n",
       " 'TempleofTheWinds',\n",
       " 'Sleepless',\n",
       " 'MurderHasaSweetTooth',\n",
       " 'WiseguyLifeInaMafiaFamily',\n",
       " 'WeekInDecember',\n",
       " 'NumaFiles1.Serpent',\n",
       " 'LostWorldsof2001',\n",
       " 'LazarusVendetta',\n",
       " 'DemolishedMan',\n",
       " 'LostLight',\n",
       " 'PlaceCalledFreedom',\n",
       " 'Stinger',\n",
       " 'bronte-jane-178',\n",
       " 'GraveyardBook',\n",
       " 'RoyalAssassin',\n",
       " 'PrivateSector',\n",
       " 'WrinkleInTime',\n",
       " 'BattleoftheLabyrinth',\n",
       " 'a.b.c.MurdersaHerculePoirotMys',\n",
       " 'PrivateLife',\n",
       " 'DeathMask',\n",
       " 'PrenticeAlvin',\n",
       " 'NakedEmpire',\n",
       " 'VanishingActs',\n",
       " 'WinnerStandsAlone',\n",
       " 'VampireArmand',\n",
       " 'madambov',\n",
       " 'PomesPenyeach',\n",
       " 'BacktotheBedroom',\n",
       " 'Dracula',\n",
       " 'Breathless',\n",
       " 'BrotherOdd',\n",
       " 'Excession',\n",
       " 'DarkTower',\n",
       " 'ThirteenthTaleaNovel',\n",
       " 'CaribbeanMystery',\n",
       " 'LifeUniverseAndEverything',\n",
       " 'ShipofDestiny',\n",
       " 'LivingDeadInDallas',\n",
       " 'HumanTraces',\n",
       " 'ChelseaChelseaBangBang',\n",
       " 'DeadAndGone',\n",
       " 'BreathofSnowAndAshes',\n",
       " 'Babbitt',\n",
       " 'NavigatoraNovelfromtheNUMAFil',\n",
       " 'LeanMeanThirteen',\n",
       " 'CavernofBlackIce',\n",
       " 'PolarShift',\n",
       " 'CompleteAdventuresofLuckyStarr',\n",
       " 'TimetoKill',\n",
       " 'MonsterHunterInternational',\n",
       " 'Harlequin',\n",
       " 'AnansiBoys',\n",
       " 'CanticleforLeibowitz',\n",
       " 'BicentennialManAndOtherStories',\n",
       " 'PaintedHouse',\n",
       " 'AppointmentWithDeathaHerculePoirot',\n",
       " 'JansonDirective',\n",
       " 'ClarksononCars',\n",
       " 'JusttheSexiestManAlive',\n",
       " 'treas10',\n",
       " 'TestofTheTwins',\n",
       " 'ThatHideousStrength',\n",
       " 'GreatSimoleanCaper',\n",
       " 'Birdsong',\n",
       " 'ElephantsCanRemember',\n",
       " 'ZahiraNovelofObsession',\n",
       " 'LovelyBones',\n",
       " 'FifthElephant',\n",
       " 'ConfessionsofACrapArtistJackIsido',\n",
       " 'QueenofSorcery',\n",
       " 'LordEdgwareDies',\n",
       " 'YonIllWind',\n",
       " 'ArtemisFowlAndtheAtlantisComplex',\n",
       " 'ActofTreason',\n",
       " 'GettingOldIstoDiefor',\n",
       " 'TippingPointHowLittleThingsCan',\n",
       " 'Mary,Mary',\n",
       " 'CompleteBooksofBlood',\n",
       " 'StateofSiege',\n",
       " 'ColourofMagic',\n",
       " 'AdventuresofHuckleberryFinn',\n",
       " 'ThreeActTragedy',\n",
       " 'Footprints',\n",
       " 'SelfishGene',\n",
       " 'OneCorpseTooManytheSecondChronicl',\n",
       " 'JimmytheHand',\n",
       " 'ChasingtheDime',\n",
       " 'MisterSlaughter',\n",
       " 'SoulfofTheFire',\n",
       " 'BloodCanticle',\n",
       " 'Coup',\n",
       " 'JackAndJill',\n",
       " 'SwellFoop',\n",
       " '2000010',\n",
       " 'NightfallTwoScienceFictionStories',\n",
       " 'BlackEchotheBlackIcetheCon',\n",
       " 'EveryLastDrop',\n",
       " 'SwordofShannara',\n",
       " 'NowWaitforLastYear',\n",
       " 'EnderInExile',\n",
       " 'HarlequinTeaSetAndOtherStories',\n",
       " 'LionIntheValley',\n",
       " 'OneDay',\n",
       " 'FirstKingofShannara',\n",
       " 'MonstrousRegiment',\n",
       " 'Pariah',\n",
       " 'BlackHawkDownAStoryofModernWar',\n",
       " 'Jumper',\n",
       " 'Fahrenheit451',\n",
       " 'Wintersmith',\n",
       " 'IncubusDreams',\n",
       " 'cather-o-366',\n",
       " 'FullScoop',\n",
       " 'RaisetheTitanic!',\n",
       " 'Centaur',\n",
       " 'TheyDoItWithMirrors',\n",
       " 'DemonLordofKaranda',\n",
       " 'ArtofRacingIntheRain',\n",
       " 'FaunAndGames',\n",
       " 'SecretofChimneys',\n",
       " 'WeeFreeMen',\n",
       " 'SavingtheWorldAndOtherExtremeSport',\n",
       " 'WorldWithoutIce',\n",
       " 'Imperfectionists',\n",
       " 'SpinstersInJeopardy',\n",
       " 'Testament',\n",
       " 'Blackout',\n",
       " 'NightProbe!',\n",
       " 'LaborsofHercules',\n",
       " 'Xenocide',\n",
       " 'baum-marvelous-278',\n",
       " 'SomethingWickedThisWayComes',\n",
       " 'RestoftheRobots',\n",
       " 'ThiefofTimetheDiscworldSeries',\n",
       " 'BreakingDawn',\n",
       " 'QueenoftheDamned',\n",
       " 'HeavenAndEarth',\n",
       " 'BoneConjurer',\n",
       " 'ManWiththeGoldenGunaJamesBon',\n",
       " 'HulaDoneIt',\n",
       " 'Neuromancer',\n",
       " 'BeachRoad',\n",
       " 'HowtoTalktoAnyone',\n",
       " 'DebtofBones',\n",
       " 'GraveMistake',\n",
       " 'LunaticCafe',\n",
       " 'RunningWiththeDemon',\n",
       " 'ChesapeakeBlue',\n",
       " 'CrewelLyeaCausticYarn',\n",
       " 'SeducedByMoonlight',\n",
       " 'LiesMyTeacherToldMeAboutChristophe',\n",
       " 'Terrorist',\n",
       " 'InvisibleMonsters',\n",
       " 'LustLizardofMelancholyCove',\n",
       " 'ShTMyDadSays',\n",
       " 'ChildrenofMen',\n",
       " 'Unbelievable',\n",
       " 'AptPupil',\n",
       " 'ServantofTheEmpire',\n",
       " 'GlamoramaaNovel',\n",
       " 'SevenUp',\n",
       " 'HonouredEnemy',\n",
       " 'BlockadeBilly',\n",
       " 'Client',\n",
       " 'JumperCable',\n",
       " 'CassandraCompact',\n",
       " 'MakingMoney',\n",
       " 'TheyCametoBaghdad',\n",
       " 'CentaurAisle',\n",
       " 'DiedIntheWool',\n",
       " 'CalloftheWildAndWhiteFang',\n",
       " 'DragononAPedestal',\n",
       " 'MurderIsEasy',\n",
       " 'CatchofTheDay',\n",
       " 'BorntoRunaHiddenTribe,Superathlet',\n",
       " 'LordofLight',\n",
       " 'HeartofTheSea',\n",
       " 'ReturnofRafeMackade',\n",
       " 'Voyager',\n",
       " 'SoLongndThanksforAlltheFish',\n",
       " 'Inversions',\n",
       " 'BlueGold',\n",
       " 'AllTogetherDead',\n",
       " 'DragonsofAutumnTwilight',\n",
       " 'TaleofTwoCities',\n",
       " 'DeadtotheWorld',\n",
       " 'ReaperMan',\n",
       " 'ArtistsInCrime',\n",
       " 'VoidofMoon',\n",
       " 'AdventuresofSherlockHolmes',\n",
       " 'GoodOmenstheNiceAndAccurateProphe',\n",
       " 'YouSuckaLoveStory',\n",
       " 'RedStormRising',\n",
       " 'KitchenConfidentialAdventuresInthe',\n",
       " 'MediterraneanCaper',\n",
       " 'Anathem',\n",
       " 'SixStories',\n",
       " 'Caliban',\n",
       " 'CradleAndAll',\n",
       " 'RedRabbit',\n",
       " 'Diary',\n",
       " 'ManfromMundania',\n",
       " 'Phantom',\n",
       " 'KrondortheBetrayal',\n",
       " 'TakenattheFlood',\n",
       " 'Instinctive',\n",
       " 'FallingforRachel',\n",
       " 'VisionInWhite',\n",
       " 'GettingOldIsaDisaster',\n",
       " 'MythstoLiveBy',\n",
       " 'CourtingCatherine',\n",
       " 'LincolnLawyer',\n",
       " 'WaitingforNick',\n",
       " 'Cryptonomicon',\n",
       " 'forLoveofEvil',\n",
       " 'I,Robot',\n",
       " 'TransferofPower',\n",
       " 'BeachHouse',\n",
       " 'UBIK',\n",
       " 'LastBattle',\n",
       " 'wells-invisible-187',\n",
       " 'IlexCross',\n",
       " 'MurderonTheLinks',\n",
       " 'LostCity',\n",
       " 'BeatriceAndVirgil',\n",
       " 'Algebraist',\n",
       " 'MummyCase',\n",
       " 'XoneofContention',\n",
       " 'Vixen',\n",
       " 'Pact',\n",
       " 'ParisEnigma',\n",
       " 'JewelsofTheSun',\n",
       " 'EarlyAsimovOr,ElevenYearsofTr',\n",
       " 'TortillaFlat',\n",
       " 'ClearAndPresentDanger',\n",
       " 'FullBloom',\n",
       " 'ThousandSplendidSuns',\n",
       " 'RobotsofDawn',\n",
       " 'Chamber',\n",
       " 'CommittedaSkepticMakesPeaceWithMa',\n",
       " 'WhereLatetheSweetBirdsSang',\n",
       " 'BarrelFever',\n",
       " 'Whiteout',\n",
       " 'Lawless',\n",
       " 'TeethoftheTiger',\n",
       " 'Treasure',\n",
       " 'Prince',\n",
       " 'HereticsofDune',\n",
       " 'SomethingAboutYou',\n",
       " 'RunawayJury',\n",
       " 'ObsidianButterfly',\n",
       " 'BriefHistoryofTime',\n",
       " 'Creepers',\n",
       " 'HostaNovel',\n",
       " 'burnett-secret-313',\n",
       " 'Nightfall',\n",
       " 'NightShiftNightShadow',\n",
       " 'DebtofHonor',\n",
       " 'TimeofTheTwins',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([594]),\n",
       " array([24244, 24260]),\n",
       " array([[15877,  4216, 11908,  6401,  4282,  3145,  3884,  4943, 13286,\n",
       "         13033,  6123,  7563,  8515, 12616, 12683, 16270, 14106, 14560,\n",
       "          1034, 11762,  5316,  8330,  1127, 13751, 14323, 15413,  3297,\n",
       "         15014,  8582,  5786,  6853, 12616, 14546,  8330,   953,  8330,\n",
       "         14882,  4758,  5309,  4462, 14266,  7678,  6064,  8700,  1443,\n",
       "         14266,  7678,  6699,  6699,  7678, 14266,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 7894, 13389, 11956,  5129,  5073,   727, 12251,  6775, 13375,\n",
       "         11598, 10010,  5641,  6725, 14876,  3021,  3723, 10999,  2754,\n",
       "          3718, 12683, 11121, 12683, 16270, 11598,  9488,  6345,   453,\n",
       "          8330, 14087,  8582,  5786,  9811, 12094, 14370, 15274,  3331,\n",
       "          9801, 13372,  2511,  9106,  4282,  1014, 15413,  8178,  3931,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [11535,   378, 15067,  6199, 10611,  3533,  3533,  1842, 15104,\n",
       "         10250, 15923, 14930,  6216,  1623, 13312,  5224, 10695, 14072,\n",
       "         14020,  5616, 15413,  8582,  5786, 15927,  3900,  2586, 14072,\n",
       "         12794,  2712,  7608,  2561, 15379,  5368, 16165, 10634, 14490,\n",
       "         16002,  8330, 16165, 11408,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 5224, 10695, 14072, 14020,  5616, 15413,  8582,  5786, 15927,\n",
       "          3900,  2586, 14072, 12794,  2712,  7608,  2561, 15379,  5368,\n",
       "         16165, 10634, 14490, 16002,  8330, 16165, 11408,  9681,  6021,\n",
       "          1814, 10631,  3255,  6778,   682,  6733,  3005,  5876,  7146,\n",
       "           120,   158,  7110,  1510,  1092,  7857,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 6678,  8796,  3021,   634,  1204,   724,  5819, 10906,  9254,\n",
       "          1152,  5610, 12156,  6660,  3664,  3923, 11270,  3208,  3992,\n",
       "          7930, 14818,   210, 14974,  3021,  3503,   381,  6419, 11732,\n",
       "         13182, 15140, 12067, 10167,  7511, 14870, 14557, 10836, 16279,\n",
       "          4238, 14130, 12385,  5617,  7110,  4643, 14407,  5883,  3608,\n",
       "         14130, 15312,   821,  5368, 11121,  3021,  6602, 15300, 15488,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_data[:3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptor_log = 'models/descriptors.log'\n",
    "trajectory_log = 'models/trajectories.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding/hidden dimensionality\n",
    "d_word = We.shape[1]\n",
    "d_char = 50\n",
    "d_book = 50\n",
    "d_hidden = 50\n",
    "\n",
    "# number of descriptors\n",
    "num_descs = 30\n",
    "\n",
    "# number of negative samples per relationship\n",
    "num_negs = 50\n",
    "\n",
    "# word dropout probability\n",
    "p_drop = 0.75\n",
    "\n",
    "n_epochs = 15\n",
    "lr = 0.001\n",
    "eps = 1e-6\n",
    "\n",
    "num_chars = len(cmap)\n",
    "num_books = len(bmap)\n",
    "num_traj = len(span_data)\n",
    "len_voc = len(wmap)\n",
    "revmap = {}\n",
    "\n",
    "for w in wmap:\n",
    "    revmap[wmap[w]] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 116 30 16414 56579 1378 20046\n"
     ]
    }
   ],
   "source": [
    "print (d_word, span_size, num_descs, len_voc, num_chars, num_books, num_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_spans = tf.placeholder(tf.int32, [1, span_size], name=\"input_spans\")\n",
    "\n",
    "input_neg = tf.placeholder(tf.int32, [num_negs, span_size], name=\"input_neg\")\n",
    "\n",
    "input_chars = tf.placeholder(tf.int32, [2, ], name=\"input_chars\")#\n",
    "\n",
    "input_book = tf.placeholder(tf.int32, [1, ], name=\"input_book\")\n",
    "\n",
    "input_currmask = tf.placeholder(tf.float32, [1, span_size], name=\"input_currmask\")\n",
    "\n",
    "input_dropmask = tf.placeholder(tf.float32, [1, span_size], name=\"input_dropmask\")\n",
    "\n",
    "input_negmask = tf.placeholder(tf.float32, [num_negs, span_size], name=\"input_negmask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_spans:0\", shape=(1, 116), dtype=int32) \n",
      " Tensor(\"input_neg:0\", shape=(50, 116), dtype=int32) \n",
      " Tensor(\"input_chars:0\", shape=(2,), dtype=int32) \n",
      " Tensor(\"input_book:0\", shape=(1,), dtype=int32) \n",
      " Tensor(\"input_currmask:0\", shape=(1, 116), dtype=float32) \n",
      " Tensor(\"input_dropmask:0\", shape=(1, 116), dtype=float32) \n",
      " Tensor(\"input_negmask:0\", shape=(50, 116), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print ( input_spans,'\\n', input_neg,'\\n', input_chars,'\\n', input_book,'\\n', input_currmask,'\\n', input_dropmask,'\\n', input_negmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_spans.get_shape()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=int(input_spans.get_shape()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EmbeddingLayer(E_w, input_spans, train=False):\n",
    "    W = tf.Variable( E_w,name=\"Embedding_W\",trainable=train)\n",
    "    embedding_spans = tf.nn.embedding_lookup(W,input_spans)\n",
    "    return embedding_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(1, 116, 300) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(tf.Variable( We,name=\"Emb\",trainable=False),input_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# negative examples should use same embedding matrix\n",
    "embedding_spans = EmbeddingLayer(We,input_spans)\n",
    "embedding_neg = EmbeddingLayer(We,input_neg)\n",
    "embedding_chars = EmbeddingLayer(tf.random_uniform([num_chars, d_char], -1.0, 1.0), input_chars)\n",
    "embedding_books = EmbeddingLayer(tf.random_uniform([num_books, d_book], -1.0, 1.0), input_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_1:0\", shape=(1, 116, 300), dtype=float32) \n",
      " Tensor(\"embedding_lookup_2:0\", shape=(50, 116, 300), dtype=float32) \n",
      " Tensor(\"embedding_lookup_3:0\", shape=(2, 50), dtype=float32) \n",
      " Tensor(\"embedding_lookup_4:0\", shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (embedding_spans,'\\n',embedding_neg,'\\n',embedding_chars,'\\n',embedding_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AverageLayer(emb , mask, d_word=300):\n",
    "    emb_average = tf.reduce_sum(emb * mask[:, :, None], 1,keep_dims=True)/tf.reduce_sum(input_currmask, 1,keep_dims=True)[:,None]\n",
    "    print (tf.reshape(emb_average,[-1,d_word]))\n",
    "    return tf.reshape(emb_average,[-1,d_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(1, 300), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(1, 300), dtype=float32)\n",
      "Tensor(\"Reshape_4:0\", shape=(50, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "emb_average = AverageLayer(embedding_spans,input_currmask,d_word)\n",
    "drop_average = AverageLayer(embedding_spans,input_dropmask,d_word)\n",
    "neg_average = AverageLayer(embedding_neg,input_negmask,d_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ConcatLayer(inputs, input_d, chars_d ,books_d, **kwargs):\n",
    "    W_i = tf.get_variable(\"W_i3\", shape=[input_d, input_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_i = tf.Variable(tf.constant(0.1, shape=[input_d,]), name=\"b_i\")\n",
    "    W_c = tf.get_variable(\"W_c3\", shape=[input_d, chars_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_b = tf.get_variable(\"W_b3\", shape=[input_d, books_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    input_vec = tf.nn.xw_plus_b(drop_average, W_i, b_i, name=\"input_vec\")\n",
    "    char_vec = tf.matmul(W_c,tf.transpose(tf.reduce_sum(embedding_chars,axis=0,keep_dims=True)),name=\"char_vec\")\n",
    "    book_vec = tf.matmul(W_b,tf.transpose(tf.reshape(embedding_books,[-1,d_book])),name=\"book_vec\")\n",
    "    print (tf.nn.relu(input_vec+tf.transpose(char_vec)+tf.transpose(book_vec)))\n",
    "    return tf.nn.relu(input_vec+tf.transpose(char_vec)+tf.transpose(book_vec),name=\"concat_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(1, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_concat=ConcatLayer([drop_average,embedding_chars,embedding_books],d_word,d_char,d_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_C = tf.get_variable(\"1\", shape=[d_word, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W_P = tf.get_variable(\"2\", shape=[num_descs, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "alpha = tf.constant(0.5,name='alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "previous_state  = tf.zeros([1, num_descs],name='previous_state') \n",
    "states=list()\n",
    "num_state=0\n",
    "for row in tf.unpack(input_concat):\n",
    "    state = previous_state = alpha*tf.nn.softmax(tf.matmul(tf.expand_dims(row,0),W_C)+tf.matmul(previous_state,W_P))+(1-alpha)*previous_state\n",
    "    states.append(state)\n",
    "    num_state+=1\n",
    "state=tf.pack(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_7:0' shape=(1, 30) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(state,[num_state,num_descs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_relu:0' shape=(1, 300) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RecurrnetRelationshipLayer(concat, d_word, d_hidden, num_descs):\n",
    "    W_C = tf.get_variable(\"W_C123\", shape=[d_word, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W_P = tf.get_variable(\"W_P123\", shape=[num_descs, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    alpha = tf.constant(0.5,name='alpha')\n",
    "    previous_state  = tf.zeros([int(concat.get_shape()[0]), num_descs],name='previous_state') \n",
    "    state = previous_state = alpha*tf.nn.softmax(tf.matmul(input_concat,W_C)+tf.matmul(previous_state,W_P))+(1-alpha)*previous_state\n",
    "    print (state,'\\n',input_concat,'\\n',previous_state,'\\n',W_C, '\\n',W_P)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_7:0\", shape=(1, 30), dtype=float32) \n",
      " Tensor(\"concat_relu:0\", shape=(1, 300), dtype=float32) \n",
      " Tensor(\"add_7:0\", shape=(1, 30), dtype=float32) \n",
      " Tensor(\"W_C123/read:0\", shape=(300, 30), dtype=float32) \n",
      " Tensor(\"W_P123/read:0\", shape=(30, 30), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_recu = RecurrnetRelationshipLayer(input_concat,d_word,d_hidden,num_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReconstructionLayer(recu,num_descs,d_word):\n",
    "    W_R = tf.get_variable(\"desciptor_R\", shape=[num_descs, d_word], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    recon = tf.matmul(recu,W_R,name='recon')\n",
    "    print(recon, W_R)\n",
    "    return recon, W_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"recon:0\", shape=(1, 300), dtype=float32) Tensor(\"desciptor_R/read:0\", shape=(30, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_recon, desciptor_R = ReconstructionLayer(input_recu,num_descs,d_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a8d11fdf7463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesciptor_R\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    513\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minitialized_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3617\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3619\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   3620\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3621\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "np.array(desciptor_R.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"l2_normalize:0\", shape=(1, 300), dtype=float32) \n",
      " Tensor(\"l2_normalize_1:0\", shape=(50, 300), dtype=float32) \n",
      " Tensor(\"l2_normalize_2:0\", shape=(1, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "emb = tf.nn.l2_normalize(emb_average,1)\n",
    "neg = tf.nn.l2_normalize(neg_average,1)\n",
    "recon = tf.nn.l2_normalize(input_recon,1)\n",
    "print(emb,'\\n',neg,'\\n',recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(emb,neg,recon):\n",
    "    Hinge_loss = tf.reduce_sum(1.-tf.reduce_sum( recon*emb, 1,keep_dims=True)+tf.matmul(recon,tf.transpose(neg)), 1, keep_dims=True)\n",
    "    return Hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = hinge_loss(emb, neg, recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum_8:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalty(desciptor_R, eps):\n",
    "    norm_R = tf.nn.l2_normalize(desciptor_R,1)\n",
    "    ortho_penalty = eps * tf.reduce_sum((tf.matmul(norm_R,norm_R,transpose_b=True) - tf.eye(num_descs))**2)\n",
    "    return ortho_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss += penalty(desciptor_R, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_9:0' shape=(1, 1) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py, random, csv, gzip, time \n",
    "import pickle\n",
    "from util import *\n",
    "import os\n",
    "class RMN(object):\n",
    "\tdef __init__(self, d_word, d_char, d_book, d_hidden, len_voc, num_descs, num_chars, num_books, span_size, We, freeze_words=True, eps=1e-5, lr=0.01, negs=10 ):\n",
    "\t\tself.d_word = d_word\n",
    "\t\tself.d_char = d_char\n",
    "\t\tself.d_book = d_book\n",
    "\t\tself.d_hidden = d_hidden\n",
    "\t\tself.len_voc = len_voc\n",
    "\t\tself.num_descs = num_descs\n",
    "\t\tself.num_chars = num_chars\n",
    "\t\tself.num_books = num_books\n",
    "\t\tself.span_size = span_size\n",
    "\t\tself.We = We\n",
    "\t\tself.freeze_words = freeze_words\n",
    "\t\tself.eps = eps\n",
    "\t\tself.lr = lr\n",
    "\t\tself.num_negs = negs\n",
    "        \n",
    "\t\tself.input_spans = tf.placeholder(tf.int32, [1, self.span_size], name=\"input_spans\")\n",
    "\n",
    "\t\tself.input_neg = tf.placeholder(tf.int32, [self.num_negs, self.span_size], name=\"input_neg\")\n",
    "\n",
    "\t\tself.input_chars = tf.placeholder(tf.int32, [2, ], name=\"input_chars\")#\n",
    "\n",
    "\t\tself.input_book = tf.placeholder(tf.int32, [1, ], name=\"input_book\")\n",
    "\n",
    "\t\tself.input_currmask = tf.placeholder(tf.float32, [1, self.span_size], name=\"input_currmask\")\n",
    "\n",
    "\t\tself.input_dropmask = tf.placeholder(tf.float32, [1, self.span_size], name=\"input_dropmask\")\n",
    "\n",
    "\t\tself.input_negmask = tf.placeholder(tf.float32, [self.num_negs, self.span_size], name=\"input_negmask\")\n",
    "\n",
    "\t\tprint ( self.input_spans,'\\n', self.input_neg,'\\n', self.input_chars,'\\n', self.input_book,'\\n', self.input_currmask,'\\n', self.input_dropmask,'\\n', self.input_negmask)\n",
    "\n",
    "\t\tself.loss = self._RMN_NETWORK()\n",
    "        \n",
    "\tdef EmbeddingLayer(self, E_w, input_spans, train=False):\n",
    "\t\tW = tf.Variable( E_w,name=\"Embedding_W\",trainable=train)\n",
    "\t\tembedding_spans = tf.nn.embedding_lookup(W,input_spans)\n",
    "\t\treturn embedding_spans\n",
    "\t\t\n",
    "\tdef AverageLayer(self, emb , mask, d_word=300):\n",
    "\t\temb_average = tf.reduce_sum(emb * mask[:, :, None], 1,keep_dims=True)/tf.reduce_sum(mask, 1,keep_dims=True)[:,None]\n",
    "\t\tprint (tf.reshape(emb_average,[-1,d_word]))\n",
    "\t\treturn tf.reshape(emb_average,[-1,d_word])\n",
    "\t\t\n",
    "\tdef ConcatLayer(self, inputs, input_d, chars_d ,books_d, **kwargs):\n",
    "\t\tW_i = tf.get_variable(\"W_i\", shape=[input_d, input_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\tb_i = tf.Variable(tf.constant(0.1, shape=[input_d,]), name=\"b_i\")\n",
    "\t\tW_c = tf.get_variable(\"W_c\", shape=[input_d, chars_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\tW_b = tf.get_variable(\"W_b\", shape=[input_d, books_d], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\tinput_vec = tf.nn.xw_plus_b(inputs[0], W_i, b_i, name=\"input_vec\")\n",
    "\t\tchar_vec = tf.matmul(W_c,tf.transpose(tf.reduce_sum(inputs[1],axis=0,keep_dims=True)),name=\"char_vec\")\n",
    "\t\tbook_vec = tf.matmul(W_b,tf.transpose(tf.reshape(inputs[2],[-1,books_d])),name=\"book_vec\")\n",
    "\t\tprint (tf.nn.relu(input_vec+tf.transpose(char_vec)+tf.transpose(book_vec)))\n",
    "\t\treturn tf.nn.relu(input_vec+tf.transpose(char_vec)+tf.transpose(book_vec),name=\"concat_relu\")\n",
    "\t\t\n",
    "\tdef RecurrnetRelationshipLayer(self, concat, d_word, d_hidden, num_descs):\n",
    "\t\tW_C = tf.get_variable(\"W_C\", shape=[d_word, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\tW_P = tf.get_variable(\"W_P\", shape=[num_descs, num_descs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\talpha = tf.constant(0.5,name='alpha')\n",
    "\t\tprevious_state  = tf.zeros([1, num_descs],name='previous_state') \n",
    "\t\tstate = previous_state = alpha * tf.nn.softmax(tf.matmul(concat,W_C)+tf.matmul(previous_state,W_P))+(1-alpha)*previous_state\n",
    "\t\tprint (state,'\\n',concat,'\\n',previous_state,'\\n',W_C, '\\n',W_P)\n",
    "\t\treturn state\n",
    "\t\t\n",
    "\tdef ReconstructionLayer(self, recu,num_descs,d_word):\n",
    "\t\tW_R = tf.get_variable(\"R_desciptor\", shape=[num_descs, d_word], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\t\trecon = tf.matmul(recu,W_R,name='recon')\n",
    "\t\tprint( recon )\n",
    "\t\treturn recon, W_R\n",
    "\t\t\n",
    "\tdef hinge_loss(self, emb, neg, recon):\n",
    "\t\tHinge_loss = tf.reduce_sum(1.-tf.reduce_sum( recon*emb, 1,keep_dims=True)+tf.matmul(recon,tf.transpose(neg)), 1, keep_dims=True)\n",
    "\t\treturn Hinge_loss\n",
    "\t\t\n",
    "\tdef penalty(self, desciptor_R, eps):\n",
    "\t\tnorm_R = tf.nn.l2_normalize(desciptor_R,1)\n",
    "\t\tortho_penalty = eps * tf.reduce_sum((tf.matmul(norm_R,norm_R,transpose_b=True) - tf.eye(num_descs))**2)\n",
    "\t\treturn ortho_penalty\n",
    "\t\t\n",
    "\tdef _RMN_NETWORK(self):\n",
    "\t# negative examples should use same embedding matrix\n",
    "\t\twith tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "\t\t\tembedding_spans = self.EmbeddingLayer(self.We, self.input_spans)\n",
    "\t\t\tembedding_neg = self.EmbeddingLayer(self.We, self.input_neg)\n",
    "\t\t\tembedding_chars = self.EmbeddingLayer(tf.random_uniform([num_chars, d_char], -1.0, 1.0), self.input_chars)\n",
    "\t\t\tembedding_books = self.EmbeddingLayer(tf.random_uniform([num_books, d_book], -1.0, 1.0), self.input_book)\n",
    "\t\t\t\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"AverageLayer\" ):\n",
    "\t\t\temb_average = self.AverageLayer(embedding_spans, self.input_currmask, self.d_word)\n",
    "\t\t\tdrop_average = self.AverageLayer(embedding_spans, self.input_dropmask ,self.d_word)\n",
    "\t\t\tneg_average = self.AverageLayer(embedding_neg, self.input_negmask, self.d_word)\n",
    "\t\t\t\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"ConcatLayer\" ):\n",
    "\t\t\tinput_concat = self.ConcatLayer([drop_average,embedding_chars,embedding_books], self.d_word, self.d_char, self.d_book)\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"RecurrnetRelationshipLayer\" ):\n",
    "\t\t\tinput_recu = self.RecurrnetRelationshipLayer(input_concat, self.d_word, self.d_hidden, self.num_descs)\n",
    "\t\t\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"ReconstructionLayer\"):\n",
    "\t\t\tinput_recon, desciptor_R = self.ReconstructionLayer(input_recu, self.num_descs, self.d_word)\n",
    "\t\t\t#print(\"recon\",input_recon)\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"l2_normalize\"):\n",
    "\t\t\temb = tf.nn.l2_normalize(emb_average,1)\n",
    "\t\t\tneg = tf.nn.l2_normalize(neg_average,1)\n",
    "\t\t\trecon = tf.nn.l2_normalize(input_recon,1)\n",
    "\t\t\tprint(emb,'\\n',neg,'\\n',recon)\n",
    "\t\twith tf.device('/gpu:0'), tf.name_scope(\"loss\"):\n",
    "\t\t\tloss = self.hinge_loss(emb, neg, recon)\n",
    "\t\t\tloss += self.penalty(desciptor_R, self.eps)\n",
    "\t\t\tprint (loss)\n",
    "\t\treturn loss\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n",
      "Tensor(\"input_spans:0\", shape=(?, 116), dtype=int32) \n",
      " Tensor(\"input_neg:0\", shape=(50, 116), dtype=int32) \n",
      " Tensor(\"input_chars:0\", shape=(2,), dtype=int32) \n",
      " Tensor(\"input_book:0\", shape=(1,), dtype=int32) \n",
      " Tensor(\"input_currmask:0\", shape=(?, 116), dtype=float32) \n",
      " Tensor(\"input_dropmask:0\", shape=(?, 116), dtype=float32) \n",
      " Tensor(\"input_negmask:0\", shape=(50, 116), dtype=float32)\n",
      "Tensor(\"l2_normalize/l2_normalize:0\", shape=(?, 300), dtype=float32, device=/device:GPU:0) \n",
      " Tensor(\"l2_normalize/l2_normalize_1:0\", shape=(50, 300), dtype=float32, device=/device:GPU:0) \n",
      " Tensor(\"l2_normalize/l2_normalize_2:0\", shape=(?, 300), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"loss/add_1:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "done compiling, now training...\n",
      "6\n",
      "2017-07-12T16:07:26.693418 : step 1 loss [[ 49.73849487]]\n",
      "6\n",
      "2017-07-12T16:07:26.721437 : step 2 loss [[ 50.02331161]]\n",
      "6\n",
      "2017-07-12T16:07:26.735447 : step 3 loss [[ 48.47849274]]\n",
      "6\n",
      "2017-07-12T16:07:26.740451 : step 4 loss [[ 51.26253128]]\n",
      "6\n",
      "2017-07-12T16:07:26.745455 : step 5 loss [[ 47.34350967]]\n",
      "6\n",
      "2017-07-12T16:07:26.751459 : step 6 loss [[ 47.03477478]]\n",
      "5\n",
      "2017-07-12T16:07:26.759464 : step 7 loss [[ 45.90806961]]\n",
      "5\n",
      "2017-07-12T16:07:26.764468 : step 8 loss [[ 47.43865967]]\n",
      "5\n",
      "2017-07-12T16:07:26.769973 : step 9 loss [[ 45.80504227]]\n",
      "5\n",
      "2017-07-12T16:07:26.774977 : step 10 loss [[ 45.6223793]]\n",
      "5\n",
      "2017-07-12T16:07:26.780995 : step 11 loss [[ 47.34014893]]\n",
      "11\n",
      "2017-07-12T16:07:26.786495 : step 12 loss [[ 49.40141296]]\n",
      "11\n",
      "2017-07-12T16:07:26.791491 : step 13 loss [[ 49.14431763]]\n",
      "11\n",
      "2017-07-12T16:07:26.796495 : step 14 loss [[ 48.67103195]]\n",
      "11\n",
      "2017-07-12T16:07:26.802498 : step 15 loss [[ 47.62262726]]\n",
      "11\n",
      "2017-07-12T16:07:26.807502 : step 16 loss [[ 48.63695145]]\n",
      "11\n",
      "2017-07-12T16:07:26.813507 : step 17 loss [[ 47.59419632]]\n",
      "11\n",
      "2017-07-12T16:07:26.819511 : step 18 loss [[ 45.46210098]]\n",
      "11\n",
      "2017-07-12T16:07:26.824514 : step 19 loss [[ 49.14276123]]\n",
      "11\n",
      "2017-07-12T16:07:26.829518 : step 20 loss [[ 48.14560699]]\n",
      "11\n",
      "2017-07-12T16:07:26.834522 : step 21 loss [[ 47.45449829]]\n",
      "11\n",
      "2017-07-12T16:07:26.839525 : step 22 loss [[ 47.06238174]]\n",
      "30\n",
      "2017-07-12T16:07:26.845532 : step 23 loss [[ 46.40823364]]\n",
      "30\n",
      "2017-07-12T16:07:26.850532 : step 24 loss [[ 45.7800293]]\n",
      "30\n",
      "2017-07-12T16:07:26.855536 : step 25 loss [[ 47.54713821]]\n",
      "30\n",
      "2017-07-12T16:07:26.860539 : step 26 loss [[ 50.70877457]]\n",
      "30\n",
      "2017-07-12T16:07:26.864543 : step 27 loss [[ 47.81663895]]\n",
      "30\n",
      "2017-07-12T16:07:26.870049 : step 28 loss [[ 46.99938583]]\n",
      "30\n",
      "2017-07-12T16:07:26.875553 : step 29 loss [[ 47.00265121]]\n",
      "30\n",
      "2017-07-12T16:07:26.880056 : step 30 loss [[ 50.03158951]]\n",
      "30\n",
      "2017-07-12T16:07:26.885059 : step 31 loss [[ 48.16493988]]\n",
      "30\n",
      "2017-07-12T16:07:26.889564 : step 32 loss [[ 49.8889122]]\n",
      "30\n",
      "2017-07-12T16:07:26.894567 : step 33 loss [[ 48.63389587]]\n",
      "30\n",
      "2017-07-12T16:07:26.899571 : step 34 loss [[ 47.46882248]]\n",
      "30\n",
      "2017-07-12T16:07:26.904575 : step 35 loss [[ 47.18354416]]\n",
      "30\n",
      "2017-07-12T16:07:26.909578 : step 36 loss [[ 46.19787216]]\n",
      "30\n",
      "2017-07-12T16:07:26.914582 : step 37 loss [[ 46.93099594]]\n",
      "30\n",
      "2017-07-12T16:07:26.920586 : step 38 loss [[ 50.88890457]]\n",
      "30\n",
      "2017-07-12T16:07:26.925590 : step 39 loss [[ 51.73347855]]\n",
      "30\n",
      "2017-07-12T16:07:26.930593 : step 40 loss [[ 50.24250412]]\n",
      "30\n",
      "2017-07-12T16:07:26.937598 : step 41 loss [[ 49.22130203]]\n",
      "30\n",
      "2017-07-12T16:07:26.943603 : step 42 loss [[ 45.74860764]]\n",
      "30\n",
      "2017-07-12T16:07:26.948606 : step 43 loss [[ 44.88542557]]\n",
      "30\n",
      "2017-07-12T16:07:26.954610 : step 44 loss [[ 48.59188461]]\n",
      "30\n",
      "2017-07-12T16:07:26.959614 : step 45 loss [[ 46.52480316]]\n",
      "30\n",
      "2017-07-12T16:07:26.964617 : step 46 loss [[ 44.66420364]]\n",
      "30\n",
      "2017-07-12T16:07:26.970123 : step 47 loss [[ 43.71512985]]\n",
      "30\n",
      "2017-07-12T16:07:26.975627 : step 48 loss [[ 42.19203949]]\n",
      "30\n",
      "2017-07-12T16:07:26.980130 : step 49 loss [[ 42.9616127]]\n",
      "30\n",
      "2017-07-12T16:07:26.985133 : step 50 loss [[ 43.33616638]]\n",
      "30\n",
      "2017-07-12T16:07:26.989638 : step 51 loss [[ 44.35173035]]\n",
      "30\n",
      "2017-07-12T16:07:26.994641 : step 52 loss [[ 47.6904335]]\n",
      "6\n",
      "2017-07-12T16:07:26.999645 : step 53 loss [[ 51.76496124]]\n",
      "6\n",
      "2017-07-12T16:07:27.004649 : step 54 loss [[ 53.40930557]]\n",
      "6\n",
      "2017-07-12T16:07:27.009653 : step 55 loss [[ 51.34283066]]\n",
      "6\n",
      "2017-07-12T16:07:27.014655 : step 56 loss [[ 52.61808777]]\n",
      "6\n",
      "2017-07-12T16:07:27.019659 : step 57 loss [[ 50.58654022]]\n",
      "6\n",
      "2017-07-12T16:07:27.024663 : step 58 loss [[ 49.82392502]]\n",
      "30\n",
      "2017-07-12T16:07:27.029667 : step 59 loss [[ 49.74981689]]\n",
      "30\n",
      "2017-07-12T16:07:27.034670 : step 60 loss [[ 49.16356277]]\n",
      "30\n",
      "2017-07-12T16:07:27.039673 : step 61 loss [[ 48.51691818]]\n",
      "30\n",
      "2017-07-12T16:07:27.044677 : step 62 loss [[ 46.42976761]]\n",
      "30\n",
      "2017-07-12T16:07:27.048680 : step 63 loss [[ 47.76906204]]\n",
      "30\n",
      "2017-07-12T16:07:27.053683 : step 64 loss [[ 46.91900253]]\n",
      "30\n",
      "2017-07-12T16:07:27.058687 : step 65 loss [[ 44.95984268]]\n",
      "30\n",
      "2017-07-12T16:07:27.063690 : step 66 loss [[ 45.74503708]]\n",
      "30\n",
      "2017-07-12T16:07:27.068694 : step 67 loss [[ 47.88271713]]\n",
      "30\n",
      "2017-07-12T16:07:27.073700 : step 68 loss [[ 49.6856041]]\n",
      "30\n",
      "2017-07-12T16:07:27.078703 : step 69 loss [[ 52.41794205]]\n",
      "30\n",
      "2017-07-12T16:07:27.083206 : step 70 loss [[ 52.71733475]]\n",
      "30\n",
      "2017-07-12T16:07:27.088261 : step 71 loss [[ 53.68160629]]\n",
      "30\n",
      "2017-07-12T16:07:27.093265 : step 72 loss [[ 52.81392288]]\n",
      "30\n",
      "2017-07-12T16:07:27.097268 : step 73 loss [[ 52.5690155]]\n",
      "30\n",
      "2017-07-12T16:07:27.103272 : step 74 loss [[ 49.48356247]]\n",
      "30\n",
      "2017-07-12T16:07:27.108276 : step 75 loss [[ 54.05778122]]\n",
      "30\n",
      "2017-07-12T16:07:27.112278 : step 76 loss [[ 51.87137604]]\n",
      "30\n",
      "2017-07-12T16:07:27.117282 : step 77 loss [[ 50.93138504]]\n",
      "30\n",
      "2017-07-12T16:07:27.122286 : step 78 loss [[ 45.58128738]]\n",
      "30\n",
      "2017-07-12T16:07:27.127289 : step 79 loss [[ 44.31067657]]\n",
      "30\n",
      "2017-07-12T16:07:27.133294 : step 80 loss [[ 47.25318527]]\n",
      "30\n",
      "2017-07-12T16:07:27.138297 : step 81 loss [[ 46.57453918]]\n",
      "30\n",
      "2017-07-12T16:07:27.143301 : step 82 loss [[ 46.07821274]]\n",
      "30\n",
      "2017-07-12T16:07:27.148304 : step 83 loss [[ 46.2100029]]\n",
      "30\n",
      "2017-07-12T16:07:27.153308 : step 84 loss [[ 46.83224106]]\n",
      "30\n",
      "2017-07-12T16:07:27.159312 : step 85 loss [[ 46.2159462]]\n",
      "30\n",
      "2017-07-12T16:07:27.163315 : step 86 loss [[ 46.2144928]]\n",
      "30\n",
      "2017-07-12T16:07:27.168318 : step 87 loss [[ 47.63476181]]\n",
      "30\n",
      "2017-07-12T16:07:27.173824 : step 88 loss [[ 46.06373596]]\n",
      "79\n",
      "2017-07-12T16:07:27.178828 : step 89 loss [[ 46.87337112]]\n",
      "79\n",
      "2017-07-12T16:07:27.183331 : step 90 loss [[ 45.26119232]]\n",
      "79\n",
      "2017-07-12T16:07:27.187838 : step 91 loss [[ 46.80387497]]\n",
      "79\n",
      "2017-07-12T16:07:27.192841 : step 92 loss [[ 45.14719009]]\n",
      "79\n",
      "2017-07-12T16:07:27.197844 : step 93 loss [[ 45.75521851]]\n",
      "79\n",
      "2017-07-12T16:07:27.202848 : step 94 loss [[ 47.26406479]]\n",
      "79\n",
      "2017-07-12T16:07:27.207852 : step 95 loss [[ 48.44026566]]\n",
      "79\n",
      "2017-07-12T16:07:27.211855 : step 96 loss [[ 49.28821182]]\n",
      "79\n",
      "2017-07-12T16:07:27.216858 : step 97 loss [[ 50.66696167]]\n",
      "79\n",
      "2017-07-12T16:07:27.221862 : step 98 loss [[ 44.15763092]]\n",
      "79\n",
      "2017-07-12T16:07:27.226865 : step 99 loss [[ 43.10393143]]\n",
      "79\n",
      "2017-07-12T16:07:27.231869 : step 100 loss [[ 43.47156525]]\n",
      "79\n",
      "2017-07-12T16:07:27.236873 : step 101 loss [[ 44.19917679]]\n",
      "79\n",
      "2017-07-12T16:07:27.240875 : step 102 loss [[ 42.43002701]]\n",
      "79\n",
      "2017-07-12T16:07:27.245879 : step 103 loss [[ 42.17555618]]\n",
      "79\n",
      "2017-07-12T16:07:27.250882 : step 104 loss [[ 41.97390366]]\n",
      "79\n",
      "2017-07-12T16:07:27.255886 : step 105 loss [[ 38.97248077]]\n",
      "79\n",
      "2017-07-12T16:07:27.260889 : step 106 loss [[ 41.79937363]]\n",
      "79\n",
      "2017-07-12T16:07:27.264892 : step 107 loss [[ 42.76773071]]\n",
      "79\n",
      "2017-07-12T16:07:27.270899 : step 108 loss [[ 39.77384567]]\n",
      "79\n",
      "2017-07-12T16:07:27.275402 : step 109 loss [[ 44.25901413]]\n",
      "79\n",
      "2017-07-12T16:07:27.280906 : step 110 loss [[ 40.27328491]]\n",
      "79\n",
      "2017-07-12T16:07:27.285910 : step 111 loss [[ 38.23760605]]\n",
      "79\n",
      "2017-07-12T16:07:27.290916 : step 112 loss [[ 41.36293793]]\n",
      "79\n",
      "2017-07-12T16:07:27.295919 : step 113 loss [[ 45.41623306]]\n",
      "79\n",
      "2017-07-12T16:07:27.300923 : step 114 loss [[ 41.37353897]]\n",
      "79\n",
      "2017-07-12T16:07:27.305926 : step 115 loss [[ 42.34608078]]\n",
      "79\n",
      "2017-07-12T16:07:27.311931 : step 116 loss [[ 41.42970276]]\n",
      "79\n",
      "2017-07-12T16:07:27.316934 : step 117 loss [[ 41.56394577]]\n",
      "79\n",
      "2017-07-12T16:07:27.321938 : step 118 loss [[ 41.71435165]]\n",
      "79\n",
      "2017-07-12T16:07:27.326941 : step 119 loss [[ 42.67306137]]\n",
      "79\n",
      "2017-07-12T16:07:27.331945 : step 120 loss [[ 41.04375839]]\n",
      "79\n",
      "2017-07-12T16:07:27.336948 : step 121 loss [[ 41.72249985]]\n",
      "79\n",
      "2017-07-12T16:07:27.341952 : step 122 loss [[ 41.3037529]]\n",
      "79\n",
      "2017-07-12T16:07:27.347956 : step 123 loss [[ 41.39806747]]\n",
      "79\n",
      "2017-07-12T16:07:27.352960 : step 124 loss [[ 44.31813049]]\n",
      "79\n",
      "2017-07-12T16:07:27.357963 : step 125 loss [[ 43.80595016]]\n",
      "79\n",
      "2017-07-12T16:07:27.362967 : step 126 loss [[ 42.48511887]]\n",
      "79\n",
      "2017-07-12T16:07:27.367970 : step 127 loss [[ 44.03020859]]\n",
      "79\n",
      "2017-07-12T16:07:27.373977 : step 128 loss [[ 47.67670059]]\n",
      "79\n",
      "2017-07-12T16:07:27.378980 : step 129 loss [[ 48.85999298]]\n",
      "79\n",
      "2017-07-12T16:07:27.384485 : step 130 loss [[ 48.15052414]]\n",
      "79\n",
      "2017-07-12T16:07:27.388990 : step 131 loss [[ 46.3975029]]\n",
      "79\n",
      "2017-07-12T16:07:27.393994 : step 132 loss [[ 44.28230667]]\n",
      "79\n",
      "2017-07-12T16:07:27.398998 : step 133 loss [[ 41.08549118]]\n",
      "79\n",
      "2017-07-12T16:07:27.404001 : step 134 loss [[ 39.47595596]]\n",
      "79\n",
      "2017-07-12T16:07:27.409005 : step 135 loss [[ 37.22124863]]\n",
      "79\n",
      "2017-07-12T16:07:27.415009 : step 136 loss [[ 43.70233536]]\n",
      "79\n",
      "2017-07-12T16:07:27.420019 : step 137 loss [[ 46.40298843]]\n",
      "79\n",
      "2017-07-12T16:07:27.425016 : step 138 loss [[ 44.70391464]]\n",
      "79\n",
      "2017-07-12T16:07:27.430020 : step 139 loss [[ 41.02408981]]\n",
      "79\n",
      "2017-07-12T16:07:27.435023 : step 140 loss [[ 37.7869072]]\n",
      "79\n",
      "2017-07-12T16:07:27.440027 : step 141 loss [[ 36.69257355]]\n",
      "79\n",
      "2017-07-12T16:07:27.445030 : step 142 loss [[ 38.02843857]]\n",
      "79\n",
      "2017-07-12T16:07:27.450034 : step 143 loss [[ 38.31007385]]\n",
      "79\n",
      "2017-07-12T16:07:27.455038 : step 144 loss [[ 39.48816299]]\n",
      "79\n",
      "2017-07-12T16:07:27.460041 : step 145 loss [[ 38.10232544]]\n",
      "79\n",
      "2017-07-12T16:07:27.465044 : step 146 loss [[ 39.72590256]]\n",
      "79\n",
      "2017-07-12T16:07:27.471051 : step 147 loss [[ 40.53027725]]\n",
      "79\n",
      "2017-07-12T16:07:27.476055 : step 148 loss [[ 40.30791473]]\n",
      "79\n",
      "2017-07-12T16:07:27.481058 : step 149 loss [[ 44.03110886]]\n",
      "79\n",
      "2017-07-12T16:07:27.486954 : step 150 loss [[ 43.13540268]]\n",
      "79\n",
      "2017-07-12T16:07:27.491958 : step 151 loss [[ 45.24106216]]\n",
      "79\n",
      "2017-07-12T16:07:27.496961 : step 152 loss [[ 39.85273361]]\n",
      "79\n",
      "2017-07-12T16:07:27.501965 : step 153 loss [[ 39.04314041]]\n",
      "79\n",
      "2017-07-12T16:07:27.506969 : step 154 loss [[ 39.68079758]]\n",
      "79\n",
      "2017-07-12T16:07:27.511972 : step 155 loss [[ 39.44589233]]\n",
      "79\n",
      "2017-07-12T16:07:27.516975 : step 156 loss [[ 39.26910019]]\n",
      "79\n",
      "2017-07-12T16:07:27.521979 : step 157 loss [[ 38.82729721]]\n",
      "79\n",
      "2017-07-12T16:07:27.526983 : step 158 loss [[ 37.25080109]]\n",
      "79\n",
      "2017-07-12T16:07:27.532987 : step 159 loss [[ 34.42981339]]\n",
      "79\n",
      "2017-07-12T16:07:27.537991 : step 160 loss [[ 35.06660843]]\n",
      "79\n",
      "2017-07-12T16:07:27.542994 : step 161 loss [[ 34.85509109]]\n",
      "79\n",
      "2017-07-12T16:07:27.547998 : step 162 loss [[ 35.7569809]]\n",
      "79\n",
      "2017-07-12T16:07:27.555010 : step 163 loss [[ 35.76190186]]\n",
      "79\n",
      "2017-07-12T16:07:27.560006 : step 164 loss [[ 36.28837204]]\n",
      "79\n",
      "2017-07-12T16:07:27.565010 : step 165 loss [[ 35.25976562]]\n",
      "79\n",
      "2017-07-12T16:07:27.571015 : step 166 loss [[ 32.70257187]]\n",
      "79\n",
      "2017-07-12T16:07:27.575519 : step 167 loss [[ 29.79740715]]\n",
      "9\n",
      "2017-07-12T16:07:27.580522 : step 168 loss [[ 41.83195496]]\n",
      "9\n",
      "2017-07-12T16:07:27.585526 : step 169 loss [[ 41.24156189]]\n",
      "9\n",
      "2017-07-12T16:07:27.590638 : step 170 loss [[ 44.2231102]]\n",
      "9\n",
      "2017-07-12T16:07:27.595649 : step 171 loss [[ 47.25052643]]\n",
      "9\n",
      "2017-07-12T16:07:27.600646 : step 172 loss [[ 49.42892075]]\n",
      "9\n",
      "2017-07-12T16:07:27.605649 : step 173 loss [[ 51.04169464]]\n",
      "9\n",
      "2017-07-12T16:07:27.610653 : step 174 loss [[ 49.09086227]]\n",
      "9\n",
      "2017-07-12T16:07:27.615656 : step 175 loss [[ 54.58733749]]\n",
      "9\n",
      "2017-07-12T16:07:27.620660 : step 176 loss [[ 46.64336014]]\n",
      "6\n",
      "2017-07-12T16:07:27.626664 : step 177 loss [[ 46.79508591]]\n",
      "6\n",
      "2017-07-12T16:07:27.631667 : step 178 loss [[ 46.28201294]]\n",
      "6\n",
      "2017-07-12T16:07:27.636671 : step 179 loss [[ 43.48923874]]\n",
      "6\n",
      "2017-07-12T16:07:27.641675 : step 180 loss [[ 43.83813477]]\n",
      "6\n",
      "2017-07-12T16:07:27.646678 : step 181 loss [[ 55.17074585]]\n",
      "6\n",
      "2017-07-12T16:07:27.651682 : step 182 loss [[ 36.69054031]]\n",
      "9\n",
      "2017-07-12T16:07:27.656686 : step 183 loss [[ 44.82540894]]\n",
      "9\n",
      "2017-07-12T16:07:27.662690 : step 184 loss [[ 48.19453812]]\n",
      "9\n",
      "2017-07-12T16:07:27.667693 : step 185 loss [[ 50.75342941]]\n",
      "9\n",
      "2017-07-12T16:07:27.673201 : step 186 loss [[ 47.49222183]]\n",
      "9\n",
      "2017-07-12T16:07:27.678205 : step 187 loss [[ 37.14586258]]\n",
      "9\n",
      "2017-07-12T16:07:27.683209 : step 188 loss [[ 32.37685013]]\n",
      "9\n",
      "2017-07-12T16:07:27.688214 : step 189 loss [[ 33.94372177]]\n",
      "9\n",
      "2017-07-12T16:07:27.693218 : step 190 loss [[ 34.0655899]]\n",
      "9\n",
      "2017-07-12T16:07:27.698221 : step 191 loss [[ 33.41644287]]\n",
      "5\n",
      "2017-07-12T16:07:27.703225 : step 192 loss [[ 50.19730759]]\n",
      "5\n",
      "2017-07-12T16:07:27.708228 : step 193 loss [[ 50.31490326]]\n",
      "5\n",
      "2017-07-12T16:07:27.713232 : step 194 loss [[ 56.12741089]]\n",
      "5\n",
      "2017-07-12T16:07:27.718235 : step 195 loss [[ 56.20067215]]\n",
      "5\n",
      "2017-07-12T16:07:27.723239 : step 196 loss [[ 55.71534729]]\n",
      "14\n",
      "2017-07-12T16:07:27.728243 : step 197 loss [[ 59.62539673]]\n",
      "14\n",
      "2017-07-12T16:07:27.733246 : step 198 loss [[ 53.40086365]]\n",
      "14\n",
      "2017-07-12T16:07:27.738250 : step 199 loss [[ 47.91315079]]\n",
      "14\n",
      "2017-07-12T16:07:27.743253 : step 200 loss [[ 45.92070007]]\n",
      "14\n",
      "2017-07-12T16:07:27.749258 : step 201 loss [[ 57.84472656]]\n",
      "14\n",
      "2017-07-12T16:07:27.754261 : step 202 loss [[ 57.91040039]]\n",
      "14\n",
      "2017-07-12T16:07:27.760265 : step 203 loss [[ 55.4656105]]\n",
      "14\n",
      "2017-07-12T16:07:27.765269 : step 204 loss [[ 53.10256577]]\n",
      "14\n",
      "2017-07-12T16:07:27.770273 : step 205 loss [[ 49.64113998]]\n",
      "14\n",
      "2017-07-12T16:07:27.774778 : step 206 loss [[ 50.06930542]]\n",
      "14\n",
      "2017-07-12T16:07:27.779781 : step 207 loss [[ 50.72874451]]\n",
      "14\n",
      "2017-07-12T16:07:27.784785 : step 208 loss [[ 49.15249252]]\n",
      "14\n",
      "2017-07-12T16:07:27.788969 : step 209 loss [[ 48.99480057]]\n",
      "14\n",
      "2017-07-12T16:07:27.793973 : step 210 loss [[ 48.78211212]]\n",
      "cost:  [[ 9575.38574219]]\n",
      "10\n",
      "2017-07-12T16:07:27.798976 : step 211 loss [[ 50.43013763]]\n",
      "10\n",
      "2017-07-12T16:07:27.803980 : step 212 loss [[ 54.43731308]]\n",
      "10\n",
      "2017-07-12T16:07:27.808983 : step 213 loss [[ 54.50401306]]\n",
      "10\n",
      "2017-07-12T16:07:27.813987 : step 214 loss [[ 54.96241379]]\n",
      "10\n",
      "2017-07-12T16:07:27.818994 : step 215 loss [[ 58.90145111]]\n",
      "10\n",
      "2017-07-12T16:07:27.823994 : step 216 loss [[ 56.26165009]]\n",
      "10\n",
      "2017-07-12T16:07:27.828997 : step 217 loss [[ 56.16213226]]\n",
      "10\n",
      "2017-07-12T16:07:27.834001 : step 218 loss [[ 53.62044525]]\n",
      "10\n",
      "2017-07-12T16:07:27.838004 : step 219 loss [[ 50.58646774]]\n",
      "10\n",
      "2017-07-12T16:07:27.844013 : step 220 loss [[ 52.29024506]]\n",
      "cost:  [[ 10117.54199219]]\n",
      "10\n",
      "2017-07-12T16:07:27.850022 : step 221 loss [[ 47.68491364]]\n",
      "10\n",
      "2017-07-12T16:07:27.855016 : step 222 loss [[ 45.49072266]]\n",
      "10\n",
      "2017-07-12T16:07:27.860019 : step 223 loss [[ 38.2611618]]\n",
      "10\n",
      "2017-07-12T16:07:27.865031 : step 224 loss [[ 36.60554886]]\n",
      "10\n",
      "2017-07-12T16:07:27.871029 : step 225 loss [[ 35.31434631]]\n",
      "10\n",
      "2017-07-12T16:07:27.876033 : step 226 loss [[ 44.61820602]]\n",
      "10\n",
      "2017-07-12T16:07:27.881537 : step 227 loss [[ 42.61035919]]\n",
      "10\n",
      "2017-07-12T16:07:27.886039 : step 228 loss [[ 42.02641296]]\n",
      "10\n",
      "2017-07-12T16:07:27.891075 : step 229 loss [[ 39.86026764]]\n",
      "10\n",
      "2017-07-12T16:07:27.896079 : step 230 loss [[ 43.25077438]]\n",
      "cost:  [[ 10533.26367188]]\n",
      "6\n",
      "2017-07-12T16:07:27.901082 : step 231 loss [[ 47.61333466]]\n",
      "6\n",
      "2017-07-12T16:07:27.907087 : step 232 loss [[ 48.79115295]]\n",
      "6\n",
      "2017-07-12T16:07:27.912090 : step 233 loss [[ 49.16754913]]\n",
      "6\n",
      "2017-07-12T16:07:27.917094 : step 234 loss [[ 50.96948624]]\n",
      "6\n",
      "2017-07-12T16:07:27.922097 : step 235 loss [[ 47.37997818]]\n",
      "6\n",
      "2017-07-12T16:07:27.927100 : step 236 loss [[ 49.73471069]]\n",
      "5\n",
      "2017-07-12T16:07:27.932104 : step 237 loss [[ 50.09186172]]\n",
      "5\n",
      "2017-07-12T16:07:27.938109 : step 238 loss [[ 51.91267395]]\n",
      "5\n",
      "2017-07-12T16:07:27.944112 : step 239 loss [[ 52.1725769]]\n",
      "5\n",
      "2017-07-12T16:07:27.949116 : step 240 loss [[ 53.54508972]]\n",
      "5\n",
      "2017-07-12T16:07:27.954120 : step 241 loss [[ 52.20461655]]\n",
      "15\n",
      "2017-07-12T16:07:27.960125 : step 242 loss [[ 55.96389008]]\n",
      "15\n",
      "2017-07-12T16:07:27.967130 : step 243 loss [[ 49.98264313]]\n",
      "15\n",
      "2017-07-12T16:07:27.972135 : step 244 loss [[ 48.80591965]]\n",
      "15\n",
      "2017-07-12T16:07:27.977639 : step 245 loss [[ 48.65887833]]\n",
      "15\n",
      "2017-07-12T16:07:27.982142 : step 246 loss [[ 49.13540649]]\n",
      "15\n",
      "2017-07-12T16:07:27.987165 : step 247 loss [[ 47.29774094]]\n",
      "15\n",
      "2017-07-12T16:07:27.992169 : step 248 loss [[ 50.31830978]]\n",
      "15\n",
      "2017-07-12T16:07:27.997173 : step 249 loss [[ 45.09468842]]\n",
      "15\n",
      "2017-07-12T16:07:28.002176 : step 250 loss [[ 51.71772766]]\n",
      "15\n",
      "2017-07-12T16:07:28.007180 : step 251 loss [[ 54.20643616]]\n",
      "15\n",
      "2017-07-12T16:07:28.011183 : step 252 loss [[ 52.69139481]]\n",
      "15\n",
      "2017-07-12T16:07:28.017188 : step 253 loss [[ 52.87757111]]\n",
      "15\n",
      "2017-07-12T16:07:28.021190 : step 254 loss [[ 49.82651901]]\n",
      "15\n",
      "2017-07-12T16:07:28.026194 : step 255 loss [[ 49.01604462]]\n",
      "15\n",
      "2017-07-12T16:07:28.031197 : step 256 loss [[ 51.17858505]]\n",
      "12\n",
      "2017-07-12T16:07:28.036200 : step 257 loss [[ 49.84578323]]\n",
      "12\n",
      "2017-07-12T16:07:28.041204 : step 258 loss [[ 45.52120209]]\n",
      "12\n",
      "2017-07-12T16:07:28.046208 : step 259 loss [[ 41.78085327]]\n",
      "12\n",
      "2017-07-12T16:07:28.051211 : step 260 loss [[ 35.13982773]]\n",
      "12\n",
      "2017-07-12T16:07:28.056215 : step 261 loss [[ 39.04284668]]\n",
      "12\n",
      "2017-07-12T16:07:28.061218 : step 262 loss [[ 36.38602448]]\n",
      "12\n",
      "2017-07-12T16:07:28.066222 : step 263 loss [[ 40.80744171]]\n",
      "12\n",
      "2017-07-12T16:07:28.071225 : step 264 loss [[ 45.57081604]]\n",
      "12\n",
      "2017-07-12T16:07:28.076231 : step 265 loss [[ 42.35741425]]\n",
      "12\n",
      "2017-07-12T16:07:28.080734 : step 266 loss [[ 42.727314]]\n",
      "12\n",
      "2017-07-12T16:07:28.085737 : step 267 loss [[ 42.5924263]]\n",
      "12\n",
      "2017-07-12T16:07:28.090286 : step 268 loss [[ 46.48208237]]\n",
      "23\n",
      "2017-07-12T16:07:28.095289 : step 269 loss [[ 52.48015213]]\n",
      "23\n",
      "2017-07-12T16:07:28.100293 : step 270 loss [[ 54.38503265]]\n",
      "23\n",
      "2017-07-12T16:07:28.105297 : step 271 loss [[ 54.33709717]]\n",
      "23\n",
      "2017-07-12T16:07:28.110300 : step 272 loss [[ 49.90042114]]\n",
      "23\n",
      "2017-07-12T16:07:28.115304 : step 273 loss [[ 51.15951538]]\n",
      "23\n",
      "2017-07-12T16:07:28.120307 : step 274 loss [[ 53.43092346]]\n",
      "23\n",
      "2017-07-12T16:07:28.125311 : step 275 loss [[ 51.35955811]]\n",
      "23\n",
      "2017-07-12T16:07:28.130314 : step 276 loss [[ 50.68380737]]\n",
      "23\n",
      "2017-07-12T16:07:28.135318 : step 277 loss [[ 49.16052246]]\n",
      "23\n",
      "2017-07-12T16:07:28.139321 : step 278 loss [[ 50.38071442]]\n",
      "23\n",
      "2017-07-12T16:07:28.145325 : step 279 loss [[ 51.90335846]]\n",
      "23\n",
      "2017-07-12T16:07:28.150329 : step 280 loss [[ 51.16233063]]\n",
      "23\n",
      "2017-07-12T16:07:28.155333 : step 281 loss [[ 50.17199707]]\n",
      "23\n",
      "2017-07-12T16:07:28.160336 : step 282 loss [[ 49.77148438]]\n",
      "23\n",
      "2017-07-12T16:07:28.166341 : step 283 loss [[ 47.25640106]]\n",
      "23\n",
      "2017-07-12T16:07:28.171343 : step 284 loss [[ 48.31472778]]\n",
      "23\n",
      "2017-07-12T16:07:28.176349 : step 285 loss [[ 49.33980179]]\n",
      "23\n",
      "2017-07-12T16:07:28.181353 : step 286 loss [[ 50.23007965]]\n",
      "23\n",
      "2017-07-12T16:07:28.186356 : step 287 loss [[ 51.29331207]]\n",
      "23\n",
      "2017-07-12T16:07:28.191649 : step 288 loss [[ 50.38245392]]\n",
      "23\n",
      "2017-07-12T16:07:28.196652 : step 289 loss [[ 51.04492188]]\n",
      "23\n",
      "2017-07-12T16:07:28.201655 : step 290 loss [[ 49.91137695]]\n",
      "23\n",
      "2017-07-12T16:07:28.206659 : step 291 loss [[ 48.67050934]]\n",
      "32\n",
      "2017-07-12T16:07:28.211662 : step 292 loss [[ 51.24020767]]\n",
      "32\n",
      "2017-07-12T16:07:28.216666 : step 293 loss [[ 46.6011734]]\n",
      "32\n",
      "2017-07-12T16:07:28.221670 : step 294 loss [[ 46.81079483]]\n",
      "32\n",
      "2017-07-12T16:07:28.226673 : step 295 loss [[ 46.46563721]]\n",
      "32\n",
      "2017-07-12T16:07:28.231677 : step 296 loss [[ 48.21061325]]\n",
      "32\n",
      "2017-07-12T16:07:28.236680 : step 297 loss [[ 49.23956299]]\n",
      "32\n",
      "2017-07-12T16:07:28.240683 : step 298 loss [[ 50.7355423]]\n",
      "32\n",
      "2017-07-12T16:07:28.245687 : step 299 loss [[ 41.03794098]]\n",
      "32\n",
      "2017-07-12T16:07:28.251691 : step 300 loss [[ 39.85753632]]\n",
      "32\n",
      "2017-07-12T16:07:28.256695 : step 301 loss [[ 38.52616119]]\n",
      "32\n",
      "2017-07-12T16:07:28.260698 : step 302 loss [[ 39.85160828]]\n",
      "32\n",
      "2017-07-12T16:07:28.265701 : step 303 loss [[ 46.32319641]]\n",
      "32\n",
      "2017-07-12T16:07:28.271207 : step 304 loss [[ 42.88277054]]\n",
      "32\n",
      "2017-07-12T16:07:28.276210 : step 305 loss [[ 37.62443924]]\n",
      "32\n",
      "2017-07-12T16:07:28.281214 : step 306 loss [[ 42.02623749]]\n",
      "32\n",
      "2017-07-12T16:07:28.286218 : step 307 loss [[ 43.71524048]]\n",
      "32\n",
      "2017-07-12T16:07:28.291425 : step 308 loss [[ 41.57520676]]\n",
      "32\n",
      "2017-07-12T16:07:28.295428 : step 309 loss [[ 42.19047165]]\n",
      "32\n",
      "2017-07-12T16:07:28.300432 : step 310 loss [[ 41.52436066]]\n",
      "32\n",
      "2017-07-12T16:07:28.305436 : step 311 loss [[ 43.33926392]]\n",
      "32\n",
      "2017-07-12T16:07:28.310439 : step 312 loss [[ 45.38464355]]\n",
      "32\n",
      "2017-07-12T16:07:28.315442 : step 313 loss [[ 43.31568146]]\n",
      "32\n",
      "2017-07-12T16:07:28.320446 : step 314 loss [[ 40.61220551]]\n",
      "32\n",
      "2017-07-12T16:07:28.325450 : step 315 loss [[ 36.78855896]]\n",
      "32\n",
      "2017-07-12T16:07:28.330453 : step 316 loss [[ 41.86877441]]\n",
      "32\n",
      "2017-07-12T16:07:28.335457 : step 317 loss [[ 43.13836288]]\n",
      "32\n",
      "2017-07-12T16:07:28.339460 : step 318 loss [[ 42.68223953]]\n",
      "32\n",
      "2017-07-12T16:07:28.346464 : step 319 loss [[ 43.6115303]]\n",
      "32\n",
      "2017-07-12T16:07:28.351468 : step 320 loss [[ 52.7853775]]\n",
      "32\n",
      "2017-07-12T16:07:28.356472 : step 321 loss [[ 55.17307281]]\n",
      "32\n",
      "2017-07-12T16:07:28.360475 : step 322 loss [[ 55.28166199]]\n",
      "32\n",
      "2017-07-12T16:07:28.366479 : step 323 loss [[ 53.17074203]]\n",
      "6\n",
      "2017-07-12T16:07:28.371986 : step 324 loss [[ 42.39207458]]\n",
      "6\n",
      "2017-07-12T16:07:28.376989 : step 325 loss [[ 48.44511414]]\n",
      "6\n",
      "2017-07-12T16:07:28.381993 : step 326 loss [[ 46.10577774]]\n",
      "6\n",
      "2017-07-12T16:07:28.386496 : step 327 loss [[ 43.9082222]]\n",
      "6\n",
      "2017-07-12T16:07:28.391623 : step 328 loss [[ 41.17043304]]\n",
      "6\n",
      "2017-07-12T16:07:28.396626 : step 329 loss [[ 40.67900085]]\n",
      "12\n",
      "2017-07-12T16:07:28.401630 : step 330 loss [[ 51.25398254]]\n",
      "12\n",
      "2017-07-12T16:07:28.406634 : step 331 loss [[ 41.37437439]]\n",
      "12\n",
      "2017-07-12T16:07:28.412638 : step 332 loss [[ 45.4654541]]\n",
      "12\n",
      "2017-07-12T16:07:28.417641 : step 333 loss [[ 46.76008606]]\n",
      "12\n",
      "2017-07-12T16:07:28.423646 : step 334 loss [[ 46.9176445]]\n",
      "12\n",
      "2017-07-12T16:07:28.428649 : step 335 loss [[ 45.38370514]]\n",
      "12\n",
      "2017-07-12T16:07:28.433653 : step 336 loss [[ 40.10749435]]\n",
      "12\n",
      "2017-07-12T16:07:28.438656 : step 337 loss [[ 36.57469177]]\n",
      "12\n",
      "2017-07-12T16:07:28.443660 : step 338 loss [[ 34.5411377]]\n",
      "12\n",
      "2017-07-12T16:07:28.447663 : step 339 loss [[ 40.08188248]]\n",
      "12\n",
      "2017-07-12T16:07:28.452666 : step 340 loss [[ 47.39941025]]\n",
      "12\n",
      "2017-07-12T16:07:28.457670 : step 341 loss [[ 51.47389984]]\n",
      "9\n",
      "2017-07-12T16:07:28.462673 : step 342 loss [[ 51.14335632]]\n",
      "9\n",
      "2017-07-12T16:07:28.467677 : step 343 loss [[ 44.71011734]]\n",
      "9\n",
      "2017-07-12T16:07:28.473184 : step 344 loss [[ 47.71232986]]\n",
      "9\n",
      "2017-07-12T16:07:28.478187 : step 345 loss [[ 51.42169571]]\n",
      "9\n",
      "2017-07-12T16:07:28.483191 : step 346 loss [[ 54.17861176]]\n",
      "9\n",
      "2017-07-12T16:07:28.487845 : step 347 loss [[ 47.05323029]]\n",
      "9\n",
      "2017-07-12T16:07:28.492848 : step 348 loss [[ 47.11457825]]\n",
      "9\n",
      "2017-07-12T16:07:28.497852 : step 349 loss [[ 53.77978516]]\n",
      "9\n",
      "2017-07-12T16:07:28.502855 : step 350 loss [[ 52.01543808]]\n",
      "cost:  [[ 16191.35742188]]\n",
      "8\n",
      "2017-07-12T16:07:28.507858 : step 351 loss [[ 49.19169998]]\n",
      "8\n",
      "2017-07-12T16:07:28.512862 : step 352 loss [[ 50.21798706]]\n",
      "8\n",
      "2017-07-12T16:07:28.517866 : step 353 loss [[ 49.66666412]]\n",
      "8\n",
      "2017-07-12T16:07:28.521869 : step 354 loss [[ 49.7225647]]\n",
      "8\n",
      "2017-07-12T16:07:28.526872 : step 355 loss [[ 50.18471909]]\n",
      "8\n",
      "2017-07-12T16:07:28.531876 : step 356 loss [[ 48.05288315]]\n",
      "8\n",
      "2017-07-12T16:07:28.536879 : step 357 loss [[ 48.14730453]]\n",
      "8\n",
      "2017-07-12T16:07:28.541883 : step 358 loss [[ 48.62134933]]\n",
      "18\n",
      "2017-07-12T16:07:28.546887 : step 359 loss [[ 50.15287781]]\n",
      "18\n",
      "2017-07-12T16:07:28.551890 : step 360 loss [[ 49.31121063]]\n",
      "18\n",
      "2017-07-12T16:07:28.556893 : step 361 loss [[ 51.24424744]]\n",
      "18\n",
      "2017-07-12T16:07:28.561897 : step 362 loss [[ 51.24176025]]\n",
      "18\n",
      "2017-07-12T16:07:28.566901 : step 363 loss [[ 50.16117859]]\n",
      "18\n",
      "2017-07-12T16:07:28.572407 : step 364 loss [[ 51.33012009]]\n",
      "18\n",
      "2017-07-12T16:07:28.577911 : step 365 loss [[ 49.98241043]]\n",
      "18\n",
      "2017-07-12T16:07:28.582414 : step 366 loss [[ 50.43247223]]\n",
      "18\n",
      "2017-07-12T16:07:28.586918 : step 367 loss [[ 49.37953568]]\n",
      "18\n",
      "2017-07-12T16:07:28.592075 : step 368 loss [[ 51.86830139]]\n",
      "18\n",
      "2017-07-12T16:07:28.597078 : step 369 loss [[ 51.25977325]]\n",
      "18\n",
      "2017-07-12T16:07:28.602082 : step 370 loss [[ 51.68689346]]\n",
      "18\n",
      "2017-07-12T16:07:28.606084 : step 371 loss [[ 50.9974823]]\n",
      "18\n",
      "2017-07-12T16:07:28.611088 : step 372 loss [[ 49.49393082]]\n",
      "18\n",
      "2017-07-12T16:07:28.616092 : step 373 loss [[ 49.63206482]]\n",
      "18\n",
      "2017-07-12T16:07:28.621095 : step 374 loss [[ 49.2671814]]\n",
      "18\n",
      "2017-07-12T16:07:28.626099 : step 375 loss [[ 48.17119217]]\n",
      "18\n",
      "2017-07-12T16:07:28.631102 : step 376 loss [[ 48.48861313]]\n",
      "7\n",
      "2017-07-12T16:07:28.636106 : step 377 loss [[ 38.89794922]]\n",
      "7\n",
      "2017-07-12T16:07:28.640109 : step 378 loss [[ 40.32025909]]\n",
      "7\n",
      "2017-07-12T16:07:28.645112 : step 379 loss [[ 41.97184372]]\n",
      "7\n",
      "2017-07-12T16:07:28.650116 : step 380 loss [[ 40.67345428]]\n",
      "7\n",
      "2017-07-12T16:07:28.655119 : step 381 loss [[ 37.64063644]]\n",
      "7\n",
      "2017-07-12T16:07:28.660123 : step 382 loss [[ 36.96381378]]\n",
      "7\n",
      "2017-07-12T16:07:28.665127 : step 383 loss [[ 39.49741364]]\n",
      "55\n",
      "2017-07-12T16:07:28.670130 : step 384 loss [[ 43.21560669]]\n",
      "55\n",
      "2017-07-12T16:07:28.675636 : step 385 loss [[ 46.97840118]]\n",
      "55\n",
      "2017-07-12T16:07:28.680140 : step 386 loss [[ 38.44538116]]\n",
      "55\n",
      "2017-07-12T16:07:28.685143 : step 387 loss [[ 43.21610641]]\n",
      "55\n",
      "2017-07-12T16:07:28.689650 : step 388 loss [[ 43.63394928]]\n",
      "55\n",
      "2017-07-12T16:07:28.694653 : step 389 loss [[ 42.0511322]]\n",
      "55\n",
      "2017-07-12T16:07:28.699657 : step 390 loss [[ 37.05694199]]\n",
      "55\n",
      "2017-07-12T16:07:28.703660 : step 391 loss [[ 35.43238831]]\n",
      "55\n",
      "2017-07-12T16:07:28.708663 : step 392 loss [[ 49.39118958]]\n",
      "55\n",
      "2017-07-12T16:07:28.713667 : step 393 loss [[ 48.39179993]]\n",
      "55\n",
      "2017-07-12T16:07:28.718670 : step 394 loss [[ 46.35399246]]\n",
      "55\n",
      "2017-07-12T16:07:28.723674 : step 395 loss [[ 46.67598343]]\n",
      "55\n",
      "2017-07-12T16:07:28.728678 : step 396 loss [[ 48.69514465]]\n",
      "55\n",
      "2017-07-12T16:07:28.733681 : step 397 loss [[ 44.66107941]]\n",
      "55\n",
      "2017-07-12T16:07:28.738685 : step 398 loss [[ 42.1727829]]\n",
      "55\n",
      "2017-07-12T16:07:28.743688 : step 399 loss [[ 50.15503693]]\n",
      "55\n",
      "2017-07-12T16:07:28.749693 : step 400 loss [[ 51.66052246]]\n",
      "55\n",
      "2017-07-12T16:07:28.754696 : step 401 loss [[ 39.65142441]]\n",
      "55\n",
      "2017-07-12T16:07:28.759699 : step 402 loss [[ 45.71963501]]\n",
      "55\n",
      "2017-07-12T16:07:28.764703 : step 403 loss [[ 43.68987274]]\n",
      "55\n",
      "2017-07-12T16:07:28.769707 : step 404 loss [[ 51.91971207]]\n",
      "55\n",
      "2017-07-12T16:07:28.774716 : step 405 loss [[ 54.91757965]]\n",
      "55\n",
      "2017-07-12T16:07:28.780220 : step 406 loss [[ 51.39955139]]\n",
      "55\n",
      "2017-07-12T16:07:28.785223 : step 407 loss [[ 49.53056717]]\n",
      "55\n",
      "2017-07-12T16:07:28.790614 : step 408 loss [[ 49.47661591]]\n",
      "55\n",
      "2017-07-12T16:07:28.795617 : step 409 loss [[ 50.67327118]]\n",
      "55\n",
      "2017-07-12T16:07:28.800629 : step 410 loss [[ 51.11520386]]\n",
      "55\n",
      "2017-07-12T16:07:28.805625 : step 411 loss [[ 48.57797241]]\n",
      "55\n",
      "2017-07-12T16:07:28.809627 : step 412 loss [[ 50.09954453]]\n",
      "55\n",
      "2017-07-12T16:07:28.814630 : step 413 loss [[ 47.75861359]]\n",
      "55\n",
      "2017-07-12T16:07:28.819634 : step 414 loss [[ 51.62909317]]\n",
      "55\n",
      "2017-07-12T16:07:28.824638 : step 415 loss [[ 56.04727936]]\n",
      "55\n",
      "2017-07-12T16:07:28.829641 : step 416 loss [[ 51.12663269]]\n",
      "55\n",
      "2017-07-12T16:07:28.834645 : step 417 loss [[ 51.41343689]]\n",
      "55\n",
      "2017-07-12T16:07:28.839648 : step 418 loss [[ 50.11865997]]\n",
      "55\n",
      "2017-07-12T16:07:28.844652 : step 419 loss [[ 47.30513]]\n",
      "55\n",
      "2017-07-12T16:07:28.849656 : step 420 loss [[ 49.73665619]]\n",
      "55\n",
      "2017-07-12T16:07:28.854659 : step 421 loss [[ 53.92642212]]\n",
      "55\n",
      "2017-07-12T16:07:28.859663 : step 422 loss [[ 51.88764954]]\n",
      "55\n",
      "2017-07-12T16:07:28.864666 : step 423 loss [[ 51.11777115]]\n",
      "55\n",
      "2017-07-12T16:07:28.868669 : step 424 loss [[ 51.54373169]]\n",
      "55\n",
      "2017-07-12T16:07:28.874678 : step 425 loss [[ 51.8274231]]\n",
      "55\n",
      "2017-07-12T16:07:28.879682 : step 426 loss [[ 48.52035141]]\n",
      "55\n",
      "2017-07-12T16:07:28.884685 : step 427 loss [[ 48.09726334]]\n",
      "55\n",
      "2017-07-12T16:07:28.889358 : step 428 loss [[ 42.29319763]]\n",
      "55\n",
      "2017-07-12T16:07:28.894362 : step 429 loss [[ 44.76292419]]\n",
      "55\n",
      "2017-07-12T16:07:28.899365 : step 430 loss [[ 45.23711395]]\n",
      "55\n",
      "2017-07-12T16:07:28.904369 : step 431 loss [[ 43.4134407]]\n",
      "55\n",
      "2017-07-12T16:07:28.909373 : step 432 loss [[ 43.45415497]]\n",
      "55\n",
      "2017-07-12T16:07:28.914376 : step 433 loss [[ 44.67121887]]\n",
      "55\n",
      "2017-07-12T16:07:28.919380 : step 434 loss [[ 45.183815]]\n",
      "55\n",
      "2017-07-12T16:07:28.924383 : step 435 loss [[ 42.74042511]]\n",
      "55\n",
      "2017-07-12T16:07:28.929387 : step 436 loss [[ 39.25663757]]\n",
      "55\n",
      "2017-07-12T16:07:28.934390 : step 437 loss [[ 38.53982544]]\n",
      "55\n",
      "2017-07-12T16:07:28.939394 : step 438 loss [[ 41.46326447]]\n",
      "7\n",
      "2017-07-12T16:07:28.944397 : step 439 loss [[ 48.05454254]]\n",
      "7\n",
      "2017-07-12T16:07:28.949401 : step 440 loss [[ 48.71620178]]\n",
      "7\n",
      "2017-07-12T16:07:28.954405 : step 441 loss [[ 49.44709778]]\n",
      "7\n",
      "2017-07-12T16:07:28.960409 : step 442 loss [[ 49.83123016]]\n",
      "7\n",
      "2017-07-12T16:07:28.967414 : step 443 loss [[ 48.05335236]]\n",
      "7\n",
      "2017-07-12T16:07:28.972420 : step 444 loss [[ 43.07298279]]\n",
      "7\n",
      "2017-07-12T16:07:28.977423 : step 445 loss [[ 46.45545959]]\n",
      "cost:  [[ 20676.88867188]]\n",
      "103\n",
      "2017-07-12T16:07:28.982928 : step 446 loss [[ 58.06871796]]\n",
      "103\n",
      "2017-07-12T16:07:28.987430 : step 447 loss [[ 55.25398636]]\n",
      "103\n",
      "2017-07-12T16:07:28.992506 : step 448 loss [[ 54.37308884]]\n",
      "103\n",
      "2017-07-12T16:07:28.998510 : step 449 loss [[ 55.08901596]]\n",
      "103\n",
      "2017-07-12T16:07:29.003513 : step 450 loss [[ 55.53769302]]\n",
      "103\n",
      "2017-07-12T16:07:29.009517 : step 451 loss [[ 50.67088318]]\n",
      "103\n",
      "2017-07-12T16:07:29.014521 : step 452 loss [[ 56.38075638]]\n",
      "103\n",
      "2017-07-12T16:07:29.019525 : step 453 loss [[ 53.76525879]]\n",
      "103\n",
      "2017-07-12T16:07:29.023528 : step 454 loss [[ 54.86913681]]\n",
      "103\n",
      "2017-07-12T16:07:29.028531 : step 455 loss [[ 53.50928116]]\n",
      "103\n",
      "2017-07-12T16:07:29.033535 : step 456 loss [[ 53.05257416]]\n",
      "103\n",
      "2017-07-12T16:07:29.038538 : step 457 loss [[ 50.8262558]]\n",
      "103\n",
      "2017-07-12T16:07:29.043542 : step 458 loss [[ 51.07406998]]\n",
      "103\n",
      "2017-07-12T16:07:29.048546 : step 459 loss [[ 52.60777664]]\n",
      "103\n",
      "2017-07-12T16:07:29.052548 : step 460 loss [[ 51.87575531]]\n",
      "103\n",
      "2017-07-12T16:07:29.057552 : step 461 loss [[ 50.36746597]]\n",
      "103\n",
      "2017-07-12T16:07:29.062556 : step 462 loss [[ 48.63180542]]\n",
      "103\n",
      "2017-07-12T16:07:29.067559 : step 463 loss [[ 52.3425827]]\n",
      "103\n",
      "2017-07-12T16:07:29.072562 : step 464 loss [[ 51.97205353]]\n",
      "103\n",
      "2017-07-12T16:07:29.077568 : step 465 loss [[ 54.18475342]]\n",
      "103\n",
      "2017-07-12T16:07:29.082071 : step 466 loss [[ 57.43375778]]\n",
      "103\n",
      "2017-07-12T16:07:29.086574 : step 467 loss [[ 54.1765213]]\n",
      "103\n",
      "2017-07-12T16:07:29.090926 : step 468 loss [[ 53.20454025]]\n",
      "103\n",
      "2017-07-12T16:07:29.095929 : step 469 loss [[ 52.48083115]]\n",
      "103\n",
      "2017-07-12T16:07:29.100932 : step 470 loss [[ 51.80692291]]\n",
      "103\n",
      "2017-07-12T16:07:29.105936 : step 471 loss [[ 50.19678497]]\n",
      "103\n",
      "2017-07-12T16:07:29.110939 : step 472 loss [[ 53.34978867]]\n",
      "103\n",
      "2017-07-12T16:07:29.114942 : step 473 loss [[ 53.73108673]]\n",
      "103\n",
      "2017-07-12T16:07:29.119946 : step 474 loss [[ 52.66863251]]\n",
      "103\n",
      "2017-07-12T16:07:29.124949 : step 475 loss [[ 52.95584106]]\n",
      "103\n",
      "2017-07-12T16:07:29.129953 : step 476 loss [[ 50.87774658]]\n",
      "103\n",
      "2017-07-12T16:07:29.133956 : step 477 loss [[ 52.97863388]]\n",
      "103\n",
      "2017-07-12T16:07:29.138959 : step 478 loss [[ 46.2455368]]\n",
      "103\n",
      "2017-07-12T16:07:29.143963 : step 479 loss [[ 48.40058136]]\n",
      "103\n",
      "2017-07-12T16:07:29.147965 : step 480 loss [[ 54.96284103]]\n",
      "103\n",
      "2017-07-12T16:07:29.152969 : step 481 loss [[ 46.83744049]]\n",
      "103\n",
      "2017-07-12T16:07:29.157973 : step 482 loss [[ 44.15493774]]\n",
      "103\n",
      "2017-07-12T16:07:29.162976 : step 483 loss [[ 47.32416153]]\n",
      "103\n",
      "2017-07-12T16:07:29.167980 : step 484 loss [[ 48.89668274]]\n",
      "103\n",
      "2017-07-12T16:07:29.172986 : step 485 loss [[ 51.1199646]]\n",
      "103\n",
      "2017-07-12T16:07:29.177489 : step 486 loss [[ 54.80053329]]\n",
      "103\n",
      "2017-07-12T16:07:29.182993 : step 487 loss [[ 49.59162903]]\n",
      "103\n",
      "2017-07-12T16:07:29.187496 : step 488 loss [[ 51.67197418]]\n",
      "103\n",
      "2017-07-12T16:07:29.192749 : step 489 loss [[ 47.83973312]]\n",
      "103\n",
      "2017-07-12T16:07:29.197753 : step 490 loss [[ 46.48934937]]\n",
      "103\n",
      "2017-07-12T16:07:29.203757 : step 491 loss [[ 51.67390442]]\n",
      "103\n",
      "2017-07-12T16:07:29.208761 : step 492 loss [[ 48.55683517]]\n",
      "103\n",
      "2017-07-12T16:07:29.213764 : step 493 loss [[ 47.62712097]]\n",
      "103\n",
      "2017-07-12T16:07:29.218768 : step 494 loss [[ 50.48095703]]\n",
      "103\n",
      "2017-07-12T16:07:29.222770 : step 495 loss [[ 49.75203705]]\n",
      "103\n",
      "2017-07-12T16:07:29.227774 : step 496 loss [[ 49.00688171]]\n",
      "103\n",
      "2017-07-12T16:07:29.232778 : step 497 loss [[ 46.87349701]]\n",
      "103\n",
      "2017-07-12T16:07:29.237782 : step 498 loss [[ 46.96981812]]\n",
      "103\n",
      "2017-07-12T16:07:29.242785 : step 499 loss [[ 47.69407654]]\n",
      "103\n",
      "2017-07-12T16:07:29.247789 : step 500 loss [[ 48.11628723]]\n",
      "103\n",
      "2017-07-12T16:07:29.251792 : step 501 loss [[ 49.01841736]]\n",
      "103\n",
      "2017-07-12T16:07:29.256795 : step 502 loss [[ 51.4115715]]\n",
      "103\n",
      "2017-07-12T16:07:29.261798 : step 503 loss [[ 51.64759064]]\n",
      "103\n",
      "2017-07-12T16:07:29.266802 : step 504 loss [[ 53.37011719]]\n",
      "103\n",
      "2017-07-12T16:07:29.271806 : step 505 loss [[ 51.10423279]]\n",
      "103\n",
      "2017-07-12T16:07:29.276811 : step 506 loss [[ 53.03292465]]\n",
      "103\n",
      "2017-07-12T16:07:29.281815 : step 507 loss [[ 55.63647079]]\n",
      "103\n",
      "2017-07-12T16:07:29.286819 : step 508 loss [[ 53.87506866]]\n",
      "103\n",
      "2017-07-12T16:07:29.290823 : step 509 loss [[ 53.79161835]]\n",
      "103\n",
      "2017-07-12T16:07:29.295827 : step 510 loss [[ 50.88749313]]\n",
      "103\n",
      "2017-07-12T16:07:29.300830 : step 511 loss [[ 52.34113693]]\n",
      "103\n",
      "2017-07-12T16:07:29.304833 : step 512 loss [[ 50.53944397]]\n",
      "103\n",
      "2017-07-12T16:07:29.309837 : step 513 loss [[ 49.02662277]]\n",
      "103\n",
      "2017-07-12T16:07:29.315841 : step 514 loss [[ 48.42002487]]\n",
      "103\n",
      "2017-07-12T16:07:29.319844 : step 515 loss [[ 47.84176636]]\n",
      "103\n",
      "2017-07-12T16:07:29.324848 : step 516 loss [[ 48.97044754]]\n",
      "103\n",
      "2017-07-12T16:07:29.329851 : step 517 loss [[ 50.14093018]]\n",
      "103\n",
      "2017-07-12T16:07:29.334855 : step 518 loss [[ 50.74580765]]\n",
      "103\n",
      "2017-07-12T16:07:29.339858 : step 519 loss [[ 49.93236923]]\n",
      "103\n",
      "2017-07-12T16:07:29.344862 : step 520 loss [[ 48.29656982]]\n",
      "103\n",
      "2017-07-12T16:07:29.348864 : step 521 loss [[ 46.31125259]]\n",
      "103\n",
      "2017-07-12T16:07:29.353868 : step 522 loss [[ 45.33338928]]\n",
      "103\n",
      "2017-07-12T16:07:29.358871 : step 523 loss [[ 44.03952026]]\n",
      "103\n",
      "2017-07-12T16:07:29.362874 : step 524 loss [[ 44.1672821]]\n",
      "103\n",
      "2017-07-12T16:07:29.368878 : step 525 loss [[ 50.21677399]]\n",
      "103\n",
      "2017-07-12T16:07:29.374389 : step 526 loss [[ 51.26625443]]\n",
      "103\n",
      "2017-07-12T16:07:29.379392 : step 527 loss [[ 53.00946808]]\n",
      "103\n",
      "2017-07-12T16:07:29.383895 : step 528 loss [[ 47.5586853]]\n",
      "103\n",
      "2017-07-12T16:07:29.388866 : step 529 loss [[ 46.08163834]]\n",
      "103\n",
      "2017-07-12T16:07:29.393869 : step 530 loss [[ 44.79064178]]\n",
      "103\n",
      "2017-07-12T16:07:29.397872 : step 531 loss [[ 46.89918518]]\n",
      "103\n",
      "2017-07-12T16:07:29.403877 : step 532 loss [[ 47.14488983]]\n",
      "103\n",
      "2017-07-12T16:07:29.409881 : step 533 loss [[ 46.82319641]]\n",
      "103\n",
      "2017-07-12T16:07:29.414884 : step 534 loss [[ 46.4274826]]\n",
      "103\n",
      "2017-07-12T16:07:29.419888 : step 535 loss [[ 45.22007751]]\n",
      "103\n",
      "2017-07-12T16:07:29.424891 : step 536 loss [[ 45.98196411]]\n",
      "103\n",
      "2017-07-12T16:07:29.429895 : step 537 loss [[ 47.10407639]]\n",
      "103\n",
      "2017-07-12T16:07:29.434899 : step 538 loss [[ 45.55573273]]\n",
      "103\n",
      "2017-07-12T16:07:29.439902 : step 539 loss [[ 46.28862762]]\n",
      "103\n",
      "2017-07-12T16:07:29.444906 : step 540 loss [[ 47.45087051]]\n",
      "103\n",
      "2017-07-12T16:07:29.449910 : step 541 loss [[ 46.90247345]]\n",
      "103\n",
      "2017-07-12T16:07:29.453912 : step 542 loss [[ 50.69853973]]\n",
      "103\n",
      "2017-07-12T16:07:29.458916 : step 543 loss [[ 53.14398956]]\n",
      "103\n",
      "2017-07-12T16:07:29.463919 : step 544 loss [[ 41.43618011]]\n",
      "103\n",
      "2017-07-12T16:07:29.468923 : step 545 loss [[ 47.82802963]]\n",
      "103\n",
      "2017-07-12T16:07:29.473929 : step 546 loss [[ 50.98379517]]\n",
      "103\n",
      "2017-07-12T16:07:29.478432 : step 547 loss [[ 48.78652191]]\n",
      "103\n",
      "2017-07-12T16:07:29.483435 : step 548 loss [[ 46.70005035]]\n",
      "18\n",
      "2017-07-12T16:07:29.487439 : step 549 loss [[ 47.41394806]]\n",
      "18\n",
      "2017-07-12T16:07:29.492496 : step 550 loss [[ 49.92674255]]\n",
      "18\n",
      "2017-07-12T16:07:29.497499 : step 551 loss [[ 49.26728439]]\n",
      "18\n",
      "2017-07-12T16:07:29.502503 : step 552 loss [[ 50.68370819]]\n",
      "18\n",
      "2017-07-12T16:07:29.507506 : step 553 loss [[ 47.08290482]]\n",
      "18\n",
      "2017-07-12T16:07:29.511509 : step 554 loss [[ 45.42704391]]\n",
      "18\n",
      "2017-07-12T16:07:29.516513 : step 555 loss [[ 46.13952637]]\n",
      "18\n",
      "2017-07-12T16:07:29.521516 : step 556 loss [[ 46.68154144]]\n",
      "18\n",
      "2017-07-12T16:07:29.525519 : step 557 loss [[ 52.84283447]]\n",
      "18\n",
      "2017-07-12T16:07:29.530523 : step 558 loss [[ 46.63726807]]\n",
      "18\n",
      "2017-07-12T16:07:29.535526 : step 559 loss [[ 46.17317963]]\n",
      "18\n",
      "2017-07-12T16:07:29.540530 : step 560 loss [[ 46.1014328]]\n",
      "18\n",
      "2017-07-12T16:07:29.545533 : step 561 loss [[ 46.08301544]]\n",
      "18\n",
      "2017-07-12T16:07:29.550537 : step 562 loss [[ 47.01680374]]\n",
      "18\n",
      "2017-07-12T16:07:29.554540 : step 563 loss [[ 47.69300079]]\n",
      "18\n",
      "2017-07-12T16:07:29.559544 : step 564 loss [[ 48.1697197]]\n",
      "18\n",
      "2017-07-12T16:07:29.565548 : step 565 loss [[ 50.84805679]]\n",
      "18\n",
      "2017-07-12T16:07:29.570551 : step 566 loss [[ 50.10168457]]\n",
      "6\n",
      "2017-07-12T16:07:29.576057 : step 567 loss [[ 50.09620667]]\n",
      "6\n",
      "2017-07-12T16:07:29.580561 : step 568 loss [[ 49.80634308]]\n",
      "6\n",
      "2017-07-12T16:07:29.585064 : step 569 loss [[ 49.53609467]]\n",
      "6\n",
      "2017-07-12T16:07:29.591072 : step 570 loss [[ 49.39803314]]\n",
      "6\n",
      "2017-07-12T16:07:29.597076 : step 571 loss [[ 47.30808258]]\n",
      "6\n",
      "2017-07-12T16:07:29.602079 : step 572 loss [[ 48.95568085]]\n",
      "5\n",
      "2017-07-12T16:07:29.607083 : step 573 loss [[ 50.57540894]]\n",
      "5\n",
      "2017-07-12T16:07:29.612087 : step 574 loss [[ 50.51159668]]\n",
      "5\n",
      "2017-07-12T16:07:29.616089 : step 575 loss [[ 50.21861267]]\n",
      "5\n",
      "2017-07-12T16:07:29.622094 : step 576 loss [[ 51.62208557]]\n",
      "5\n",
      "2017-07-12T16:07:29.627097 : step 577 loss [[ 51.64577484]]\n",
      "6\n",
      "2017-07-12T16:07:29.632101 : step 578 loss [[ 51.10650253]]\n",
      "6\n",
      "2017-07-12T16:07:29.637104 : step 579 loss [[ 52.17736816]]\n",
      "6\n",
      "2017-07-12T16:07:29.643109 : step 580 loss [[ 49.13957977]]\n",
      "6\n",
      "2017-07-12T16:07:29.649113 : step 581 loss [[ 47.51583099]]\n",
      "6\n",
      "2017-07-12T16:07:29.658120 : step 582 loss [[ 49.56559753]]\n",
      "6\n",
      "2017-07-12T16:07:29.663123 : step 583 loss [[ 45.2833252]]\n",
      "26\n",
      "2017-07-12T16:07:29.669127 : step 584 loss [[ 49.33374405]]\n",
      "26\n",
      "2017-07-12T16:07:29.674134 : step 585 loss [[ 57.2206192]]\n",
      "26\n",
      "2017-07-12T16:07:29.679138 : step 586 loss [[ 57.80071259]]\n",
      "26\n",
      "2017-07-12T16:07:29.685643 : step 587 loss [[ 54.4850502]]\n",
      "26\n",
      "2017-07-12T16:07:29.690878 : step 588 loss [[ 47.82617569]]\n",
      "26\n",
      "2017-07-12T16:07:29.695882 : step 589 loss [[ 46.92145538]]\n",
      "26\n",
      "2017-07-12T16:07:29.700887 : step 590 loss [[ 45.52661133]]\n",
      "26\n",
      "2017-07-12T16:07:29.705889 : step 591 loss [[ 45.71554947]]\n",
      "26\n",
      "2017-07-12T16:07:29.710893 : step 592 loss [[ 46.37282181]]\n",
      "26\n",
      "2017-07-12T16:07:29.717899 : step 593 loss [[ 46.46590805]]\n",
      "26\n",
      "2017-07-12T16:07:29.723903 : step 594 loss [[ 48.01279068]]\n",
      "26\n",
      "2017-07-12T16:07:29.728906 : step 595 loss [[ 50.77836609]]\n",
      "26\n",
      "2017-07-12T16:07:29.733909 : step 596 loss [[ 49.72854996]]\n",
      "26\n",
      "2017-07-12T16:07:29.739913 : step 597 loss [[ 50.72532272]]\n",
      "26\n",
      "2017-07-12T16:07:29.743916 : step 598 loss [[ 52.67164612]]\n",
      "26\n",
      "2017-07-12T16:07:29.749920 : step 599 loss [[ 53.32487488]]\n",
      "26\n",
      "2017-07-12T16:07:29.755925 : step 600 loss [[ 54.03163147]]\n",
      "26\n",
      "2017-07-12T16:07:29.760928 : step 601 loss [[ 53.15342712]]\n",
      "26\n",
      "2017-07-12T16:07:29.765932 : step 602 loss [[ 53.65396118]]\n",
      "26\n",
      "2017-07-12T16:07:29.772440 : step 603 loss [[ 53.01406479]]\n",
      "26\n",
      "2017-07-12T16:07:29.777943 : step 604 loss [[ 48.87785339]]\n",
      "26\n",
      "2017-07-12T16:07:29.782947 : step 605 loss [[ 51.62855911]]\n",
      "26\n",
      "2017-07-12T16:07:29.787950 : step 606 loss [[ 51.54035187]]\n",
      "26\n",
      "2017-07-12T16:07:29.793002 : step 607 loss [[ 50.98989487]]\n",
      "26\n",
      "2017-07-12T16:07:29.798005 : step 608 loss [[ 47.8438797]]\n",
      "26\n",
      "2017-07-12T16:07:29.803009 : step 609 loss [[ 50.80695724]]\n",
      "39\n",
      "2017-07-12T16:07:29.808012 : step 610 loss [[ 49.84378052]]\n",
      "39\n",
      "2017-07-12T16:07:29.813016 : step 611 loss [[ 50.0656662]]\n",
      "39\n",
      "2017-07-12T16:07:29.818020 : step 612 loss [[ 48.75841141]]\n",
      "39\n",
      "2017-07-12T16:07:29.823024 : step 613 loss [[ 50.68493652]]\n",
      "39\n",
      "2017-07-12T16:07:29.829028 : step 614 loss [[ 49.84354782]]\n",
      "39\n",
      "2017-07-12T16:07:29.834032 : step 615 loss [[ 49.69616699]]\n",
      "39\n",
      "2017-07-12T16:07:29.839034 : step 616 loss [[ 49.51853943]]\n",
      "39\n",
      "2017-07-12T16:07:29.844038 : step 617 loss [[ 49.76480103]]\n",
      "39\n",
      "2017-07-12T16:07:29.849042 : step 618 loss [[ 49.20261765]]\n",
      "39\n",
      "2017-07-12T16:07:29.854045 : step 619 loss [[ 49.14932251]]\n",
      "39\n",
      "2017-07-12T16:07:29.859049 : step 620 loss [[ 50.65088654]]\n",
      "39\n",
      "2017-07-12T16:07:29.864052 : step 621 loss [[ 50.41152954]]\n",
      "39\n",
      "2017-07-12T16:07:29.869056 : step 622 loss [[ 47.73428345]]\n",
      "39\n",
      "2017-07-12T16:07:29.874062 : step 623 loss [[ 49.07522202]]\n",
      "39\n",
      "2017-07-12T16:07:29.879065 : step 624 loss [[ 50.00575638]]\n",
      "39\n",
      "2017-07-12T16:07:29.884069 : step 625 loss [[ 52.11507416]]\n",
      "39\n",
      "2017-07-12T16:07:29.888572 : step 626 loss [[ 52.42656708]]\n",
      "39\n",
      "2017-07-12T16:07:29.893577 : step 627 loss [[ 51.14839172]]\n",
      "39\n",
      "2017-07-12T16:07:29.897580 : step 628 loss [[ 48.78450012]]\n",
      "39\n",
      "2017-07-12T16:07:29.902584 : step 629 loss [[ 48.95127487]]\n",
      "39\n",
      "2017-07-12T16:07:29.907587 : step 630 loss [[ 47.24386597]]\n",
      "39\n",
      "2017-07-12T16:07:29.912591 : step 631 loss [[ 47.96448517]]\n",
      "39\n",
      "2017-07-12T16:07:29.917594 : step 632 loss [[ 48.24181747]]\n",
      "39\n",
      "2017-07-12T16:07:29.921597 : step 633 loss [[ 49.77362061]]\n",
      "39\n",
      "2017-07-12T16:07:29.926601 : step 634 loss [[ 48.81803894]]\n",
      "39\n",
      "2017-07-12T16:07:29.931604 : step 635 loss [[ 48.76973343]]\n",
      "39\n",
      "2017-07-12T16:07:29.937609 : step 636 loss [[ 45.7832489]]\n",
      "39\n",
      "2017-07-12T16:07:29.942612 : step 637 loss [[ 46.32680511]]\n",
      "39\n",
      "2017-07-12T16:07:29.947616 : step 638 loss [[ 46.38063049]]\n",
      "39\n",
      "2017-07-12T16:07:29.952620 : step 639 loss [[ 47.0037384]]\n",
      "39\n",
      "2017-07-12T16:07:29.956622 : step 640 loss [[ 48.01541901]]\n",
      "39\n",
      "2017-07-12T16:07:29.962627 : step 641 loss [[ 46.93323517]]\n",
      "39\n",
      "2017-07-12T16:07:29.968631 : step 642 loss [[ 49.99446869]]\n",
      "39\n",
      "2017-07-12T16:07:29.974138 : step 643 loss [[ 52.7562027]]\n",
      "39\n",
      "2017-07-12T16:07:29.979141 : step 644 loss [[ 52.8904953]]\n",
      "39\n",
      "2017-07-12T16:07:29.983644 : step 645 loss [[ 52.13012695]]\n",
      "39\n",
      "2017-07-12T16:07:29.988147 : step 646 loss [[ 45.57705688]]\n",
      "39\n",
      "2017-07-12T16:07:29.994391 : step 647 loss [[ 44.11118698]]\n",
      "39\n",
      "2017-07-12T16:07:29.999395 : step 648 loss [[ 45.55661774]]\n",
      "13\n",
      "2017-07-12T16:07:30.004398 : step 649 loss [[ 48.5773201]]\n",
      "13\n",
      "2017-07-12T16:07:30.009402 : step 650 loss [[ 49.6826973]]\n",
      "13\n",
      "2017-07-12T16:07:30.014405 : step 651 loss [[ 46.54917908]]\n",
      "13\n",
      "2017-07-12T16:07:30.018408 : step 652 loss [[ 47.60365295]]\n",
      "13\n",
      "2017-07-12T16:07:30.023412 : step 653 loss [[ 51.3744278]]\n",
      "13\n",
      "2017-07-12T16:07:30.030418 : step 654 loss [[ 51.32299423]]\n",
      "13\n",
      "2017-07-12T16:07:30.035420 : step 655 loss [[ 50.63744354]]\n",
      "13\n",
      "2017-07-12T16:07:30.041426 : step 656 loss [[ 49.64466858]]\n",
      "13\n",
      "2017-07-12T16:07:30.046428 : step 657 loss [[ 51.48444748]]\n",
      "13\n",
      "2017-07-12T16:07:30.051433 : step 658 loss [[ 49.97141266]]\n",
      "13\n",
      "2017-07-12T16:07:30.056435 : step 659 loss [[ 49.46869278]]\n",
      "13\n",
      "2017-07-12T16:07:30.061439 : step 660 loss [[ 48.45111084]]\n",
      "13\n",
      "2017-07-12T16:07:30.066442 : step 661 loss [[ 48.27354431]]\n",
      "43\n",
      "2017-07-12T16:07:30.072446 : step 662 loss [[ 46.34085846]]\n",
      "43\n",
      "2017-07-12T16:07:30.077453 : step 663 loss [[ 51.0071106]]\n",
      "43\n",
      "2017-07-12T16:07:30.081955 : step 664 loss [[ 53.24820709]]\n",
      "43\n",
      "2017-07-12T16:07:30.087459 : step 665 loss [[ 51.54499817]]\n",
      "43\n",
      "2017-07-12T16:07:30.092465 : step 666 loss [[ 48.63307953]]\n",
      "43\n",
      "2017-07-12T16:07:30.097469 : step 667 loss [[ 47.90585327]]\n",
      "43\n",
      "2017-07-12T16:07:30.102473 : step 668 loss [[ 55.21674347]]\n",
      "43\n",
      "2017-07-12T16:07:30.107476 : step 669 loss [[ 61.99652863]]\n",
      "43\n",
      "2017-07-12T16:07:30.112480 : step 670 loss [[ 58.67266083]]\n",
      "43\n",
      "2017-07-12T16:07:30.117484 : step 671 loss [[ 58.84209061]]\n",
      "43\n",
      "2017-07-12T16:07:30.122487 : step 672 loss [[ 57.69483185]]\n",
      "43\n",
      "2017-07-12T16:07:30.128491 : step 673 loss [[ 60.22851181]]\n",
      "43\n",
      "2017-07-12T16:07:30.132494 : step 674 loss [[ 59.30052948]]\n",
      "43\n",
      "2017-07-12T16:07:30.138499 : step 675 loss [[ 52.31402588]]\n",
      "43\n",
      "2017-07-12T16:07:30.143502 : step 676 loss [[ 53.3106308]]\n",
      "43\n",
      "2017-07-12T16:07:30.148505 : step 677 loss [[ 55.24465942]]\n",
      "43\n",
      "2017-07-12T16:07:30.154510 : step 678 loss [[ 49.99255371]]\n",
      "43\n",
      "2017-07-12T16:07:30.159513 : step 679 loss [[ 48.71479416]]\n",
      "43\n",
      "2017-07-12T16:07:30.164517 : step 680 loss [[ 56.52985001]]\n",
      "43\n",
      "2017-07-12T16:07:30.169520 : step 681 loss [[ 55.71811676]]\n",
      "43\n",
      "2017-07-12T16:07:30.174528 : step 682 loss [[ 53.34573746]]\n",
      "43\n",
      "2017-07-12T16:07:30.180031 : step 683 loss [[ 53.6568985]]\n",
      "43\n",
      "2017-07-12T16:07:30.185035 : step 684 loss [[ 54.23865891]]\n",
      "43\n",
      "2017-07-12T16:07:30.190540 : step 685 loss [[ 56.84435272]]\n",
      "43\n",
      "2017-07-12T16:07:30.194543 : step 686 loss [[ 55.58305359]]\n",
      "43\n",
      "2017-07-12T16:07:30.199546 : step 687 loss [[ 53.87992477]]\n",
      "43\n",
      "2017-07-12T16:07:30.204550 : step 688 loss [[ 54.75818634]]\n",
      "43\n",
      "2017-07-12T16:07:30.209554 : step 689 loss [[ 52.95927048]]\n",
      "43\n",
      "2017-07-12T16:07:30.214557 : step 690 loss [[ 51.67118073]]\n",
      "43\n",
      "2017-07-12T16:07:30.219561 : step 691 loss [[ 53.1303215]]\n",
      "43\n",
      "2017-07-12T16:07:30.224565 : step 692 loss [[ 53.56097794]]\n",
      "43\n",
      "2017-07-12T16:07:30.229568 : step 693 loss [[ 51.45575333]]\n",
      "43\n",
      "2017-07-12T16:07:30.235573 : step 694 loss [[ 50.00312424]]\n",
      "43\n",
      "2017-07-12T16:07:30.240576 : step 695 loss [[ 50.82745743]]\n",
      "43\n",
      "2017-07-12T16:07:30.246580 : step 696 loss [[ 49.14080429]]\n",
      "43\n",
      "2017-07-12T16:07:30.251584 : step 697 loss [[ 53.83687592]]\n",
      "43\n",
      "2017-07-12T16:07:30.256587 : step 698 loss [[ 52.81912994]]\n",
      "43\n",
      "2017-07-12T16:07:30.260590 : step 699 loss [[ 50.32606888]]\n",
      "43\n",
      "2017-07-12T16:07:30.265594 : step 700 loss [[ 50.25534058]]\n",
      "43\n",
      "2017-07-12T16:07:30.270597 : step 701 loss [[ 49.66432953]]\n",
      "43\n",
      "2017-07-12T16:07:30.275604 : step 702 loss [[ 46.48487473]]\n",
      "43\n",
      "2017-07-12T16:07:30.281108 : step 703 loss [[ 46.82661057]]\n",
      "43\n",
      "2017-07-12T16:07:30.285611 : step 704 loss [[ 55.93189621]]\n",
      "22\n",
      "2017-07-12T16:07:30.291149 : step 705 loss [[ 51.01937103]]\n",
      "22\n",
      "2017-07-12T16:07:30.295152 : step 706 loss [[ 53.06156158]]\n",
      "22\n",
      "2017-07-12T16:07:30.301157 : step 707 loss [[ 51.43372345]]\n",
      "22\n",
      "2017-07-12T16:07:30.306160 : step 708 loss [[ 48.73463058]]\n",
      "22\n",
      "2017-07-12T16:07:30.311164 : step 709 loss [[ 47.80696869]]\n",
      "22\n",
      "2017-07-12T16:07:30.316167 : step 710 loss [[ 48.0100441]]\n",
      "22\n",
      "2017-07-12T16:07:30.322172 : step 711 loss [[ 49.27187347]]\n",
      "22\n",
      "2017-07-12T16:07:30.327175 : step 712 loss [[ 50.97077942]]\n",
      "22\n",
      "2017-07-12T16:07:30.332179 : step 713 loss [[ 43.77775574]]\n",
      "22\n",
      "2017-07-12T16:07:30.338183 : step 714 loss [[ 44.10606766]]\n",
      "22\n",
      "2017-07-12T16:07:30.343186 : step 715 loss [[ 43.75044632]]\n",
      "22\n",
      "2017-07-12T16:07:30.348190 : step 716 loss [[ 48.28507996]]\n",
      "22\n",
      "2017-07-12T16:07:30.353193 : step 717 loss [[ 46.98198318]]\n",
      "22\n",
      "2017-07-12T16:07:30.358197 : step 718 loss [[ 46.33851624]]\n",
      "22\n",
      "2017-07-12T16:07:30.363200 : step 719 loss [[ 50.30534744]]\n",
      "22\n",
      "2017-07-12T16:07:30.368204 : step 720 loss [[ 48.42339706]]\n",
      "22\n",
      "2017-07-12T16:07:30.374212 : step 721 loss [[ 43.68566132]]\n",
      "22\n",
      "2017-07-12T16:07:30.379215 : step 722 loss [[ 43.5464325]]\n",
      "22\n",
      "2017-07-12T16:07:30.384219 : step 723 loss [[ 46.02204514]]\n",
      "22\n",
      "2017-07-12T16:07:30.389725 : step 724 loss [[ 46.15452576]]\n",
      "22\n",
      "2017-07-12T16:07:30.393728 : step 725 loss [[ 47.71221924]]\n",
      "22\n",
      "2017-07-12T16:07:30.398732 : step 726 loss [[ 48.4407692]]\n",
      "28\n",
      "2017-07-12T16:07:30.404737 : step 727 loss [[ 49.3047142]]\n",
      "28\n",
      "2017-07-12T16:07:30.409739 : step 728 loss [[ 54.96603012]]\n",
      "28\n",
      "2017-07-12T16:07:30.414743 : step 729 loss [[ 54.16172791]]\n",
      "28\n",
      "2017-07-12T16:07:30.419747 : step 730 loss [[ 52.67995834]]\n",
      "28\n",
      "2017-07-12T16:07:30.424750 : step 731 loss [[ 51.61975098]]\n",
      "28\n",
      "2017-07-12T16:07:30.429754 : step 732 loss [[ 52.22853088]]\n",
      "28\n",
      "2017-07-12T16:07:30.434758 : step 733 loss [[ 51.31255722]]\n",
      "28\n",
      "2017-07-12T16:07:30.439762 : step 734 loss [[ 56.58596802]]\n",
      "28\n",
      "2017-07-12T16:07:30.444765 : step 735 loss [[ 53.79653549]]\n",
      "28\n",
      "2017-07-12T16:07:30.449768 : step 736 loss [[ 53.97692108]]\n",
      "28\n",
      "2017-07-12T16:07:30.454772 : step 737 loss [[ 52.07940674]]\n",
      "28\n",
      "2017-07-12T16:07:30.459775 : step 738 loss [[ 57.91699219]]\n",
      "28\n",
      "2017-07-12T16:07:30.464779 : step 739 loss [[ 53.22870255]]\n",
      "28\n",
      "2017-07-12T16:07:30.469783 : step 740 loss [[ 53.7807579]]\n",
      "28\n",
      "2017-07-12T16:07:30.475292 : step 741 loss [[ 51.59420013]]\n",
      "28\n",
      "2017-07-12T16:07:30.480296 : step 742 loss [[ 53.35908508]]\n",
      "28\n",
      "2017-07-12T16:07:30.484799 : step 743 loss [[ 47.9544487]]\n",
      "28\n",
      "2017-07-12T16:07:30.489687 : step 744 loss [[ 53.68707657]]\n",
      "28\n",
      "2017-07-12T16:07:30.494691 : step 745 loss [[ 42.46519852]]\n",
      "28\n",
      "2017-07-12T16:07:30.500695 : step 746 loss [[ 51.20928574]]\n",
      "28\n",
      "2017-07-12T16:07:30.504698 : step 747 loss [[ 53.26607132]]\n",
      "28\n",
      "2017-07-12T16:07:30.509701 : step 748 loss [[ 54.08509827]]\n",
      "28\n",
      "2017-07-12T16:07:30.514705 : step 749 loss [[ 50.37121964]]\n",
      "28\n",
      "2017-07-12T16:07:30.519709 : step 750 loss [[ 50.74825287]]\n",
      "28\n",
      "2017-07-12T16:07:30.524712 : step 751 loss [[ 44.01022339]]\n",
      "28\n",
      "2017-07-12T16:07:30.529716 : step 752 loss [[ 46.86004639]]\n",
      "28\n",
      "2017-07-12T16:07:30.534719 : step 753 loss [[ 52.71604919]]\n",
      "28\n",
      "2017-07-12T16:07:30.538722 : step 754 loss [[ 49.4793663]]\n",
      "10\n",
      "2017-07-12T16:07:30.543726 : step 755 loss [[ 54.59690857]]\n",
      "10\n",
      "2017-07-12T16:07:30.548729 : step 756 loss [[ 52.69958115]]\n",
      "10\n",
      "2017-07-12T16:07:30.553733 : step 757 loss [[ 53.08272552]]\n",
      "10\n",
      "2017-07-12T16:07:30.558736 : step 758 loss [[ 50.19609451]]\n",
      "10\n",
      "2017-07-12T16:07:30.562739 : step 759 loss [[ 50.07021713]]\n",
      "10\n",
      "2017-07-12T16:07:30.567743 : step 760 loss [[ 50.21614456]]\n",
      "10\n",
      "2017-07-12T16:07:30.573249 : step 761 loss [[ 51.78213882]]\n",
      "10\n",
      "2017-07-12T16:07:30.578252 : step 762 loss [[ 50.78466797]]\n",
      "10\n",
      "2017-07-12T16:07:30.583259 : step 763 loss [[ 51.03395462]]\n",
      "10\n",
      "2017-07-12T16:07:30.588259 : step 764 loss [[ 48.1098175]]\n",
      "6\n",
      "2017-07-12T16:07:30.592764 : step 765 loss [[ 51.30095291]]\n",
      "6\n",
      "2017-07-12T16:07:30.598768 : step 766 loss [[ 50.52803802]]\n",
      "6\n",
      "2017-07-12T16:07:30.603771 : step 767 loss [[ 55.16259384]]\n",
      "6\n",
      "2017-07-12T16:07:30.608775 : step 768 loss [[ 55.6016655]]\n",
      "6\n",
      "2017-07-12T16:07:30.613779 : step 769 loss [[ 53.43104553]]\n",
      "6\n",
      "2017-07-12T16:07:30.618782 : step 770 loss [[ 46.11867142]]\n",
      "cost:  [[ 37034.48828125]]\n",
      "19\n",
      "2017-07-12T16:07:30.623786 : step 771 loss [[ 49.18026352]]\n",
      "19\n",
      "2017-07-12T16:07:30.628789 : step 772 loss [[ 51.48698044]]\n",
      "19\n",
      "2017-07-12T16:07:30.633793 : step 773 loss [[ 44.81328201]]\n",
      "19\n",
      "2017-07-12T16:07:30.638797 : step 774 loss [[ 49.16159439]]\n",
      "19\n",
      "2017-07-12T16:07:30.644801 : step 775 loss [[ 46.3092308]]\n",
      "19\n",
      "2017-07-12T16:07:30.649804 : step 776 loss [[ 48.82818222]]\n",
      "19\n",
      "2017-07-12T16:07:30.658811 : step 777 loss [[ 50.13762665]]\n",
      "19\n",
      "2017-07-12T16:07:30.663814 : step 778 loss [[ 53.07768631]]\n",
      "19\n",
      "2017-07-12T16:07:30.668818 : step 779 loss [[ 54.86457062]]\n",
      "19\n",
      "2017-07-12T16:07:30.674829 : step 780 loss [[ 51.01808167]]\n",
      "19\n",
      "2017-07-12T16:07:30.679832 : step 781 loss [[ 51.8830452]]\n",
      "19\n",
      "2017-07-12T16:07:30.684836 : step 782 loss [[ 50.27949142]]\n",
      "19\n",
      "2017-07-12T16:07:30.689841 : step 783 loss [[ 51.56412888]]\n",
      "19\n",
      "2017-07-12T16:07:30.694844 : step 784 loss [[ 49.61228561]]\n",
      "19\n",
      "2017-07-12T16:07:30.699847 : step 785 loss [[ 48.9029274]]\n",
      "19\n",
      "2017-07-12T16:07:30.704852 : step 786 loss [[ 49.04597855]]\n",
      "19\n",
      "2017-07-12T16:07:30.709854 : step 787 loss [[ 53.95235825]]\n",
      "19\n",
      "2017-07-12T16:07:30.713857 : step 788 loss [[ 49.48067474]]\n",
      "19\n",
      "2017-07-12T16:07:30.719862 : step 789 loss [[ 47.46072006]]\n",
      "5\n",
      "2017-07-12T16:07:30.725866 : step 790 loss [[ 50.12790298]]\n",
      "5\n",
      "2017-07-12T16:07:30.730870 : step 791 loss [[ 51.49221802]]\n",
      "5\n",
      "2017-07-12T16:07:30.735873 : step 792 loss [[ 53.96160126]]\n",
      "5\n",
      "2017-07-12T16:07:30.740877 : step 793 loss [[ 51.44760132]]\n",
      "5\n",
      "2017-07-12T16:07:30.745880 : step 794 loss [[ 49.79395676]]\n",
      "16\n",
      "2017-07-12T16:07:30.750884 : step 795 loss [[ 46.33206558]]\n",
      "16\n",
      "2017-07-12T16:07:30.756888 : step 796 loss [[ 50.7258873]]\n",
      "16\n",
      "2017-07-12T16:07:30.760891 : step 797 loss [[ 48.42266846]]\n",
      "16\n",
      "2017-07-12T16:07:30.765894 : step 798 loss [[ 48.34308624]]\n",
      "16\n",
      "2017-07-12T16:07:30.770898 : step 799 loss [[ 48.82499695]]\n",
      "16\n",
      "2017-07-12T16:07:30.776405 : step 800 loss [[ 49.89736176]]\n",
      "16\n",
      "2017-07-12T16:07:30.781409 : step 801 loss [[ 49.00593185]]\n",
      "16\n",
      "2017-07-12T16:07:30.786412 : step 802 loss [[ 49.08710098]]\n",
      "16\n",
      "2017-07-12T16:07:30.791596 : step 803 loss [[ 44.53291321]]\n",
      "16\n",
      "2017-07-12T16:07:30.795598 : step 804 loss [[ 47.24097443]]\n",
      "16\n",
      "2017-07-12T16:07:30.800602 : step 805 loss [[ 50.22247314]]\n",
      "16\n",
      "2017-07-12T16:07:30.805605 : step 806 loss [[ 45.35230255]]\n",
      "16\n",
      "2017-07-12T16:07:30.810609 : step 807 loss [[ 51.25497437]]\n",
      "16\n",
      "2017-07-12T16:07:30.815612 : step 808 loss [[ 51.03324127]]\n",
      "16\n",
      "2017-07-12T16:07:30.820616 : step 809 loss [[ 47.63213348]]\n",
      "16\n",
      "2017-07-12T16:07:30.825620 : step 810 loss [[ 47.79923248]]\n",
      "cost:  [[ 39018.08203125]]\n",
      "24\n",
      "2017-07-12T16:07:30.830623 : step 811 loss [[ 48.76083374]]\n",
      "24\n",
      "2017-07-12T16:07:30.835627 : step 812 loss [[ 48.31893921]]\n",
      "24\n",
      "2017-07-12T16:07:30.840630 : step 813 loss [[ 51.26054001]]\n",
      "24\n",
      "2017-07-12T16:07:30.845634 : step 814 loss [[ 51.90022278]]\n",
      "24\n",
      "2017-07-12T16:07:30.851638 : step 815 loss [[ 50.35509872]]\n",
      "24\n",
      "2017-07-12T16:07:30.856642 : step 816 loss [[ 50.96585083]]\n",
      "24\n",
      "2017-07-12T16:07:30.861646 : step 817 loss [[ 48.61636353]]\n",
      "24\n",
      "2017-07-12T16:07:30.865648 : step 818 loss [[ 44.89712524]]\n",
      "24\n",
      "2017-07-12T16:07:30.871652 : step 819 loss [[ 45.16973114]]\n",
      "24\n",
      "2017-07-12T16:07:30.876659 : step 820 loss [[ 45.1294136]]\n",
      "24\n",
      "2017-07-12T16:07:30.881662 : step 821 loss [[ 46.18103409]]\n",
      "24\n",
      "2017-07-12T16:07:30.886166 : step 822 loss [[ 46.74113846]]\n",
      "24\n",
      "2017-07-12T16:07:30.890845 : step 823 loss [[ 48.54071808]]\n",
      "24\n",
      "2017-07-12T16:07:30.895849 : step 824 loss [[ 46.75580597]]\n",
      "24\n",
      "2017-07-12T16:07:30.900853 : step 825 loss [[ 48.28043365]]\n",
      "24\n",
      "2017-07-12T16:07:30.905857 : step 826 loss [[ 49.4259491]]\n",
      "24\n",
      "2017-07-12T16:07:30.910860 : step 827 loss [[ 50.39382553]]\n",
      "24\n",
      "2017-07-12T16:07:30.915863 : step 828 loss [[ 50.2799263]]\n",
      "24\n",
      "2017-07-12T16:07:30.920867 : step 829 loss [[ 47.28574371]]\n",
      "24\n",
      "2017-07-12T16:07:30.924870 : step 830 loss [[ 43.14208984]]\n",
      "24\n",
      "2017-07-12T16:07:30.929873 : step 831 loss [[ 46.01089478]]\n",
      "24\n",
      "2017-07-12T16:07:30.935878 : step 832 loss [[ 47.45307159]]\n",
      "24\n",
      "2017-07-12T16:07:30.940881 : step 833 loss [[ 51.21064377]]\n",
      "24\n",
      "2017-07-12T16:07:30.945885 : step 834 loss [[ 47.82995605]]\n",
      "7\n",
      "2017-07-12T16:07:30.951889 : step 835 loss [[ 44.6462326]]\n",
      "7\n",
      "2017-07-12T16:07:30.957893 : step 836 loss [[ 42.13230133]]\n",
      "7\n",
      "2017-07-12T16:07:30.962897 : step 837 loss [[ 43.25032043]]\n",
      "7\n",
      "2017-07-12T16:07:30.968901 : step 838 loss [[ 44.50152588]]\n",
      "7\n",
      "2017-07-12T16:07:30.974908 : step 839 loss [[ 49.04743195]]\n",
      "7\n",
      "2017-07-12T16:07:30.979912 : step 840 loss [[ 47.49661255]]\n",
      "7\n",
      "2017-07-12T16:07:30.984915 : step 841 loss [[ 45.78468704]]\n",
      "9\n",
      "2017-07-12T16:07:30.991060 : step 842 loss [[ 59.07252884]]\n",
      "9\n",
      "2017-07-12T16:07:30.996063 : step 843 loss [[ 59.3777771]]\n",
      "9\n",
      "2017-07-12T16:07:31.002068 : step 844 loss [[ 55.58628464]]\n",
      "9\n",
      "2017-07-12T16:07:31.008072 : step 845 loss [[ 56.42752075]]\n",
      "9\n",
      "2017-07-12T16:07:31.013075 : step 846 loss [[ 56.27498627]]\n",
      "9\n",
      "2017-07-12T16:07:31.018079 : step 847 loss [[ 55.15319824]]\n",
      "9\n",
      "2017-07-12T16:07:31.023083 : step 848 loss [[ 56.83231735]]\n",
      "9\n",
      "2017-07-12T16:07:31.028086 : step 849 loss [[ 57.87262726]]\n",
      "9\n",
      "2017-07-12T16:07:31.034091 : step 850 loss [[ 56.47327042]]\n",
      "cost:  [[ 41002.90625]]\n",
      "5\n",
      "2017-07-12T16:07:31.040095 : step 851 loss [[ 55.58489609]]\n",
      "5\n",
      "2017-07-12T16:07:31.045098 : step 852 loss [[ 50.89538193]]\n",
      "5\n",
      "2017-07-12T16:07:31.050102 : step 853 loss [[ 48.61720657]]\n",
      "5\n",
      "2017-07-12T16:07:31.055106 : step 854 loss [[ 48.75494003]]\n",
      "5\n",
      "2017-07-12T16:07:31.061110 : step 855 loss [[ 51.05704117]]\n",
      "cost:  [[ 41257.81640625]]\n",
      "12\n",
      "2017-07-12T16:07:31.066114 : step 856 loss [[ 47.59775162]]\n",
      "12\n",
      "2017-07-12T16:07:31.072117 : step 857 loss [[ 45.96495438]]\n",
      "12\n",
      "2017-07-12T16:07:31.076623 : step 858 loss [[ 46.44276047]]\n",
      "12\n",
      "2017-07-12T16:07:31.081627 : step 859 loss [[ 49.67389297]]\n",
      "12\n",
      "2017-07-12T16:07:31.086630 : step 860 loss [[ 47.2794342]]\n",
      "12\n",
      "2017-07-12T16:07:31.091637 : step 861 loss [[ 49.22190475]]\n",
      "12\n",
      "2017-07-12T16:07:31.096640 : step 862 loss [[ 48.43703842]]\n",
      "12\n",
      "2017-07-12T16:07:31.101643 : step 863 loss [[ 50.79250336]]\n",
      "12\n",
      "2017-07-12T16:07:31.105646 : step 864 loss [[ 51.15334702]]\n",
      "12\n",
      "2017-07-12T16:07:31.110650 : step 865 loss [[ 48.69124985]]\n",
      "12\n",
      "2017-07-12T16:07:31.115653 : step 866 loss [[ 47.49547195]]\n",
      "12\n",
      "2017-07-12T16:07:31.121658 : step 867 loss [[ 47.72513962]]\n",
      "10\n",
      "2017-07-12T16:07:31.126661 : step 868 loss [[ 55.57313538]]\n",
      "10\n",
      "2017-07-12T16:07:31.132666 : step 869 loss [[ 51.07765961]]\n",
      "10\n",
      "2017-07-12T16:07:31.137669 : step 870 loss [[ 51.75785446]]\n",
      "10\n",
      "2017-07-12T16:07:31.142673 : step 871 loss [[ 49.08787155]]\n",
      "10\n",
      "2017-07-12T16:07:31.147676 : step 872 loss [[ 49.9955101]]\n",
      "10\n",
      "2017-07-12T16:07:31.152680 : step 873 loss [[ 55.9440918]]\n",
      "10\n",
      "2017-07-12T16:07:31.157683 : step 874 loss [[ 53.89801025]]\n",
      "10\n",
      "2017-07-12T16:07:31.162687 : step 875 loss [[ 54.54085541]]\n",
      "10\n",
      "2017-07-12T16:07:31.167690 : step 876 loss [[ 51.01370239]]\n",
      "10\n",
      "2017-07-12T16:07:31.172694 : step 877 loss [[ 52.31206512]]\n",
      "11\n",
      "2017-07-12T16:07:31.178200 : step 878 loss [[ 49.14460373]]\n",
      "11\n",
      "2017-07-12T16:07:31.182703 : step 879 loss [[ 53.66951752]]\n",
      "11\n",
      "2017-07-12T16:07:31.188708 : step 880 loss [[ 53.29781723]]\n",
      "11\n",
      "2017-07-12T16:07:31.193278 : step 881 loss [[ 49.8885231]]\n",
      "11\n",
      "2017-07-12T16:07:31.198281 : step 882 loss [[ 43.49119949]]\n",
      "11\n",
      "2017-07-12T16:07:31.203285 : step 883 loss [[ 42.8067627]]\n",
      "11\n",
      "2017-07-12T16:07:31.208288 : step 884 loss [[ 49.29893112]]\n",
      "11\n",
      "2017-07-12T16:07:31.214292 : step 885 loss [[ 49.1670723]]\n",
      "11\n",
      "2017-07-12T16:07:31.220297 : step 886 loss [[ 47.08871078]]\n",
      "11\n",
      "2017-07-12T16:07:31.225300 : step 887 loss [[ 46.75238419]]\n",
      "11\n",
      "2017-07-12T16:07:31.230304 : step 888 loss [[ 47.65452957]]\n",
      "5\n",
      "2017-07-12T16:07:31.235307 : step 889 loss [[ 52.9707756]]\n",
      "5\n",
      "2017-07-12T16:07:31.240311 : step 890 loss [[ 54.98601532]]\n",
      "5\n",
      "2017-07-12T16:07:31.244313 : step 891 loss [[ 55.36100006]]\n",
      "5\n",
      "2017-07-12T16:07:31.250318 : step 892 loss [[ 55.38088989]]\n",
      "5\n",
      "2017-07-12T16:07:31.255322 : step 893 loss [[ 53.41383362]]\n",
      "6\n",
      "2017-07-12T16:07:31.260325 : step 894 loss [[ 48.39146042]]\n",
      "6\n",
      "2017-07-12T16:07:31.265328 : step 895 loss [[ 49.97388077]]\n",
      "6\n",
      "2017-07-12T16:07:31.271334 : step 896 loss [[ 48.63409042]]\n",
      "6\n",
      "2017-07-12T16:07:31.278342 : step 897 loss [[ 50.28939438]]\n",
      "6\n",
      "2017-07-12T16:07:31.283345 : step 898 loss [[ 50.96264648]]\n",
      "6\n",
      "2017-07-12T16:07:31.288849 : step 899 loss [[ 54.75924683]]\n",
      "21\n",
      "2017-07-12T16:07:31.293535 : step 900 loss [[ 42.85284042]]\n",
      "21\n",
      "2017-07-12T16:07:31.298538 : step 901 loss [[ 46.1836319]]\n",
      "21\n",
      "2017-07-12T16:07:31.303542 : step 902 loss [[ 46.80832672]]\n",
      "21\n",
      "2017-07-12T16:07:31.308545 : step 903 loss [[ 46.97509003]]\n",
      "21\n",
      "2017-07-12T16:07:31.313549 : step 904 loss [[ 49.55426788]]\n",
      "21\n",
      "2017-07-12T16:07:31.319553 : step 905 loss [[ 52.79958725]]\n",
      "21\n",
      "2017-07-12T16:07:31.324557 : step 906 loss [[ 51.76681519]]\n",
      "21\n",
      "2017-07-12T16:07:31.329560 : step 907 loss [[ 43.71026611]]\n",
      "21\n",
      "2017-07-12T16:07:31.335567 : step 908 loss [[ 48.26593018]]\n",
      "21\n",
      "2017-07-12T16:07:31.341569 : step 909 loss [[ 48.64084625]]\n",
      "21\n",
      "2017-07-12T16:07:31.346572 : step 910 loss [[ 50.05578613]]\n",
      "21\n",
      "2017-07-12T16:07:31.351576 : step 911 loss [[ 48.43722534]]\n",
      "21\n",
      "2017-07-12T16:07:31.357580 : step 912 loss [[ 48.98157501]]\n",
      "21\n",
      "2017-07-12T16:07:31.361583 : step 913 loss [[ 47.21199799]]\n",
      "21\n",
      "2017-07-12T16:07:31.367588 : step 914 loss [[ 44.48447418]]\n",
      "21\n",
      "2017-07-12T16:07:31.373592 : step 915 loss [[ 43.98852921]]\n",
      "21\n",
      "2017-07-12T16:07:31.378598 : step 916 loss [[ 43.90140533]]\n",
      "21\n",
      "2017-07-12T16:07:31.384102 : step 917 loss [[ 44.05029297]]\n",
      "21\n",
      "2017-07-12T16:07:31.389105 : step 918 loss [[ 39.54676819]]\n",
      "21\n",
      "2017-07-12T16:07:31.394110 : step 919 loss [[ 42.34308624]]\n",
      "21\n",
      "2017-07-12T16:07:31.399114 : step 920 loss [[ 45.225811]]\n",
      "cost:  [[ 44446.66796875]]\n",
      "61\n",
      "2017-07-12T16:07:31.406119 : step 921 loss [[ 43.82298279]]\n",
      "61\n",
      "2017-07-12T16:07:31.411122 : step 922 loss [[ 44.85228348]]\n",
      "61\n",
      "2017-07-12T16:07:31.416126 : step 923 loss [[ 40.88563538]]\n",
      "61\n",
      "2017-07-12T16:07:31.421129 : step 924 loss [[ 51.28657532]]\n",
      "61\n",
      "2017-07-12T16:07:31.426133 : step 925 loss [[ 53.41114807]]\n",
      "61\n",
      "2017-07-12T16:07:31.430136 : step 926 loss [[ 54.53130341]]\n",
      "61\n",
      "2017-07-12T16:07:31.436140 : step 927 loss [[ 50.4543457]]\n",
      "61\n",
      "2017-07-12T16:07:31.441144 : step 928 loss [[ 49.17676544]]\n",
      "61\n",
      "2017-07-12T16:07:31.446147 : step 929 loss [[ 49.48405075]]\n",
      "61\n",
      "2017-07-12T16:07:31.451151 : step 930 loss [[ 45.15984726]]\n",
      "61\n",
      "2017-07-12T16:07:31.456154 : step 931 loss [[ 46.04238129]]\n",
      "61\n",
      "2017-07-12T16:07:31.461158 : step 932 loss [[ 52.96073914]]\n",
      "61\n",
      "2017-07-12T16:07:31.466162 : step 933 loss [[ 49.24501038]]\n",
      "61\n",
      "2017-07-12T16:07:31.472166 : step 934 loss [[ 44.24017334]]\n",
      "61\n",
      "2017-07-12T16:07:31.478173 : step 935 loss [[ 43.86505127]]\n",
      "61\n",
      "2017-07-12T16:07:31.483677 : step 936 loss [[ 58.60483551]]\n",
      "61\n",
      "2017-07-12T16:07:31.488680 : step 937 loss [[ 38.57619095]]\n",
      "61\n",
      "2017-07-12T16:07:31.493996 : step 938 loss [[ 41.99248123]]\n",
      "61\n",
      "2017-07-12T16:07:31.499000 : step 939 loss [[ 63.32187653]]\n",
      "61\n",
      "2017-07-12T16:07:31.504003 : step 940 loss [[ 62.63907623]]\n",
      "61\n",
      "2017-07-12T16:07:31.510007 : step 941 loss [[ 60.01259995]]\n",
      "61\n",
      "2017-07-12T16:07:31.515011 : step 942 loss [[ 49.37693787]]\n",
      "61\n",
      "2017-07-12T16:07:31.520015 : step 943 loss [[ 56.71189117]]\n",
      "61\n",
      "2017-07-12T16:07:31.525018 : step 944 loss [[ 54.16228485]]\n",
      "61\n",
      "2017-07-12T16:07:31.530022 : step 945 loss [[ 54.04787064]]\n",
      "61\n",
      "2017-07-12T16:07:31.535025 : step 946 loss [[ 51.86420822]]\n",
      "61\n",
      "2017-07-12T16:07:31.541029 : step 947 loss [[ 56.93663025]]\n",
      "61\n",
      "2017-07-12T16:07:31.546033 : step 948 loss [[ 44.20833588]]\n",
      "61\n",
      "2017-07-12T16:07:31.551037 : step 949 loss [[ 41.4980545]]\n",
      "61\n",
      "2017-07-12T16:07:31.556040 : step 950 loss [[ 40.2440033]]\n",
      "61\n",
      "2017-07-12T16:07:31.561043 : step 951 loss [[ 41.72182465]]\n",
      "61\n",
      "2017-07-12T16:07:31.566047 : step 952 loss [[ 44.77830505]]\n",
      "61\n",
      "2017-07-12T16:07:31.571051 : step 953 loss [[ 40.19629669]]\n",
      "61\n",
      "2017-07-12T16:07:31.576559 : step 954 loss [[ 43.61923218]]\n",
      "61\n",
      "2017-07-12T16:07:31.581563 : step 955 loss [[ 43.77436066]]\n",
      "61\n",
      "2017-07-12T16:07:31.586566 : step 956 loss [[ 47.09357452]]\n",
      "61\n",
      "2017-07-12T16:07:31.592072 : step 957 loss [[ 44.90648651]]\n",
      "61\n",
      "2017-07-12T16:07:31.597076 : step 958 loss [[ 47.11305237]]\n",
      "61\n",
      "2017-07-12T16:07:31.602079 : step 959 loss [[ 49.21745682]]\n",
      "61\n",
      "2017-07-12T16:07:31.607083 : step 960 loss [[ 46.07401276]]\n",
      "61\n",
      "2017-07-12T16:07:31.612086 : step 961 loss [[ 52.95620728]]\n",
      "61\n",
      "2017-07-12T16:07:31.617091 : step 962 loss [[ 51.72814941]]\n",
      "61\n",
      "2017-07-12T16:07:31.622094 : step 963 loss [[ 49.10853577]]\n",
      "61\n",
      "2017-07-12T16:07:31.627097 : step 964 loss [[ 44.99693298]]\n",
      "61\n",
      "2017-07-12T16:07:31.632100 : step 965 loss [[ 51.80076218]]\n",
      "61\n",
      "2017-07-12T16:07:31.637104 : step 966 loss [[ 52.17564011]]\n",
      "61\n",
      "2017-07-12T16:07:31.642108 : step 967 loss [[ 48.7648735]]\n",
      "61\n",
      "2017-07-12T16:07:31.646110 : step 968 loss [[ 47.92525101]]\n",
      "61\n",
      "2017-07-12T16:07:31.652115 : step 969 loss [[ 49.51165771]]\n",
      "61\n",
      "2017-07-12T16:07:31.658119 : step 970 loss [[ 47.06675339]]\n",
      "61\n",
      "2017-07-12T16:07:31.663123 : step 971 loss [[ 48.36092758]]\n",
      "61\n",
      "2017-07-12T16:07:31.668126 : step 972 loss [[ 47.75559616]]\n",
      "61\n",
      "2017-07-12T16:07:31.673130 : step 973 loss [[ 49.00772095]]\n",
      "61\n",
      "2017-07-12T16:07:31.677635 : step 974 loss [[ 49.46738815]]\n",
      "61\n",
      "2017-07-12T16:07:31.682639 : step 975 loss [[ 42.49950027]]\n",
      "61\n",
      "2017-07-12T16:07:31.688143 : step 976 loss [[ 51.49700928]]\n",
      "61\n",
      "2017-07-12T16:07:31.693148 : step 977 loss [[ 46.31938553]]\n",
      "61\n",
      "2017-07-12T16:07:31.698152 : step 978 loss [[ 43.40671158]]\n",
      "61\n",
      "2017-07-12T16:07:31.703156 : step 979 loss [[ 45.07595444]]\n",
      "61\n",
      "2017-07-12T16:07:31.708159 : step 980 loss [[ 46.62992477]]\n",
      "61\n",
      "2017-07-12T16:07:31.712162 : step 981 loss [[ 42.90958786]]\n",
      "9\n",
      "2017-07-12T16:07:31.717165 : step 982 loss [[ 55.39307404]]\n",
      "9\n",
      "2017-07-12T16:07:31.722169 : step 983 loss [[ 50.70002365]]\n",
      "9\n",
      "2017-07-12T16:07:31.727173 : step 984 loss [[ 49.75795364]]\n",
      "9\n",
      "2017-07-12T16:07:31.732176 : step 985 loss [[ 47.29887009]]\n",
      "9\n",
      "2017-07-12T16:07:31.737180 : step 986 loss [[ 48.57640839]]\n",
      "9\n",
      "2017-07-12T16:07:31.742183 : step 987 loss [[ 49.38999557]]\n",
      "9\n",
      "2017-07-12T16:07:31.747187 : step 988 loss [[ 47.72673035]]\n",
      "9\n",
      "2017-07-12T16:07:31.752191 : step 989 loss [[ 49.35847473]]\n",
      "9\n",
      "2017-07-12T16:07:31.758195 : step 990 loss [[ 51.51159668]]\n",
      "cost:  [[ 47847.453125]]\n",
      "29\n",
      "2017-07-12T16:07:31.763198 : step 991 loss [[ 46.82954025]]\n",
      "29\n",
      "2017-07-12T16:07:31.768202 : step 992 loss [[ 54.85349274]]\n",
      "29\n",
      "2017-07-12T16:07:31.773206 : step 993 loss [[ 55.03429794]]\n",
      "29\n",
      "2017-07-12T16:07:31.778711 : step 994 loss [[ 56.12673569]]\n",
      "29\n",
      "2017-07-12T16:07:31.784216 : step 995 loss [[ 53.28325272]]\n",
      "29\n",
      "2017-07-12T16:07:31.789219 : step 996 loss [[ 47.72328568]]\n",
      "29\n",
      "2017-07-12T16:07:31.794225 : step 997 loss [[ 46.25906754]]\n",
      "29\n",
      "2017-07-12T16:07:31.798227 : step 998 loss [[ 46.43630981]]\n",
      "29\n",
      "2017-07-12T16:07:31.804232 : step 999 loss [[ 52.01726151]]\n",
      "29\n",
      "2017-07-12T16:07:31.809235 : step 1000 loss [[ 53.68441391]]\n",
      "29\n",
      "2017-07-12T16:07:31.814239 : step 1001 loss [[ 54.87497711]]\n",
      "29\n",
      "2017-07-12T16:07:31.819242 : step 1002 loss [[ 55.18373489]]\n",
      "29\n",
      "2017-07-12T16:07:31.824246 : step 1003 loss [[ 54.89421463]]\n",
      "29\n",
      "2017-07-12T16:07:31.829250 : step 1004 loss [[ 48.91172791]]\n",
      "29\n",
      "2017-07-12T16:07:31.834253 : step 1005 loss [[ 50.42846298]]\n",
      "29\n",
      "2017-07-12T16:07:31.839258 : step 1006 loss [[ 48.56809616]]\n",
      "29\n",
      "2017-07-12T16:07:31.844260 : step 1007 loss [[ 49.89892197]]\n",
      "29\n",
      "2017-07-12T16:07:31.849264 : step 1008 loss [[ 53.6603508]]\n",
      "29\n",
      "2017-07-12T16:07:31.854267 : step 1009 loss [[ 53.82650757]]\n",
      "29\n",
      "2017-07-12T16:07:31.859271 : step 1010 loss [[ 48.43357468]]\n",
      "29\n",
      "2017-07-12T16:07:31.864274 : step 1011 loss [[ 49.92209244]]\n",
      "29\n",
      "2017-07-12T16:07:31.869278 : step 1012 loss [[ 48.96797562]]\n",
      "29\n",
      "2017-07-12T16:07:31.875285 : step 1013 loss [[ 52.41680527]]\n",
      "29\n",
      "2017-07-12T16:07:31.880289 : step 1014 loss [[ 44.61204529]]\n",
      "29\n",
      "2017-07-12T16:07:31.884792 : step 1015 loss [[ 45.16996765]]\n",
      "29\n",
      "2017-07-12T16:07:31.889795 : step 1016 loss [[ 45.96073151]]\n",
      "29\n",
      "2017-07-12T16:07:31.894836 : step 1017 loss [[ 52.34020996]]\n",
      "29\n",
      "2017-07-12T16:07:31.898839 : step 1018 loss [[ 50.75086975]]\n",
      "29\n",
      "2017-07-12T16:07:31.903842 : step 1019 loss [[ 50.77027893]]\n",
      "11\n",
      "2017-07-12T16:07:31.909846 : step 1020 loss [[ 56.10712051]]\n",
      "11\n",
      "2017-07-12T16:07:31.913849 : step 1021 loss [[ 59.10560608]]\n",
      "11\n",
      "2017-07-12T16:07:31.918853 : step 1022 loss [[ 64.16956329]]\n",
      "11\n",
      "2017-07-12T16:07:31.923857 : step 1023 loss [[ 58.96522522]]\n",
      "11\n",
      "2017-07-12T16:07:31.928860 : step 1024 loss [[ 56.38625336]]\n",
      "11\n",
      "2017-07-12T16:07:31.933865 : step 1025 loss [[ 56.91971588]]\n",
      "11\n",
      "2017-07-12T16:07:31.939868 : step 1026 loss [[ 57.73031235]]\n",
      "11\n",
      "2017-07-12T16:07:31.944872 : step 1027 loss [[ 58.19227219]]\n",
      "11\n",
      "2017-07-12T16:07:31.949875 : step 1028 loss [[ 57.75854111]]\n",
      "11\n",
      "2017-07-12T16:07:31.954878 : step 1029 loss [[ 55.62012863]]\n",
      "11\n",
      "2017-07-12T16:07:31.960883 : step 1030 loss [[ 59.50629425]]\n",
      "cost:  [[ 49959.74609375]]\n",
      "10\n",
      "2017-07-12T16:07:31.966888 : step 1031 loss [[ 53.38911438]]\n",
      "10\n",
      "2017-07-12T16:07:31.971891 : step 1032 loss [[ 53.59407425]]\n",
      "10\n",
      "2017-07-12T16:07:31.977398 : step 1033 loss [[ 51.14022827]]\n",
      "10\n",
      "2017-07-12T16:07:31.981902 : step 1034 loss [[ 47.29368973]]\n",
      "10\n",
      "2017-07-12T16:07:31.986905 : step 1035 loss [[ 57.22686386]]\n",
      "10\n",
      "2017-07-12T16:07:31.991413 : step 1036 loss [[ 58.92789459]]\n",
      "10\n",
      "2017-07-12T16:07:31.996417 : step 1037 loss [[ 55.87737274]]\n",
      "10\n",
      "2017-07-12T16:07:32.001420 : step 1038 loss [[ 53.98153305]]\n",
      "10\n",
      "2017-07-12T16:07:32.006423 : step 1039 loss [[ 55.90528107]]\n",
      "10\n",
      "2017-07-12T16:07:32.011427 : step 1040 loss [[ 54.50941467]]\n",
      "cost:  [[ 50501.59375]]\n",
      "15\n",
      "2017-07-12T16:07:32.017431 : step 1041 loss [[ 44.76664352]]\n",
      "15\n",
      "2017-07-12T16:07:32.021434 : step 1042 loss [[ 46.73694611]]\n",
      "15\n",
      "2017-07-12T16:07:32.026438 : step 1043 loss [[ 51.23521423]]\n",
      "15\n",
      "2017-07-12T16:07:32.031441 : step 1044 loss [[ 51.58014679]]\n",
      "15\n",
      "2017-07-12T16:07:32.036445 : step 1045 loss [[ 48.33676529]]\n",
      "15\n",
      "2017-07-12T16:07:32.041448 : step 1046 loss [[ 48.30216599]]\n",
      "15\n",
      "2017-07-12T16:07:32.046452 : step 1047 loss [[ 46.68643951]]\n",
      "15\n",
      "2017-07-12T16:07:32.051455 : step 1048 loss [[ 50.08829117]]\n",
      "15\n",
      "2017-07-12T16:07:32.056459 : step 1049 loss [[ 48.06424713]]\n",
      "15\n",
      "2017-07-12T16:07:32.062464 : step 1050 loss [[ 52.4084816]]\n",
      "15\n",
      "2017-07-12T16:07:32.066466 : step 1051 loss [[ 50.74315643]]\n",
      "15\n",
      "2017-07-12T16:07:32.071470 : step 1052 loss [[ 47.25611115]]\n",
      "15\n",
      "2017-07-12T16:07:32.076979 : step 1053 loss [[ 45.91440201]]\n",
      "15\n",
      "2017-07-12T16:07:32.082483 : step 1054 loss [[ 51.48296738]]\n",
      "15\n",
      "2017-07-12T16:07:32.087487 : step 1055 loss [[ 50.00197983]]\n",
      "cost:  [[ 51235.203125]]\n",
      "23\n",
      "2017-07-12T16:07:32.092300 : step 1056 loss [[ 51.82590485]]\n",
      "23\n",
      "2017-07-12T16:07:32.097303 : step 1057 loss [[ 52.08761978]]\n",
      "23\n",
      "2017-07-12T16:07:32.102307 : step 1058 loss [[ 51.38558197]]\n",
      "23\n",
      "2017-07-12T16:07:32.107311 : step 1059 loss [[ 50.1087532]]\n",
      "23\n",
      "2017-07-12T16:07:32.112314 : step 1060 loss [[ 46.2214241]]\n",
      "23\n",
      "2017-07-12T16:07:32.116317 : step 1061 loss [[ 57.50672531]]\n",
      "23\n",
      "2017-07-12T16:07:32.122322 : step 1062 loss [[ 51.44841003]]\n",
      "23\n",
      "2017-07-12T16:07:32.127325 : step 1063 loss [[ 50.47170258]]\n",
      "23\n",
      "2017-07-12T16:07:32.132328 : step 1064 loss [[ 52.03016281]]\n",
      "23\n",
      "2017-07-12T16:07:32.137332 : step 1065 loss [[ 52.6580162]]\n",
      "23\n",
      "2017-07-12T16:07:32.142335 : step 1066 loss [[ 46.91123962]]\n",
      "23\n",
      "2017-07-12T16:07:32.147339 : step 1067 loss [[ 48.72779083]]\n",
      "23\n",
      "2017-07-12T16:07:32.151342 : step 1068 loss [[ 51.25107193]]\n",
      "23\n",
      "2017-07-12T16:07:32.157346 : step 1069 loss [[ 51.82030106]]\n",
      "23\n",
      "2017-07-12T16:07:32.162350 : step 1070 loss [[ 50.11683273]]\n",
      "23\n",
      "2017-07-12T16:07:32.166352 : step 1071 loss [[ 49.87109375]]\n",
      "23\n",
      "2017-07-12T16:07:32.171356 : step 1072 loss [[ 48.70478439]]\n",
      "23\n",
      "2017-07-12T16:07:32.176362 : step 1073 loss [[ 52.99825668]]\n",
      "23\n",
      "2017-07-12T16:07:32.181365 : step 1074 loss [[ 50.68288803]]\n",
      "23\n",
      "2017-07-12T16:07:32.186368 : step 1075 loss [[ 49.8973465]]\n",
      "23\n",
      "2017-07-12T16:07:32.191011 : step 1076 loss [[ 48.35190201]]\n",
      "23\n",
      "2017-07-12T16:07:32.196015 : step 1077 loss [[ 46.61394501]]\n",
      "23\n",
      "2017-07-12T16:07:32.201018 : step 1078 loss [[ 48.47103882]]\n",
      "56\n",
      "2017-07-12T16:07:32.206022 : step 1079 loss [[ 54.29825211]]\n",
      "56\n",
      "2017-07-12T16:07:32.211026 : step 1080 loss [[ 49.85721207]]\n",
      "56\n",
      "2017-07-12T16:07:32.216029 : step 1081 loss [[ 45.08977127]]\n",
      "56\n",
      "2017-07-12T16:07:32.222033 : step 1082 loss [[ 52.16281509]]\n",
      "56\n",
      "2017-07-12T16:07:32.227037 : step 1083 loss [[ 52.70071411]]\n",
      "56\n",
      "2017-07-12T16:07:32.231040 : step 1084 loss [[ 53.27207947]]\n",
      "56\n",
      "2017-07-12T16:07:32.236043 : step 1085 loss [[ 53.29946518]]\n",
      "56\n",
      "2017-07-12T16:07:32.241047 : step 1086 loss [[ 52.22636032]]\n",
      "56\n",
      "2017-07-12T16:07:32.246050 : step 1087 loss [[ 50.73388672]]\n",
      "56\n",
      "2017-07-12T16:07:32.251054 : step 1088 loss [[ 51.67382812]]\n",
      "56\n",
      "2017-07-12T16:07:32.256057 : step 1089 loss [[ 45.22285461]]\n",
      "56\n",
      "2017-07-12T16:07:32.261061 : step 1090 loss [[ 46.33812332]]\n",
      "56\n",
      "2017-07-12T16:07:32.266065 : step 1091 loss [[ 49.13579178]]\n",
      "56\n",
      "2017-07-12T16:07:32.270067 : step 1092 loss [[ 50.10091019]]\n",
      "56\n",
      "2017-07-12T16:07:32.275573 : step 1093 loss [[ 49.61111832]]\n",
      "56\n",
      "2017-07-12T16:07:32.280577 : step 1094 loss [[ 53.26378632]]\n",
      "56\n",
      "2017-07-12T16:07:32.285581 : step 1095 loss [[ 52.2409668]]\n",
      "56\n",
      "2017-07-12T16:07:32.291145 : step 1096 loss [[ 50.91851807]]\n",
      "56\n",
      "2017-07-12T16:07:32.296149 : step 1097 loss [[ 53.48138428]]\n",
      "56\n",
      "2017-07-12T16:07:32.301152 : step 1098 loss [[ 49.89569473]]\n",
      "56\n",
      "2017-07-12T16:07:32.306156 : step 1099 loss [[ 51.30406952]]\n",
      "56\n",
      "2017-07-12T16:07:32.311159 : step 1100 loss [[ 51.67753983]]\n",
      "56\n",
      "2017-07-12T16:07:32.316163 : step 1101 loss [[ 53.91181564]]\n",
      "56\n",
      "2017-07-12T16:07:32.321166 : step 1102 loss [[ 54.08059311]]\n",
      "56\n",
      "2017-07-12T16:07:32.326170 : step 1103 loss [[ 50.53955078]]\n",
      "56\n",
      "2017-07-12T16:07:32.331174 : step 1104 loss [[ 52.21565628]]\n",
      "56\n",
      "2017-07-12T16:07:32.335176 : step 1105 loss [[ 50.11016083]]\n",
      "56\n",
      "2017-07-12T16:07:32.340180 : step 1106 loss [[ 55.40239716]]\n",
      "56\n",
      "2017-07-12T16:07:32.346185 : step 1107 loss [[ 48.91758728]]\n",
      "56\n",
      "2017-07-12T16:07:32.351188 : step 1108 loss [[ 48.46726227]]\n",
      "56\n",
      "2017-07-12T16:07:32.357192 : step 1109 loss [[ 47.20291901]]\n",
      "56\n",
      "2017-07-12T16:07:32.361195 : step 1110 loss [[ 49.54145813]]\n",
      "56\n",
      "2017-07-12T16:07:32.367199 : step 1111 loss [[ 49.56682205]]\n",
      "56\n",
      "2017-07-12T16:07:32.372203 : step 1112 loss [[ 51.63459396]]\n",
      "56\n",
      "2017-07-12T16:07:32.377711 : step 1113 loss [[ 49.20011902]]\n",
      "56\n",
      "2017-07-12T16:07:32.383215 : step 1114 loss [[ 48.05509949]]\n",
      "56\n",
      "2017-07-12T16:07:32.388218 : step 1115 loss [[ 48.86197662]]\n",
      "56\n",
      "2017-07-12T16:07:32.393258 : step 1116 loss [[ 47.10063934]]\n",
      "56\n",
      "2017-07-12T16:07:32.398261 : step 1117 loss [[ 51.16030121]]\n",
      "56\n",
      "2017-07-12T16:07:32.403265 : step 1118 loss [[ 51.35630035]]\n",
      "56\n",
      "2017-07-12T16:07:32.408268 : step 1119 loss [[ 48.84268951]]\n",
      "56\n",
      "2017-07-12T16:07:32.413272 : step 1120 loss [[ 52.3423233]]\n",
      "56\n",
      "2017-07-12T16:07:32.418276 : step 1121 loss [[ 51.21720123]]\n",
      "56\n",
      "2017-07-12T16:07:32.423279 : step 1122 loss [[ 47.84308624]]\n",
      "56\n",
      "2017-07-12T16:07:32.428282 : step 1123 loss [[ 49.13237762]]\n",
      "56\n",
      "2017-07-12T16:07:32.433286 : step 1124 loss [[ 53.83995056]]\n",
      "56\n",
      "2017-07-12T16:07:32.438290 : step 1125 loss [[ 53.13889313]]\n",
      "56\n",
      "2017-07-12T16:07:32.443293 : step 1126 loss [[ 51.30151367]]\n",
      "56\n",
      "2017-07-12T16:07:32.447296 : step 1127 loss [[ 47.8212204]]\n",
      "56\n",
      "2017-07-12T16:07:32.452300 : step 1128 loss [[ 48.95719528]]\n",
      "56\n",
      "2017-07-12T16:07:32.457303 : step 1129 loss [[ 49.49897766]]\n",
      "56\n",
      "2017-07-12T16:07:32.461306 : step 1130 loss [[ 50.31035995]]\n",
      "56\n",
      "2017-07-12T16:07:32.466310 : step 1131 loss [[ 45.87949371]]\n",
      "56\n",
      "2017-07-12T16:07:32.471313 : step 1132 loss [[ 47.4231987]]\n",
      "56\n",
      "2017-07-12T16:07:32.476319 : step 1133 loss [[ 51.23669434]]\n",
      "56\n",
      "2017-07-12T16:07:32.481322 : step 1134 loss [[ 51.92959213]]\n",
      "5\n",
      "2017-07-12T16:07:32.486326 : step 1135 loss [[ 52.21604919]]\n",
      "5\n",
      "2017-07-12T16:07:32.490829 : step 1136 loss [[ 51.29056549]]\n",
      "5\n",
      "2017-07-12T16:07:32.495835 : step 1137 loss [[ 52.62895584]]\n",
      "5\n",
      "2017-07-12T16:07:32.500838 : step 1138 loss [[ 52.74476242]]\n",
      "5\n",
      "2017-07-12T16:07:32.505841 : step 1139 loss [[ 52.91136169]]\n",
      "43\n",
      "2017-07-12T16:07:32.510845 : step 1140 loss [[ 49.69591141]]\n",
      "43\n",
      "2017-07-12T16:07:32.515849 : step 1141 loss [[ 49.19857788]]\n",
      "43\n",
      "2017-07-12T16:07:32.520852 : step 1142 loss [[ 46.40697098]]\n",
      "43\n",
      "2017-07-12T16:07:32.525856 : step 1143 loss [[ 46.23488998]]\n",
      "43\n",
      "2017-07-12T16:07:32.530860 : step 1144 loss [[ 44.39491272]]\n",
      "43\n",
      "2017-07-12T16:07:32.535863 : step 1145 loss [[ 45.6512413]]\n",
      "43\n",
      "2017-07-12T16:07:32.540867 : step 1146 loss [[ 44.24148178]]\n",
      "43\n",
      "2017-07-12T16:07:32.544870 : step 1147 loss [[ 46.40604782]]\n",
      "43\n",
      "2017-07-12T16:07:32.549873 : step 1148 loss [[ 50.84701157]]\n",
      "43\n",
      "2017-07-12T16:07:32.554877 : step 1149 loss [[ 51.10610962]]\n",
      "43\n",
      "2017-07-12T16:07:32.560881 : step 1150 loss [[ 52.28316498]]\n",
      "43\n",
      "2017-07-12T16:07:32.565884 : step 1151 loss [[ 52.30659103]]\n",
      "43\n",
      "2017-07-12T16:07:32.571889 : step 1152 loss [[ 49.86569977]]\n",
      "43\n",
      "2017-07-12T16:07:32.576895 : step 1153 loss [[ 53.26445007]]\n",
      "43\n",
      "2017-07-12T16:07:32.582399 : step 1154 loss [[ 53.3409996]]\n",
      "43\n",
      "2017-07-12T16:07:32.587402 : step 1155 loss [[ 52.98207855]]\n",
      "43\n",
      "2017-07-12T16:07:32.591836 : step 1156 loss [[ 50.44669724]]\n",
      "43\n",
      "2017-07-12T16:07:32.597839 : step 1157 loss [[ 46.89408112]]\n",
      "43\n",
      "2017-07-12T16:07:32.602843 : step 1158 loss [[ 48.09342575]]\n",
      "43\n",
      "2017-07-12T16:07:32.607847 : step 1159 loss [[ 51.62992859]]\n",
      "43\n",
      "2017-07-12T16:07:32.612850 : step 1160 loss [[ 45.65130997]]\n",
      "43\n",
      "2017-07-12T16:07:32.618854 : step 1161 loss [[ 44.14447021]]\n",
      "43\n",
      "2017-07-12T16:07:32.624859 : step 1162 loss [[ 45.57082748]]\n",
      "43\n",
      "2017-07-12T16:07:32.629862 : step 1163 loss [[ 51.48445511]]\n",
      "43\n",
      "2017-07-12T16:07:32.635866 : step 1164 loss [[ 49.05764771]]\n",
      "43\n",
      "2017-07-12T16:07:32.640870 : step 1165 loss [[ 48.81858063]]\n",
      "43\n",
      "2017-07-12T16:07:32.644873 : step 1166 loss [[ 49.22926712]]\n",
      "43\n",
      "2017-07-12T16:07:32.649876 : step 1167 loss [[ 51.50229645]]\n",
      "43\n",
      "2017-07-12T16:07:32.658883 : step 1168 loss [[ 53.71099472]]\n",
      "43\n",
      "2017-07-12T16:07:32.663886 : step 1169 loss [[ 55.33172989]]\n",
      "43\n",
      "2017-07-12T16:07:32.668890 : step 1170 loss [[ 50.17201996]]\n",
      "43\n",
      "2017-07-12T16:07:32.673893 : step 1171 loss [[ 46.75234222]]\n",
      "43\n",
      "2017-07-12T16:07:32.679400 : step 1172 loss [[ 52.51599503]]\n",
      "43\n",
      "2017-07-12T16:07:32.685404 : step 1173 loss [[ 54.04782486]]\n",
      "43\n",
      "2017-07-12T16:07:32.690407 : step 1174 loss [[ 51.93349075]]\n",
      "43\n",
      "2017-07-12T16:07:32.695437 : step 1175 loss [[ 51.41129303]]\n",
      "43\n",
      "2017-07-12T16:07:32.700440 : step 1176 loss [[ 52.56147385]]\n",
      "43\n",
      "2017-07-12T16:07:32.705445 : step 1177 loss [[ 54.34633255]]\n",
      "43\n",
      "2017-07-12T16:07:32.711448 : step 1178 loss [[ 53.99836349]]\n",
      "43\n",
      "2017-07-12T16:07:32.718453 : step 1179 loss [[ 53.07729721]]\n",
      "43\n",
      "2017-07-12T16:07:32.723457 : step 1180 loss [[ 53.67047501]]\n",
      "43\n",
      "2017-07-12T16:07:32.728460 : step 1181 loss [[ 53.07913589]]\n",
      "43\n",
      "2017-07-12T16:07:32.733464 : step 1182 loss [[ 52.21574783]]\n",
      "6\n",
      "2017-07-12T16:07:32.739468 : step 1183 loss [[ 51.77403259]]\n",
      "6\n",
      "2017-07-12T16:07:32.744472 : step 1184 loss [[ 51.18340302]]\n",
      "6\n",
      "2017-07-12T16:07:32.750477 : step 1185 loss [[ 50.24332428]]\n",
      "6\n",
      "2017-07-12T16:07:32.754479 : step 1186 loss [[ 49.82024002]]\n",
      "6\n",
      "2017-07-12T16:07:32.759482 : step 1187 loss [[ 51.02677917]]\n",
      "6\n",
      "2017-07-12T16:07:32.764486 : step 1188 loss [[ 45.81864166]]\n",
      "6\n",
      "2017-07-12T16:07:32.769490 : step 1189 loss [[ 52.98697662]]\n",
      "6\n",
      "2017-07-12T16:07:32.774493 : step 1190 loss [[ 52.64463043]]\n",
      "6\n",
      "2017-07-12T16:07:32.779499 : step 1191 loss [[ 47.87191772]]\n",
      "6\n",
      "2017-07-12T16:07:32.784502 : step 1192 loss [[ 48.70000839]]\n",
      "6\n",
      "2017-07-12T16:07:32.789506 : step 1193 loss [[ 48.28857422]]\n",
      "6\n",
      "2017-07-12T16:07:32.794549 : step 1194 loss [[ 54.71473694]]\n",
      "5\n",
      "2017-07-12T16:07:32.799553 : step 1195 loss [[ 52.71759796]]\n",
      "5\n",
      "2017-07-12T16:07:32.803556 : step 1196 loss [[ 51.79207993]]\n",
      "5\n",
      "2017-07-12T16:07:32.808559 : step 1197 loss [[ 48.66038513]]\n",
      "5\n",
      "2017-07-12T16:07:32.813563 : step 1198 loss [[ 51.43933868]]\n",
      "5\n",
      "2017-07-12T16:07:32.819568 : step 1199 loss [[ 47.26449203]]\n",
      "12\n",
      "2017-07-12T16:07:32.824571 : step 1200 loss [[ 50.25962067]]\n",
      "12\n",
      "2017-07-12T16:07:32.829574 : step 1201 loss [[ 49.51836014]]\n",
      "12\n",
      "2017-07-12T16:07:32.835579 : step 1202 loss [[ 50.82141876]]\n",
      "12\n",
      "2017-07-12T16:07:32.840582 : step 1203 loss [[ 50.28885651]]\n",
      "12\n",
      "2017-07-12T16:07:32.845586 : step 1204 loss [[ 49.15573883]]\n",
      "12\n",
      "2017-07-12T16:07:32.850589 : step 1205 loss [[ 50.95837021]]\n",
      "12\n",
      "2017-07-12T16:07:32.855593 : step 1206 loss [[ 50.28490067]]\n",
      "12\n",
      "2017-07-12T16:07:32.860596 : step 1207 loss [[ 53.84643555]]\n",
      "12\n",
      "2017-07-12T16:07:32.864599 : step 1208 loss [[ 53.42988968]]\n",
      "12\n",
      "2017-07-12T16:07:32.869603 : step 1209 loss [[ 49.95729065]]\n",
      "12\n",
      "2017-07-12T16:07:32.875607 : step 1210 loss [[ 53.81002426]]\n",
      "12\n",
      "2017-07-12T16:07:32.880112 : step 1211 loss [[ 53.52472305]]\n",
      "7\n",
      "2017-07-12T16:07:32.885115 : step 1212 loss [[ 54.66471863]]\n",
      "7\n",
      "2017-07-12T16:07:32.890119 : step 1213 loss [[ 48.44537354]]\n",
      "7\n",
      "2017-07-12T16:07:32.895124 : step 1214 loss [[ 49.96926117]]\n",
      "7\n",
      "2017-07-12T16:07:32.900128 : step 1215 loss [[ 52.42956924]]\n",
      "7\n",
      "2017-07-12T16:07:32.906132 : step 1216 loss [[ 53.46335602]]\n",
      "7\n",
      "2017-07-12T16:07:32.911135 : step 1217 loss [[ 53.65827942]]\n",
      "7\n",
      "2017-07-12T16:07:32.917140 : step 1218 loss [[ 51.91397476]]\n",
      "35\n",
      "2017-07-12T16:07:32.922143 : step 1219 loss [[ 51.70549774]]\n",
      "35\n",
      "2017-07-12T16:07:32.927147 : step 1220 loss [[ 51.26990128]]\n",
      "35\n",
      "2017-07-12T16:07:32.934153 : step 1221 loss [[ 48.49752808]]\n",
      "35\n",
      "2017-07-12T16:07:32.939156 : step 1222 loss [[ 48.5892334]]\n",
      "35\n",
      "2017-07-12T16:07:32.944159 : step 1223 loss [[ 52.21481705]]\n",
      "35\n",
      "2017-07-12T16:07:32.949163 : step 1224 loss [[ 51.16404343]]\n",
      "35\n",
      "2017-07-12T16:07:32.956168 : step 1225 loss [[ 50.1627655]]\n",
      "35\n",
      "2017-07-12T16:07:32.961172 : step 1226 loss [[ 52.19091797]]\n",
      "35\n",
      "2017-07-12T16:07:32.966174 : step 1227 loss [[ 53.23185349]]\n",
      "35\n",
      "2017-07-12T16:07:32.973180 : step 1228 loss [[ 54.55613708]]\n",
      "35\n",
      "2017-07-12T16:07:32.978186 : step 1229 loss [[ 52.66726685]]\n",
      "35\n",
      "2017-07-12T16:07:32.983190 : step 1230 loss [[ 50.70707321]]\n",
      "35\n",
      "2017-07-12T16:07:32.988193 : step 1231 loss [[ 50.22520828]]\n",
      "35\n",
      "2017-07-12T16:07:32.992750 : step 1232 loss [[ 55.3675766]]\n",
      "35\n",
      "2017-07-12T16:07:32.997754 : step 1233 loss [[ 50.75354767]]\n",
      "35\n",
      "2017-07-12T16:07:33.002757 : step 1234 loss [[ 47.70307159]]\n",
      "35\n",
      "2017-07-12T16:07:33.007761 : step 1235 loss [[ 48.84051895]]\n",
      "35\n",
      "2017-07-12T16:07:33.012765 : step 1236 loss [[ 49.09360886]]\n",
      "35\n",
      "2017-07-12T16:07:33.017769 : step 1237 loss [[ 48.76298904]]\n",
      "35\n",
      "2017-07-12T16:07:33.022772 : step 1238 loss [[ 51.0547905]]\n",
      "35\n",
      "2017-07-12T16:07:33.027775 : step 1239 loss [[ 52.75053787]]\n",
      "35\n",
      "2017-07-12T16:07:33.032779 : step 1240 loss [[ 50.61822128]]\n",
      "35\n",
      "2017-07-12T16:07:33.037782 : step 1241 loss [[ 49.22262192]]\n",
      "35\n",
      "2017-07-12T16:07:33.043786 : step 1242 loss [[ 48.66450119]]\n",
      "35\n",
      "2017-07-12T16:07:33.048790 : step 1243 loss [[ 48.91182327]]\n",
      "35\n",
      "2017-07-12T16:07:33.053794 : step 1244 loss [[ 45.69830322]]\n",
      "35\n",
      "2017-07-12T16:07:33.059798 : step 1245 loss [[ 50.90768051]]\n",
      "35\n",
      "2017-07-12T16:07:33.064802 : step 1246 loss [[ 49.64709091]]\n",
      "35\n",
      "2017-07-12T16:07:33.069805 : step 1247 loss [[ 54.81751633]]\n",
      "35\n",
      "2017-07-12T16:07:33.074808 : step 1248 loss [[ 49.34329224]]\n",
      "35\n",
      "2017-07-12T16:07:33.079814 : step 1249 loss [[ 50.14223862]]\n",
      "35\n",
      "2017-07-12T16:07:33.084818 : step 1250 loss [[ 47.57508469]]\n",
      "35\n",
      "2017-07-12T16:07:33.089321 : step 1251 loss [[ 50.74045181]]\n",
      "35\n",
      "2017-07-12T16:07:33.094327 : step 1252 loss [[ 48.18439484]]\n",
      "35\n",
      "2017-07-12T16:07:33.099330 : step 1253 loss [[ 48.99055481]]\n",
      "15\n",
      "2017-07-12T16:07:33.104333 : step 1254 loss [[ 56.11104965]]\n",
      "15\n",
      "2017-07-12T16:07:33.109337 : step 1255 loss [[ 52.29385376]]\n",
      "15\n",
      "2017-07-12T16:07:33.114341 : step 1256 loss [[ 51.41350174]]\n",
      "15\n",
      "2017-07-12T16:07:33.119345 : step 1257 loss [[ 53.00868607]]\n",
      "15\n",
      "2017-07-12T16:07:33.125349 : step 1258 loss [[ 47.55290985]]\n",
      "15\n",
      "2017-07-12T16:07:33.130352 : step 1259 loss [[ 53.1594162]]\n",
      "15\n",
      "2017-07-12T16:07:33.135357 : step 1260 loss [[ 52.72756577]]\n",
      "15\n",
      "2017-07-12T16:07:33.140359 : step 1261 loss [[ 54.05430603]]\n",
      "15\n",
      "2017-07-12T16:07:33.144362 : step 1262 loss [[ 51.68647385]]\n",
      "15\n",
      "2017-07-12T16:07:33.149366 : step 1263 loss [[ 52.72026062]]\n",
      "15\n",
      "2017-07-12T16:07:33.155370 : step 1264 loss [[ 51.35873795]]\n",
      "15\n",
      "2017-07-12T16:07:33.161374 : step 1265 loss [[ 56.03140259]]\n",
      "15\n",
      "2017-07-12T16:07:33.166378 : step 1266 loss [[ 48.74147415]]\n",
      "15\n",
      "2017-07-12T16:07:33.171382 : step 1267 loss [[ 50.22662354]]\n",
      "15\n",
      "2017-07-12T16:07:33.176387 : step 1268 loss [[ 53.15988541]]\n",
      "5\n",
      "2017-07-12T16:07:33.180890 : step 1269 loss [[ 45.1008873]]\n",
      "5\n",
      "2017-07-12T16:07:33.185894 : step 1270 loss [[ 44.24354553]]\n",
      "5\n",
      "2017-07-12T16:07:33.190897 : step 1271 loss [[ 43.45593262]]\n",
      "5\n",
      "2017-07-12T16:07:33.195072 : step 1272 loss [[ 46.83511734]]\n",
      "5\n",
      "2017-07-12T16:07:33.200075 : step 1273 loss [[ 47.91189194]]\n",
      "9\n",
      "2017-07-12T16:07:33.205079 : step 1274 loss [[ 52.43378067]]\n",
      "9\n",
      "2017-07-12T16:07:33.209082 : step 1275 loss [[ 50.16489792]]\n",
      "9\n",
      "2017-07-12T16:07:33.214085 : step 1276 loss [[ 48.65572357]]\n",
      "9\n",
      "2017-07-12T16:07:33.221090 : step 1277 loss [[ 48.24117661]]\n",
      "9\n",
      "2017-07-12T16:07:33.226094 : step 1278 loss [[ 48.61231613]]\n",
      "9\n",
      "2017-07-12T16:07:33.231097 : step 1279 loss [[ 49.57079697]]\n",
      "9\n",
      "2017-07-12T16:07:33.237102 : step 1280 loss [[ 52.8997612]]\n",
      "9\n",
      "2017-07-12T16:07:33.241104 : step 1281 loss [[ 52.3524437]]\n",
      "9\n",
      "2017-07-12T16:07:33.248110 : step 1282 loss [[ 50.53078842]]\n",
      "5\n",
      "2017-07-12T16:07:33.253113 : step 1283 loss [[ 50.1198616]]\n",
      "5\n",
      "2017-07-12T16:07:33.258117 : step 1284 loss [[ 49.31761169]]\n",
      "5\n",
      "2017-07-12T16:07:33.262119 : step 1285 loss [[ 51.78637695]]\n",
      "5\n",
      "2017-07-12T16:07:33.267123 : step 1286 loss [[ 54.34460068]]\n",
      "5\n",
      "2017-07-12T16:07:33.272127 : step 1287 loss [[ 53.08386993]]\n",
      "7\n",
      "2017-07-12T16:07:33.277133 : step 1288 loss [[ 51.30498123]]\n",
      "7\n",
      "2017-07-12T16:07:33.282136 : step 1289 loss [[ 51.57997513]]\n",
      "7\n",
      "2017-07-12T16:07:33.287141 : step 1290 loss [[ 50.44961929]]\n",
      "7\n",
      "2017-07-12T16:07:33.291643 : step 1291 loss [[ 50.18639374]]\n",
      "7\n",
      "2017-07-12T16:07:33.296648 : step 1292 loss [[ 52.29794312]]\n",
      "7\n",
      "2017-07-12T16:07:33.300651 : step 1293 loss [[ 51.97866058]]\n",
      "7\n",
      "2017-07-12T16:07:33.305655 : step 1294 loss [[ 52.34204483]]\n",
      "22\n",
      "2017-07-12T16:07:33.312661 : step 1295 loss [[ 50.38843155]]\n",
      "22\n",
      "2017-07-12T16:07:33.317664 : step 1296 loss [[ 50.13540649]]\n",
      "22\n",
      "2017-07-12T16:07:33.322667 : step 1297 loss [[ 49.1120491]]\n",
      "22\n",
      "2017-07-12T16:07:33.326670 : step 1298 loss [[ 49.6047821]]\n",
      "22\n",
      "2017-07-12T16:07:33.331673 : step 1299 loss [[ 49.66971588]]\n",
      "22\n",
      "2017-07-12T16:07:33.336677 : step 1300 loss [[ 51.73529816]]\n",
      "22\n",
      "2017-07-12T16:07:33.340680 : step 1301 loss [[ 53.06902313]]\n",
      "22\n",
      "2017-07-12T16:07:33.345683 : step 1302 loss [[ 51.95101166]]\n",
      "22\n",
      "2017-07-12T16:07:33.350687 : step 1303 loss [[ 52.05410004]]\n",
      "22\n",
      "2017-07-12T16:07:33.355690 : step 1304 loss [[ 51.38147354]]\n",
      "22\n",
      "2017-07-12T16:07:33.360694 : step 1305 loss [[ 53.11156845]]\n",
      "22\n",
      "2017-07-12T16:07:33.364697 : step 1306 loss [[ 53.15439224]]\n",
      "22\n",
      "2017-07-12T16:07:33.369701 : step 1307 loss [[ 51.52567291]]\n",
      "22\n",
      "2017-07-12T16:07:33.374704 : step 1308 loss [[ 49.31797028]]\n",
      "22\n",
      "2017-07-12T16:07:33.380210 : step 1309 loss [[ 48.9663887]]\n",
      "22\n",
      "2017-07-12T16:07:33.385214 : step 1310 loss [[ 48.24029541]]\n",
      "22\n",
      "2017-07-12T16:07:33.390218 : step 1311 loss [[ 48.06689072]]\n",
      "22\n",
      "2017-07-12T16:07:33.394536 : step 1312 loss [[ 51.64048767]]\n",
      "22\n",
      "2017-07-12T16:07:33.399540 : step 1313 loss [[ 49.82432175]]\n",
      "22\n",
      "2017-07-12T16:07:33.404544 : step 1314 loss [[ 49.97233582]]\n",
      "22\n",
      "2017-07-12T16:07:33.409547 : step 1315 loss [[ 49.85614777]]\n",
      "22\n",
      "2017-07-12T16:07:33.413550 : step 1316 loss [[ 51.27478027]]\n",
      "7\n",
      "2017-07-12T16:07:33.420558 : step 1317 loss [[ 51.5159874]]\n",
      "7\n",
      "2017-07-12T16:07:33.425559 : step 1318 loss [[ 49.14952087]]\n",
      "7\n",
      "2017-07-12T16:07:33.430562 : step 1319 loss [[ 49.69525146]]\n",
      "7\n",
      "2017-07-12T16:07:33.435566 : step 1320 loss [[ 49.84283447]]\n",
      "7\n",
      "2017-07-12T16:07:33.440569 : step 1321 loss [[ 50.4379425]]\n",
      "7\n",
      "2017-07-12T16:07:33.445573 : step 1322 loss [[ 52.20590973]]\n",
      "7\n",
      "2017-07-12T16:07:33.449576 : step 1323 loss [[ 51.30745697]]\n",
      "5\n",
      "2017-07-12T16:07:33.454579 : step 1324 loss [[ 50.70613861]]\n",
      "5\n",
      "2017-07-12T16:07:33.459583 : step 1325 loss [[ 52.39233017]]\n",
      "5\n",
      "2017-07-12T16:07:33.464586 : step 1326 loss [[ 51.87310028]]\n",
      "5\n",
      "2017-07-12T16:07:33.469590 : step 1327 loss [[ 49.62989044]]\n",
      "5\n",
      "2017-07-12T16:07:33.474593 : step 1328 loss [[ 48.20805359]]\n",
      "13\n",
      "2017-07-12T16:07:33.480100 : step 1329 loss [[ 46.98962402]]\n",
      "13\n",
      "2017-07-12T16:07:33.484604 : step 1330 loss [[ 48.19695282]]\n",
      "13\n",
      "2017-07-12T16:07:33.489607 : step 1331 loss [[ 50.82951736]]\n",
      "13\n",
      "2017-07-12T16:07:33.493612 : step 1332 loss [[ 55.20355225]]\n",
      "13\n",
      "2017-07-12T16:07:33.499617 : step 1333 loss [[ 54.39385986]]\n",
      "13\n",
      "2017-07-12T16:07:33.503619 : step 1334 loss [[ 51.80610657]]\n",
      "13\n",
      "2017-07-12T16:07:33.508623 : step 1335 loss [[ 50.30636597]]\n",
      "13\n",
      "2017-07-12T16:07:33.513627 : step 1336 loss [[ 50.30347824]]\n",
      "13\n",
      "2017-07-12T16:07:33.518630 : step 1337 loss [[ 51.18002701]]\n",
      "13\n",
      "2017-07-12T16:07:33.523633 : step 1338 loss [[ 50.94594955]]\n",
      "13\n",
      "2017-07-12T16:07:33.528637 : step 1339 loss [[ 50.95967102]]\n",
      "13\n",
      "2017-07-12T16:07:33.533641 : step 1340 loss [[ 49.47612381]]\n",
      "13\n",
      "2017-07-12T16:07:33.538644 : step 1341 loss [[ 51.80596924]]\n",
      "24\n",
      "2017-07-12T16:07:33.543648 : step 1342 loss [[ 48.86072159]]\n",
      "24\n",
      "2017-07-12T16:07:33.548652 : step 1343 loss [[ 48.26288986]]\n",
      "24\n",
      "2017-07-12T16:07:33.553655 : step 1344 loss [[ 53.63644028]]\n",
      "24\n",
      "2017-07-12T16:07:33.558658 : step 1345 loss [[ 51.4733429]]\n",
      "24\n",
      "2017-07-12T16:07:33.564663 : step 1346 loss [[ 50.52907944]]\n",
      "24\n",
      "2017-07-12T16:07:33.568666 : step 1347 loss [[ 52.41307831]]\n",
      "24\n",
      "2017-07-12T16:07:33.573669 : step 1348 loss [[ 51.01986694]]\n",
      "24\n",
      "2017-07-12T16:07:33.579177 : step 1349 loss [[ 53.94781113]]\n",
      "24\n",
      "2017-07-12T16:07:33.584681 : step 1350 loss [[ 49.8405304]]\n",
      "24\n",
      "2017-07-12T16:07:33.590184 : step 1351 loss [[ 49.8934021]]\n",
      "24\n",
      "2017-07-12T16:07:33.595692 : step 1352 loss [[ 48.86009598]]\n",
      "24\n",
      "2017-07-12T16:07:33.600695 : step 1353 loss [[ 47.58351135]]\n",
      "24\n",
      "2017-07-12T16:07:33.606700 : step 1354 loss [[ 45.97350311]]\n",
      "24\n",
      "2017-07-12T16:07:33.610702 : step 1355 loss [[ 45.57458115]]\n",
      "24\n",
      "2017-07-12T16:07:33.615706 : step 1356 loss [[ 51.42041779]]\n",
      "24\n",
      "2017-07-12T16:07:33.620710 : step 1357 loss [[ 47.63726807]]\n",
      "24\n",
      "2017-07-12T16:07:33.625714 : step 1358 loss [[ 47.11103058]]\n",
      "24\n",
      "2017-07-12T16:07:33.630717 : step 1359 loss [[ 48.80315781]]\n",
      "24\n",
      "2017-07-12T16:07:33.635720 : step 1360 loss [[ 48.06316757]]\n",
      "24\n",
      "2017-07-12T16:07:33.640723 : step 1361 loss [[ 47.46200943]]\n",
      "24\n",
      "2017-07-12T16:07:33.645727 : step 1362 loss [[ 47.06430054]]\n",
      "24\n",
      "2017-07-12T16:07:33.649730 : step 1363 loss [[ 45.6821022]]\n",
      "24\n",
      "2017-07-12T16:07:33.656735 : step 1364 loss [[ 44.86281204]]\n",
      "24\n",
      "2017-07-12T16:07:33.661739 : step 1365 loss [[ 50.07512283]]\n",
      "cost:  [[ 66889.1171875]]\n",
      "9\n",
      "2017-07-12T16:07:33.667743 : step 1366 loss [[ 51.20384216]]\n",
      "9\n",
      "2017-07-12T16:07:33.671746 : step 1367 loss [[ 50.13971329]]\n",
      "9\n",
      "2017-07-12T16:07:33.677252 : step 1368 loss [[ 49.82532883]]\n",
      "9\n",
      "2017-07-12T16:07:33.682255 : step 1369 loss [[ 45.61194992]]\n",
      "9\n",
      "2017-07-12T16:07:33.687759 : step 1370 loss [[ 50.64696503]]\n",
      "9\n",
      "2017-07-12T16:07:33.692440 : step 1371 loss [[ 47.47668457]]\n",
      "9\n",
      "2017-07-12T16:07:33.697443 : step 1372 loss [[ 47.05688858]]\n",
      "9\n",
      "2017-07-12T16:07:33.702447 : step 1373 loss [[ 52.78447723]]\n",
      "9\n",
      "2017-07-12T16:07:33.707450 : step 1374 loss [[ 50.06575775]]\n",
      "11\n",
      "2017-07-12T16:07:33.712454 : step 1375 loss [[ 53.4175148]]\n",
      "11\n",
      "2017-07-12T16:07:33.717458 : step 1376 loss [[ 51.87416458]]\n",
      "11\n",
      "2017-07-12T16:07:33.722461 : step 1377 loss [[ 50.81361771]]\n",
      "11\n",
      "2017-07-12T16:07:33.727465 : step 1378 loss [[ 50.54256821]]\n",
      "11\n",
      "2017-07-12T16:07:33.732468 : step 1379 loss [[ 53.07816696]]\n",
      "11\n",
      "2017-07-12T16:07:33.737472 : step 1380 loss [[ 56.26220322]]\n",
      "11\n",
      "2017-07-12T16:07:33.742475 : step 1381 loss [[ 53.73270798]]\n",
      "11\n",
      "2017-07-12T16:07:33.747480 : step 1382 loss [[ 49.09259415]]\n",
      "11\n",
      "2017-07-12T16:07:33.752483 : step 1383 loss [[ 49.44000244]]\n",
      "11\n",
      "2017-07-12T16:07:33.757486 : step 1384 loss [[ 49.04850388]]\n",
      "11\n",
      "2017-07-12T16:07:33.762490 : step 1385 loss [[ 49.46655655]]\n",
      "cost:  [[ 67900.6875]]\n",
      "16\n",
      "2017-07-12T16:07:33.767493 : step 1386 loss [[ 52.00725555]]\n",
      "16\n",
      "2017-07-12T16:07:33.772497 : step 1387 loss [[ 49.34356308]]\n",
      "16\n",
      "2017-07-12T16:07:33.778005 : step 1388 loss [[ 46.98911667]]\n",
      "16\n",
      "2017-07-12T16:07:33.782508 : step 1389 loss [[ 48.40956879]]\n",
      "16\n",
      "2017-07-12T16:07:33.788011 : step 1390 loss [[ 47.9794426]]\n",
      "16\n",
      "2017-07-12T16:07:33.792553 : step 1391 loss [[ 48.63827896]]\n",
      "16\n",
      "2017-07-12T16:07:33.797557 : step 1392 loss [[ 48.9478569]]\n",
      "16\n",
      "2017-07-12T16:07:33.801560 : step 1393 loss [[ 50.84202194]]\n",
      "16\n",
      "2017-07-12T16:07:33.806563 : step 1394 loss [[ 50.77267075]]\n",
      "16\n",
      "2017-07-12T16:07:33.811567 : step 1395 loss [[ 50.58444214]]\n",
      "16\n",
      "2017-07-12T16:07:33.816570 : step 1396 loss [[ 52.40188599]]\n",
      "16\n",
      "2017-07-12T16:07:33.821574 : step 1397 loss [[ 52.32683563]]\n",
      "16\n",
      "2017-07-12T16:07:33.825577 : step 1398 loss [[ 53.09796524]]\n",
      "16\n",
      "2017-07-12T16:07:33.830580 : step 1399 loss [[ 48.88657379]]\n",
      "16\n",
      "2017-07-12T16:07:33.835583 : step 1400 loss [[ 49.80113602]]\n",
      "16\n",
      "2017-07-12T16:07:33.840587 : step 1401 loss [[ 53.76444244]]\n",
      "9\n",
      "2017-07-12T16:07:33.846591 : step 1402 loss [[ 51.43330383]]\n",
      "9\n",
      "2017-07-12T16:07:33.850594 : step 1403 loss [[ 51.65476608]]\n",
      "9\n",
      "2017-07-12T16:07:33.855598 : step 1404 loss [[ 51.67758179]]\n",
      "9\n",
      "2017-07-12T16:07:33.860601 : step 1405 loss [[ 51.98419952]]\n",
      "9\n",
      "2017-07-12T16:07:33.865605 : step 1406 loss [[ 51.19647217]]\n",
      "9\n",
      "2017-07-12T16:07:33.870608 : step 1407 loss [[ 50.68302917]]\n",
      "9\n",
      "2017-07-12T16:07:33.875612 : step 1408 loss [[ 49.94522858]]\n",
      "9\n",
      "2017-07-12T16:07:33.880618 : step 1409 loss [[ 49.41215515]]\n",
      "9\n",
      "2017-07-12T16:07:33.885622 : step 1410 loss [[ 50.40332031]]\n",
      "cost:  [[ 69163.875]]\n",
      "6\n",
      "2017-07-12T16:07:33.890625 : step 1411 loss [[ 49.56374359]]\n",
      "6\n",
      "2017-07-12T16:07:33.894840 : step 1412 loss [[ 47.20923615]]\n",
      "6\n",
      "2017-07-12T16:07:33.899843 : step 1413 loss [[ 49.77380753]]\n",
      "6\n",
      "2017-07-12T16:07:33.906849 : step 1414 loss [[ 51.63244247]]\n",
      "6\n",
      "2017-07-12T16:07:33.911852 : step 1415 loss [[ 49.15123749]]\n",
      "6\n",
      "2017-07-12T16:07:33.916856 : step 1416 loss [[ 49.16096115]]\n",
      "51\n",
      "2017-07-12T16:07:33.921859 : step 1417 loss [[ 46.34642029]]\n",
      "51\n",
      "2017-07-12T16:07:33.926863 : step 1418 loss [[ 47.48324966]]\n",
      "51\n",
      "2017-07-12T16:07:33.930865 : step 1419 loss [[ 45.93191147]]\n",
      "51\n",
      "2017-07-12T16:07:33.938871 : step 1420 loss [[ 49.67887878]]\n",
      "51\n",
      "2017-07-12T16:07:33.942874 : step 1421 loss [[ 49.19554138]]\n",
      "51\n",
      "2017-07-12T16:07:33.947878 : step 1422 loss [[ 49.93742371]]\n",
      "51\n",
      "2017-07-12T16:07:33.952881 : step 1423 loss [[ 49.65195465]]\n",
      "51\n",
      "2017-07-12T16:07:33.957885 : step 1424 loss [[ 48.31394196]]\n",
      "51\n",
      "2017-07-12T16:07:33.962888 : step 1425 loss [[ 48.26369476]]\n",
      "51\n",
      "2017-07-12T16:07:33.967892 : step 1426 loss [[ 50.74258423]]\n",
      "51\n",
      "2017-07-12T16:07:33.972895 : step 1427 loss [[ 49.71671295]]\n",
      "51\n",
      "2017-07-12T16:07:33.978402 : step 1428 loss [[ 51.51301575]]\n",
      "51\n",
      "2017-07-12T16:07:33.982905 : step 1429 loss [[ 48.77681732]]\n",
      "51\n",
      "2017-07-12T16:07:33.987909 : step 1430 loss [[ 45.32259369]]\n",
      "51\n",
      "2017-07-12T16:07:33.991912 : step 1431 loss [[ 46.36599731]]\n",
      "51\n",
      "2017-07-12T16:07:33.996995 : step 1432 loss [[ 43.23624039]]\n",
      "51\n",
      "2017-07-12T16:07:34.001999 : step 1433 loss [[ 49.68577576]]\n",
      "51\n",
      "2017-07-12T16:07:34.007002 : step 1434 loss [[ 44.55566025]]\n",
      "51\n",
      "2017-07-12T16:07:34.012006 : step 1435 loss [[ 47.53702545]]\n",
      "51\n",
      "2017-07-12T16:07:34.017009 : step 1436 loss [[ 47.53655243]]\n",
      "51\n",
      "2017-07-12T16:07:34.021012 : step 1437 loss [[ 48.483181]]\n",
      "51\n",
      "2017-07-12T16:07:34.026016 : step 1438 loss [[ 50.49487305]]\n",
      "51\n",
      "2017-07-12T16:07:34.031019 : step 1439 loss [[ 49.52409744]]\n",
      "51\n",
      "2017-07-12T16:07:34.037023 : step 1440 loss [[ 48.56132889]]\n",
      "51\n",
      "2017-07-12T16:07:34.042027 : step 1441 loss [[ 47.6749115]]\n",
      "51\n",
      "2017-07-12T16:07:34.046030 : step 1442 loss [[ 47.90582657]]\n",
      "51\n",
      "2017-07-12T16:07:34.051033 : step 1443 loss [[ 49.51664352]]\n",
      "51\n",
      "2017-07-12T16:07:34.056037 : step 1444 loss [[ 47.50647736]]\n",
      "51\n",
      "2017-07-12T16:07:34.061041 : step 1445 loss [[ 48.25562668]]\n",
      "51\n",
      "2017-07-12T16:07:34.066043 : step 1446 loss [[ 47.10839844]]\n",
      "51\n",
      "2017-07-12T16:07:34.071047 : step 1447 loss [[ 48.54998016]]\n",
      "51\n",
      "2017-07-12T16:07:34.076051 : step 1448 loss [[ 48.88843155]]\n",
      "51\n",
      "2017-07-12T16:07:34.081056 : step 1449 loss [[ 47.38990784]]\n",
      "51\n",
      "2017-07-12T16:07:34.085560 : step 1450 loss [[ 51.26305389]]\n",
      "51\n",
      "2017-07-12T16:07:34.091064 : step 1451 loss [[ 48.91384888]]\n",
      "51\n",
      "2017-07-12T16:07:34.096144 : step 1452 loss [[ 49.89642334]]\n",
      "51\n",
      "2017-07-12T16:07:34.100147 : step 1453 loss [[ 50.19660568]]\n",
      "51\n",
      "2017-07-12T16:07:34.105150 : step 1454 loss [[ 50.21628571]]\n",
      "51\n",
      "2017-07-12T16:07:34.110154 : step 1455 loss [[ 47.79062653]]\n",
      "51\n",
      "2017-07-12T16:07:34.115157 : step 1456 loss [[ 51.57158661]]\n",
      "51\n",
      "2017-07-12T16:07:34.120161 : step 1457 loss [[ 49.21495819]]\n",
      "51\n",
      "2017-07-12T16:07:34.126165 : step 1458 loss [[ 48.14242935]]\n",
      "51\n",
      "2017-07-12T16:07:34.131169 : step 1459 loss [[ 49.06642151]]\n",
      "51\n",
      "2017-07-12T16:07:34.136172 : step 1460 loss [[ 46.40148163]]\n",
      "51\n",
      "2017-07-12T16:07:34.141176 : step 1461 loss [[ 48.35104752]]\n",
      "51\n",
      "2017-07-12T16:07:34.146179 : step 1462 loss [[ 46.7851944]]\n",
      "51\n",
      "2017-07-12T16:07:34.151183 : step 1463 loss [[ 44.23198318]]\n",
      "51\n",
      "2017-07-12T16:07:34.156186 : step 1464 loss [[ 44.83236694]]\n",
      "51\n",
      "2017-07-12T16:07:34.161190 : step 1465 loss [[ 42.84380341]]\n",
      "51\n",
      "2017-07-12T16:07:34.167195 : step 1466 loss [[ 43.45223618]]\n",
      "51\n",
      "2017-07-12T16:07:34.172198 : step 1467 loss [[ 46.74306488]]\n",
      "45\n",
      "2017-07-12T16:07:34.177201 : step 1468 loss [[ 50.0007782]]\n",
      "45\n",
      "2017-07-12T16:07:34.182207 : step 1469 loss [[ 47.53833389]]\n",
      "45\n",
      "2017-07-12T16:07:34.187711 : step 1470 loss [[ 46.83436966]]\n",
      "45\n",
      "2017-07-12T16:07:34.192214 : step 1471 loss [[ 53.34805298]]\n",
      "45\n",
      "2017-07-12T16:07:34.197287 : step 1472 loss [[ 53.88808823]]\n",
      "45\n",
      "2017-07-12T16:07:34.202291 : step 1473 loss [[ 57.50980759]]\n",
      "45\n",
      "2017-07-12T16:07:34.206294 : step 1474 loss [[ 52.90766525]]\n",
      "45\n",
      "2017-07-12T16:07:34.211297 : step 1475 loss [[ 53.63790131]]\n",
      "45\n",
      "2017-07-12T16:07:34.216301 : step 1476 loss [[ 49.3413353]]\n",
      "45\n",
      "2017-07-12T16:07:34.221305 : step 1477 loss [[ 48.99355698]]\n",
      "45\n",
      "2017-07-12T16:07:34.226308 : step 1478 loss [[ 52.89574814]]\n",
      "45\n",
      "2017-07-12T16:07:34.230311 : step 1479 loss [[ 51.32353973]]\n",
      "45\n",
      "2017-07-12T16:07:34.236315 : step 1480 loss [[ 50.44665909]]\n",
      "45\n",
      "2017-07-12T16:07:34.241318 : step 1481 loss [[ 52.62585068]]\n",
      "45\n",
      "2017-07-12T16:07:34.246322 : step 1482 loss [[ 51.39047623]]\n",
      "45\n",
      "2017-07-12T16:07:34.251326 : step 1483 loss [[ 48.36344147]]\n",
      "45\n",
      "2017-07-12T16:07:34.256330 : step 1484 loss [[ 48.7345314]]\n",
      "45\n",
      "2017-07-12T16:07:34.262334 : step 1485 loss [[ 52.60894394]]\n",
      "45\n",
      "2017-07-12T16:07:34.267337 : step 1486 loss [[ 51.90646362]]\n",
      "45\n",
      "2017-07-12T16:07:34.272341 : step 1487 loss [[ 51.72896194]]\n",
      "45\n",
      "2017-07-12T16:07:34.277344 : step 1488 loss [[ 50.6586647]]\n",
      "45\n",
      "2017-07-12T16:07:34.282850 : step 1489 loss [[ 47.87836838]]\n",
      "45\n",
      "2017-07-12T16:07:34.287854 : step 1490 loss [[ 48.71331406]]\n",
      "45\n",
      "2017-07-12T16:07:34.292857 : step 1491 loss [[ 48.57220078]]\n",
      "45\n",
      "2017-07-12T16:07:34.297862 : step 1492 loss [[ 53.26535034]]\n",
      "45\n",
      "2017-07-12T16:07:34.301866 : step 1493 loss [[ 51.90332413]]\n",
      "45\n",
      "2017-07-12T16:07:34.306869 : step 1494 loss [[ 49.71732712]]\n",
      "45\n",
      "2017-07-12T16:07:34.311873 : step 1495 loss [[ 53.56122208]]\n",
      "45\n",
      "2017-07-12T16:07:34.316876 : step 1496 loss [[ 52.13306046]]\n",
      "45\n",
      "2017-07-12T16:07:34.321879 : step 1497 loss [[ 52.1084137]]\n",
      "45\n",
      "2017-07-12T16:07:34.326883 : step 1498 loss [[ 52.9683075]]\n",
      "45\n",
      "2017-07-12T16:07:34.330886 : step 1499 loss [[ 51.76193619]]\n",
      "45\n",
      "2017-07-12T16:07:34.335889 : step 1500 loss [[ 51.84108353]]\n",
      "45\n",
      "2017-07-12T16:07:34.340893 : step 1501 loss [[ 47.89749527]]\n",
      "45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-977cef1d844c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdrop_mask\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mex_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mex_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-977cef1d844c>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(chars, book, curr, cm, drop_mask, ns, nm, length)\u001b[0m\n\u001b[1;32m     81\u001b[0m             _, step, loss = sess.run(\n\u001b[1;32m     82\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"step\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\KWW\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py, random, csv, gzip, time \n",
    "import pickle\n",
    "from util import *\n",
    "from layers import RMN\n",
    "import os\n",
    "import datetime\n",
    "span_data, span_size, wmap, cmap, bmap = load_data('data/relationships.csv.gz', 'data/metadata.pkl')\n",
    "\n",
    "with open('data/glove.We', 'rb') as f:\n",
    "    We = pickle.load(f, encoding='latin1').astype('float32')\n",
    "\n",
    "norm_We = We / np.linalg.norm(We, axis=1)[:, None]\n",
    "We = np.nan_to_num(norm_We)\n",
    "\n",
    "descriptor_log = 'models/descriptors.log'\n",
    "trajectory_log = 'models/trajectories.log'\n",
    "\n",
    "# embedding/hidden dimensionality\n",
    "d_word = We.shape[1]\n",
    "d_char = 50\n",
    "d_book = 50\n",
    "d_hidden = 50\n",
    "\n",
    "# number of descriptors\n",
    "num_descs = 30\n",
    "\n",
    "# number of negative samples per relationship\n",
    "num_negs = 50\n",
    "\n",
    "# word dropout probability\n",
    "p_drop = 0.75\n",
    "\n",
    "n_epochs = 15\n",
    "lr = 0.001\n",
    "eps = 1e-6\n",
    "\n",
    "num_chars = len(cmap)\n",
    "num_books = len(bmap)\n",
    "num_traj = len(span_data)\n",
    "len_voc = len(wmap)\n",
    "revmap = {}\n",
    "\n",
    "for w in wmap:\n",
    "    revmap[wmap[w]] = w\n",
    "\t\n",
    "\t\n",
    "with tf.Graph().as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "    session_conf = tf.ConfigProto( allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        print ('compiling...')\n",
    "        RMN = RMN( d_word, d_char, d_book, d_hidden, len_voc, num_descs, num_chars, num_books, span_size, We, eps=eps, lr=lr, negs=num_negs )\n",
    "        print ('done compiling, now training...')\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        grads_and_vars = optimizer.compute_gradients(RMN.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "   \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # training loop\n",
    "        def train_step(chars, book, curr, cm, drop_mask, ns, nm, length):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            RMN.length = length\n",
    "            print (RMN.length)\n",
    "            feed_dict = {RMN.input_chars : chars, \n",
    "                         RMN.input_book : book, \n",
    "                         RMN.input_spans : curr, \n",
    "                         RMN.input_currmask : cm, \n",
    "                         RMN.input_dropmask : drop_mask , \n",
    "                         RMN.input_neg : ns,\n",
    "                         RMN.input_negmask : nm}\n",
    "            \n",
    "            _, step, loss = sess.run(\n",
    "                [train_op, global_step, RMN.loss],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(time_str,\":\", \"step\" ,step, \"loss\" , loss)\n",
    "            return loss\n",
    "        \n",
    "        min_cost = float('inf')\n",
    "        for epoch in range(n_epochs):\n",
    "            cost = 0.\n",
    "            random.shuffle(span_data)\n",
    "            start_time = time.time()\n",
    "            for book, chars, curr, cm, in span_data:\n",
    "                for index in range(len(curr)):\n",
    "                    \t\t\t\t\n",
    "                    ns, nm = generate_negative_samples(num_traj, span_size, num_negs, span_data)\n",
    "                \n",
    "                    drop_mask = (np.random.rand(*(cm[index].shape)) < (1 - p_drop)).astype('float32')\n",
    "                    drop_mask *= cm[index]\n",
    "                \n",
    "                    ex_cost = train_step(chars, book, np.expand_dims(curr[index],axis=0), np.expand_dims(cm[index],axis=0), np.expand_dims(drop_mask,axis=0), ns, nm, len(curr))\n",
    "                    cost += ex_cost\n",
    "                \n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 5 == 0:\n",
    "\n",
    "                    print(\"cost: \",cost)\n",
    "                \n",
    "            end_time = time.time() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_negative_samples(num_traj, span_size, negs, span_data):\n",
    "    inds = np.random.randint(0, num_traj, negs)\n",
    "    neg_words = np.zeros((negs, span_size)).astype('int32')\n",
    "    neg_masks = np.zeros((negs, span_size)).astype('float32')\n",
    "    for index, i in enumerate(inds):\n",
    "        rand_ind = np.random.randint(0, len(span_data[i][2]))\n",
    "        neg_words[index] = span_data[i][2][rand_ind]\n",
    "        neg_masks[index] = span_data[i][3][rand_ind]\n",
    "\n",
    "    return neg_words, neg_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drop_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116,), (50, 116), (116,), (25, 116), (2,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_mask.shape,ns.shape,curr[index].shape,cm.shape,chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2195511b48fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m116\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "curr.reshape(1,116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 116)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(drop_mask,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
